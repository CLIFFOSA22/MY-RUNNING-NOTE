My running notes for personal reading.
what is a shell?
	A shell is an interpreter of commands to the operatinmg system of a server.
Types of shell:
	Bash  - Bourne again shell
	Sh    - Bourne shell
	Ksh   - Korn shell
	csh   - cshell
	zsh   - zshell
How do you know the type of shells that you system has?
	you know by running the commmand  cat/etc/shells

How do you know the shell that you are currently working on?
	 	you run the commands  echo $SHELL

	 Shell Script

To creat a shell script sc, the script name must have .sh or .sc as an extension. This is for best practises in terms of a good naming convention.
For example. if the script is about file management, the script can be filemgt.sh of filemgt.sh
Therefore, let us creat a script called filemgt.sh

1. change your hostname to sc meaniung script
2. let us create a directory called sc and then cd into sc
3. vim filemgt.sh to add content to the script
4. script content must must starts with #!/bin/shell i.e #!/bin/sh or #!/bin/bash or #!/bin/ksh etc
5. add comments or a comment to descibe or explain what your script would be all about. Note, any sentences of a sentense thats starts with # is called a command and does to run in linux until a command is added.

Example of a Shell Script

#!/bin/bash
# A shell is an interpreter that interprets or explains the meaning of the various ccommand to the opening system os to act upon.
# A script is a file containing one or more command.
# Any sentence or sentences with hash sign at the beginning is called a single line comment or comments.
echo "devops is not difficult if you study hard"
echo "i will be a Sr. devops and cloud computing engineer in 2023"
echo "this message is for:"
whoami
date
whoami

You can copy the info above and paste it (vim filemgt.sh) and then save and quit.

You can run this script by running 

./filemgt.sh and if it says permission denied, it means that the filemgt.sh is not executable. you can run ll to confirm if it is executable or not. 
if it is not, run  sudo chmod +x filemgt.sh  to add executable permission to the script. You can reconfirm by running ll. after all this is done, you can now run the script.

You can always check the contect of the script before it was runned by cat -n filemgt.sh  where n= number of lines of the content.

You can run the script/code by passing the executable permssion. that is, you can decide to ignore if it has permssion or not by running the command  shellType filemgt.sh  i.e  bash filemgt.sh or sh filemgt.sh or csh filemgt.sh etc

Note. We have 2 types of comments. 
1. The single line comments slc
2. The multi line comments mlc

1. The above script filemgt.sh has a single line comments. This usually starts with hash on the sentences od sentence.

2.  This is the type that starts with << tile
											sentences you wiah to pu here
										 title
For example. let the tile be stage or dev. you then have 
									<< dev
											sentences you wiah to pu here
										dev

Example of MLC

#!/bin/bash
<< dev
	A shell is an interpreter that interprets or explains the meaning of the various ccommand to the opening system os to act upon.
	A script is a file containing one or more command.
	Any sentence or sentences with hash sign at the beginning is called a single line comment or comments.
dev
echo "devops can pay my bills comfortably"
echo "it is absolutely true"
echo "this message is for:"
whoami
date
whoami

 For a Groovy Script

1. Single line comments slc of a groovy scripts usually has just // at the beginning of any sentence.
2. Multi line comments mlc of a groovy scripts usually starts with /* amd ends with */

For a Extendable markup language Script (xml)

1. Multi line comments mlc of a xml usually starts with <!-- amd ends with -->

Making your script more portable. This is a process where creating and running your script without shebang but using a variable instead.

Example of a variable script

Let create a script called greet.sh

name=landmarktechnology
user=cliffosa
echo "$name"
echo "$user"
date
whoami

You can run the script by sh greet.sh or bash greet.sh etc

Types of variable Script

1. User defined variable (udv):	The above script, is what is called a user defined variable. It is a variable script that is been constructed, conducted and defined by a user.

2. System defined variable (sdv); This is a variable that is defined by the system or the environment you are working. This is also called environment varaiable (env).

You can run env in your system and it would bring out all the system variables in  your system. You can modify any of the variable if you so desired.

For example, 
run the command env 
You can change the HISTSIZE TO 2000 by running export HISTSIZE=2000 
but this would only be temporal because when you reboot your system, it would go back to its original size before you ultered it.

You can confirm your changes made on the HISTSIZE by running echo $HISTSIZE 
to know if the changes made became effective.

You can make the changes permanent when you run the commands

1.   vim .bashrc   this is the name of the system variable script

2.  and then type in  export HISTSIZE=2000 
3.  run               source .bashrc
4. run                echo $HISTSIZE           to confirm

You can also add the content in order to remember what you did i.e 

echo "export HISTSIZE 2000, this is system defined variable script and i increased the histsize from 1000 to 2000" >> .bashrc

To know if the content you added to the script is up and running. cat scriptName i.e
cat .bashrc

Types of user defined variable (udv).
1. Static defined variable of a bash_shell_script. The basically make use of "equal to sign". Example is below

#!/bin/sh
# To construct a form for people working in Landmark
name=cliford
company=landmarkTechnology
activity=automationServices
echo "welcome to $company, a place for authentic and reliable $activity"
echo "$name will take you around the premises for review. Thank you once again for coming". `whoami`
date

Let us create a scriptyName called udv1.sh

vim udv.sh and copy-paste the above script and the save-quit.
Run sh udv.sh

Note top recover a file that has crashed. You run vim -r fileName of scriptName. r-stands for recover.

2. Dynamic variable of a bash_shell_script

Another type of a user defined variable script is dynamic variable.
Dynamic variable of a bash_shell_script: This is a variable that is accompanied with a "read" command. It makes the script to be portable and re-usable.

Example

#!/bin/bash
# Let us create a script that can be reusable in TD bank
# Create a script with scriptName tdbnk.sh
echo "Please enter your name"
read name
echo "Please enter your pin"
read pin
echo "$name, welcome to TD bank"
echo"$name, your $pin is valid"
echo "enter the amount you want to withdraw"
read amount
echo "Cash withdrawal of $amount is completed. You can please remove your card and cash"
echo "$name, thank you for banking with us"
date
whoami

vim td.sh and copy-paste the above script and then save-quit. run the script i.e bash td.sh

Tha above script is an example of a dynamic variable script of a fintech company like banks. 
It is also called an input variable because again, the presence of the"read" command i.e dynamic variable is an input vairiable

Note for best practices and for security purposes. It is good to pass a secret command on the part of the pin to make the pin hiding. It is not good for a pin or passcode to be visible. It is a secret bridge.
Let us create another script in which the the pin is hiding.
Let the scriptName be invar.sh
where invar.sh means inputVariableScript

#Let this script be use to authenticate users
#This script can be used for ATM access only by TD customers
#!/bin/bash
echo "Please enter your name"
read name
echo "Please enter your pin"
read -s pin
echo "$name, your pin is valid. Login succsessful
echo "$name, welcome to TD bank"
date
whoami

vim invar.sh and copy-paste the above script content.
save-quit by pressing shift+zz on your keyboard
run bash invar.sh and then you will notice that the pin was actually hiding. That makes it a secured script/code

Note; The significant difference between coding and scripting is that,
Coding can help you creates a program while scripting can help you control the program that was created by coding.

Let us create a user account using Dynamic variable also know as input variable.

#This script can be used to create user account only
#You must be the root user or have sudo access to create the script
#Let the scriptName be called user.sh
#!/bin.sh
echo "Please enter the username you wish to create"
read username
echo "The username you have entered is $username"
sudo useradd $username
echo "$username has successfully be created"
whoami
date

vim user.sh
copy-paste the content above and save-quit
run the script sh user.sh
To very the username that you have created, run id username or grep username /etc/passwd

Let us create a secured user account with a password using Dynamic variable.

#This script can be used to create username and its password
#You must be the root user or have sudo access to create the script
#Let the scriptName be called user2.sh
#!/bin.sh
echo "Please enter the username you wish to create"
read username
echo "The username you have entered is $username"
sudo useradd $username
echo "please enter the password you wish to assign to $username"
read -s password
sudo passwd $username
echo "$username with password has successfully be created"
whoami
date

Let us create another user account whereby you can read the password from the standard imput (--stdin)

#--stdin means that, you should read the new password from the standard input which can be piped.
#You must be the root user or have sudo access to create the script
#SLet the scriptName be called user3.sh
#!/bin.sh
echo "Please enter the username you wish to create"
read username
echo "The username you have entered is $username"
sudo useradd $username
echo "please enter the password you wish to assign to $username"
read -s password
echo "password" | sudo passwd "$username" --stdin
echo "$username with password has successfully be created"
whoami
date

Note the above script instruction/script description is called a multi-line comment because it started with >>Std and end with Std. Please take note. Std can be anything. it can be any entity.

SIMPLE ARITHMETIC OPERATION SCRIPT (sao.sh).
1. For a static variable, you have some examples of sao as follows

Assuming x=9. y=3

expr x+y=9+3=12
expr x-y=9-3=6
expr x/y=9/3=2
expr x\*y= means x multiplied by y=9x3=27
exprs x%y- means x mode y= meaning x divid by y and the remainder is what we are looking for here. The remainder is what this expression is interested in.
that is, x%y=9/3=3 R 0   where R=remainder. Therefore, x%y=0
Let say that x=8 and y=3.  8%3=7/3=2 R 1. Therefore 8%3=2. This is the result that linux environment would give to you.

Note; when you have a division expression in Linux, the division has no remainder e.g like expr 8/3=2, the remainder would not show because it is linux environment, 7/5=1    etc but when you run expr 8%3=2 which is the remainder. expr mode(%) means remainder only.
Above is what is called a static variable soa.sh because it does not have the read command which makes it vunerable and not portable for reuse.

Note that 2x2, 2+2, etc would not run in linus environment unless you start it with "expr"

2. Dynamic variable
For example let us create a soa.sh

#!/bin/sh
#let first number be a and second number be b
echo "enter any number"
read a
echo "enter another number"
read b
echo "the total of a and b:" `expr $a + $b`
echo "the difference between a and b:" `expr $a - $b`
echo "the multiple of a and b:" `expr $a \* $b`
echo "the mode of a and b:" `expr $a % $b`

vim sao.sh and copy-paste the above script.
run sh sao.sh

This script is reusable by anyone anywhere in the world. All you need do is to just enter any number you so desired.

Note:  ` is called backtick simbol and it is found left top next to 1 in your key board.

Let us write a script to determine the profit that would resolve from the sale of home in real estate.

# This a real estate script for profit determination
# Let the scriptName be home.sh
#!/bin/bash
echo "How much did you want to sell your home"
read totalSale
echo "How much did you buy your home"
read cost
echo "How much did you spent in upgrading your home"
read marginalCost
echo "The profit you will get from the sale your home is:" `expr $totalSale - $cost - $marginalCost`
echo "The profit you get from the sale of your home is called Revenue"

copy-paste in home.sh and run bash home.sh

The above script is reusable in real estate business anywhere in the worldSS

Another thing that is very imperative to know about Bash_shell_script is called COMMAND LINE ARGUMENTS CLA

Command line arguments cla are set of commands like

echo "$0"  = scriptName
echo "$1"  = first argument
echo "$2"  = 2nd argument
echo "$3"  = 3rd argument
echo "$n"  = nth argument
echo "$#" = display number of arguments
echo "$$"  = display process ID
echo "$*"  = echo "$@" = display all the arguments
echo "$?"  = display the status of the last runned command

Note: * means ycap and it is found in 8 in the keyboard

Example to illustrate the command line arguments above.

Let us write a script for the monitoring of server or to moniter a server in which the guy that is tasked to monitor the server is John and the name of the server to monitor is the webserver in the production environment.

monitor_webserver.sh would be ther scriptName= echo "$0"
John would be the first argument= echo "$1"
Webserver would be the second argument= echo "$2"
Production would be the third argument= echo "$3"

monitor_webserver.sh     John        Webserver          Prod
echo "$0"                echo "$1"   echo "$2"          echo "$3"

USES OF THE COMMAND LINE ARGUMENTS

NOTE: use  If you run a command and it says "command not found"  you can actually know the error code of such command by running echo "$?"

For example, when you run Date or run dAte or run History or run Mkdir Jen, it when say "command not found" because of the fact that most direct commands run in linux are always in lower case letter and when wrongly enter the wrong commands, it give you "command not found" 

Usually, the error code that results from "command not found" when you run echo "$?" is 127
echo "$?" tells you the status of the last runned command.

You can also know the process ID (PID= process identification number) of the server you are using to run the commands by running echo "$$" incase you want to raise a ticket for any error to be resolve by your team members.

Also, when you cat a file that is not in existence in your system, for example, cat jennifer . jennifer file or directory is not in my linux system/environ and so it will say "No such file or directory" when you run echo "$?" it will tell you the error code to be 1

The error code that results from "No such file or directory" when you run echo "$?" is 1

Note: Process ID number PID is the identification number of the command that is being runned in linux or other operating system. For example, you can be  using redhat ec2 instances to be running a command and when you want to find out the PID of the command in the ec2 instances you are using, you can run  echo "$$" 

Every command runned in linux is a process and it has a process ID called PID

Note: error codes most of the time starts from 1-127

When you successful run a command, the error code is usually 0 if you run echo"$?"

if echo"$? = 0   is ok
if echo"$? not equal to 0   is not ok

Let is create a script with the above cla

# Let the scriptName be cla.sh
#!/bin/bash
echo "The scriptName is:" $0
echo "The first argument is:" $1
echo "The second argument is:" $2
echo "The third argument is:" $3
echo "The nineth argument is:" $9
echo "All arguments is:" $@
echo "The number of arguments is:" $#
echo "The status of the last runned command is:" $?
echo "The process ID of the server:" $$
whoami
Date
#if echo"$? = 0   is ok
#if echo"$? not equal to 0   is not ok

copy-paste in cla.sh and run bash cla.sh

You will notice that the second argument, 3rd argument, nineth argument did not display any result because nothing or no entity was assigned to them but the result of scriptname, number of arguments, status of the last command diplayed. process of ID were all shown.

Note. you will notice that the status of the last runned command is 0 because of the position of where we inserted the command. we inserted it just after the number of argument which was a correct command. if we had inserted it just after Date, it would have shown 127 because Date is a wrong command. It is suppose to be date. You will also see that Date is showing COMMAND NOT FOUND because it is a wrong command.

We can assign entity to those arguments if need be. For example

scriptName                First argument     2nd argument       3rd argument
cla.sh                    John                Webserver         Prod

Knowing that first, second and third arguments are john, webserver and prod respectively, you can assign these entities by running the command

bash scriptname firstArgument second Argument thirdArguments etc

bash cla.sh John Webserver Prod

After running the command above, it will now give you all answers expected except the nineth arguments which was not assigned.

Note you can correct Date to date for it to run successfully,

Note; echo "$*" produce the same result as echo "$@" = display all arguments

RUNNING A SHELL_SCRIPT IN A DEBUGGING MODE.
Debugging is the process of identify a problem and fixing/resolving the problem.
You can run a script in debugging mode by running

bash -x scriptName of bash -v scriptName

This helps in troubleshooting. It helps you to see the exact line where you seemed to be having issue so you can find possible way to resolve it. Especially when you have written a bulky script that has many lines like hundreds of line.

For example. Let us write a simple script and run it in debugging mode.

#Let the scriptName be called debug.sh
#!/bin/bash
echo "Happy sunday to every team members present here today"
echo "We are celebrating our successes today in church for those that can make it"
echo "This message is intended for Grace Team Members"
echo "This message is written by:" `whoami`
Pwd
date

copy-paste to debug.sh and run bash -x debug.sh or bash -debug.sh

You will notice that running it in debug mode gives you a detailed information of what was entered on the script initially which eventually shows the echo info and the results of the script when it was now runned.

Note that it is the presence of -x or -v that made it to be running in debugging mode. It gives team members an idea of both what was entered and the result when it has been runned so that they can be able to attest if the info was entered correctly or not and to also know where the problems lies because it aslo quote the line number where there seemed to be a problem "commmand not found" 

INPUT/OUTPUT REDIRECTION
This is a process of redirecting standard output (this is the correct outcome or result when a script has been runned) and standard error (stardard input and containing a line stating where there is error showing commmand not found) to another file when you have also run the command.

This sign for redirection in linux is >  the greater than sign

Let us use the debug.sh as an example of a script.

We are actually redirect the stardard error and the standard output to a file respectively

Let us redirect the standard output to a file called log and standard error to a file called error

i.e   1> log = > log   this is to redirect standard output to log
      2> error         this is to redirect to standard error to error

     > Logs 2>&1       this is to redirect both standard error and ouptut to a file 				  called Logs

     Let us run the script debug.sh by redirecting them as indicated above

    1. run   bash -x debug.sh > log
You will notice that the correct outcome went directly to log file as requested
	2. run    bash -x debug.sh 2> error
You will notice that it will redirect the standard input also called standard error which contain the mentioned line where it says command not found.
	3. run    bash -x debug.sh > Logs 2>$1
	You will notice that it will redirect the standard input also called standard error and the standard output which is the correct outcome to a file called Logs.

	Note: Inpu/ouput redirection is basically used for troubleshooting and for automation.

	comments can also be called metadata i.e  data about a date or information about a data.
	comments are very important in scripting because it gives the script more meaning and also give the direction or instruction of how script would be pilot.
For the sake of revision,
 Type osf scripts are
 1. Shell_script e.g bash. bsh, ksh, csh etc
 2. jenkins_script
 3. python_script/code also known as the groovy script

 USING SCRIPTS FOR PACKAGE MANAGEMENT
 This is a process of writing a script to manage packages by way of installing, removing, upgrading, updating of packages using package managers.

Exampls of packages are
1. nano
2. vim or vi
3. tree
4. httpd called apache webserver
5. jenkins
6. mavins
etc

Example of package managers are
1. yum or dnf for centos, redhat.
2. apt or apt-get for ubuntu, debain.
3. chocolatey or choco for window os
4. brew for mac os
5. pip, helm
etc

The above package managers are used to install packages but are interative in the sense that ther user are asked whether to continue with the installation or not

wget and curl is also an example of package managers because they are used to download/install packages without seeking permission from the user whether to continue the installation or not. They are non-interactive package managers. They are package downloader/installer

You can actually auto-approve installation by passing or adding -y to your installation command. -y means auto-approval as yes and it will now become non-interactive if you do that.

Practically example

#Let us write a script that would install relevant packages for our project
#!/bin/bash
sudo yum install tree -y
sudo yum install vim -y
sudo yum install nano -y
#let us called the script package.sh

The -y is for autoapproval. It will skip the question of whether to continue or not by typing yes or no

copy-paste the above script to package.sh and save-quit
run bash package.sh and the installation of these packages above would start to install

#Let us write a another script that would install http webserver
#let us called the script apache.sh
#!/bin/bash
sudo yum install httpd -y
sudo systemctl start httpd
sudo systemctl enable httpd
systemctl status httpd

copy-paste the above script to package.sh and save-quit
run bash apache.sh and the installation will begin

If you have already have these pacckages mentioned above in your server, you can write a script that can remove and re-install these packages

For example

#Let us write a script that would remove and reinstall relevant packages for our project
#!/bin/bash
sudo yum remove tree -y
sudo yum remove vim -y
sudo yum remove nano -y
sudo yum install tree -y
sudo yum install vim -y
sudo yum install nano -y
#let us called the script package2.sh

copy-paste the above script to package.sh and save-quit
run bash package2.sh and removal and installation will begin

#Let us write a another script that would remove and reinstall http webserver
#let us called the script apache2.sh
#!/bin/bash
sudo yum remove httpd -y
sudo systemctl disable httpd
sudo yum install httpd -y
sudo systemctl start httpd
sudo systemctl enable httpd
systemctl status httpd

copy-paste the above script to apache2.sh and save-quit
run bash apache.sh and the removal and installation will begin

We can access apache webserver from our browser by copying and pasting our serverIpaddress:80 on our internet browser

Note: apache (http and https) is accessed on port 80 on redhat server and port 443 on ubuntu server respectively but https is far more secured than http.

TROUBLESHOOT/SOLVING PROBLEMS.

Interview Question: what problem have you faced and resolved in landmark technology?

Answer:  In lamdmarkTechnology, there was a ticket raised by one of our team member that he was trying to access apache webserver on his browser by his serverIpaddaress:80 but getting errors. He shared his screen and we troubleshot and discover that port 80 was not actually opened on his server. We had to go to the security, clicked on "lauch wizard 7" link, under security group amd go to inbound rules and click on edit inbound rules" clich add rules and highglighted http with port 80. on the CIDR blocks, we highlighted 0.0.0.0/8 to be different from that of the ssh redhat server with port 22 but when we saved, we were still unable to access apache webserver on browser. I suggested that we should follow the ssh redhat configuration format by using same CIDR blocks that was used by shh with port 22. we did exactly that by highlighting 0.0.0.0/0 and save the change. apache webserver was now able to open on the browser.

To know your server public ipAddress in linux, you run curl ifconfig.me

To know your server private ipAddress in linux, you run ifconfig   it will give you your server INET ip address. INET ipaddress is your server private ip address.


FILE MANAGEMENT IN SCRIPTING

Let us write a script that can be used for fileMangement to be accessed via apache webserver.

Let us call the script as filemanagement called filemgt.sh

#!/bin/sh
sudo mkdir /var/www/html/google
sudo mkdir /var/www/html/yahoo
sudo echo "DevOps-google. This script is for the purpose of practising" >> /var/www/html/google/index.html
sudo echo "DevOps-yahoo. Thanks to Prof. Simon" >> /var/www/html/yahoo/index.html
sudo echo "I love DevOps with passion" >> /var/www/html/index.html
# index.html is a default file of apache webserver
# we created an apache webserver google directory and apache webserver yahoo directory with an index.html file inside of both directories.
date

Copy-past the script above in vim filemgt.sh and save-quit
run sudo sh filemgt.sh

Note: If it says that directory is already in existence, you can remove the script by running rm -v scriptName, remove the directory by running sudo rmdir -v directoryName and the remove the file created by running rm -v fileName so that you can repeat the process.

In the above script, the apache default file is the index.html but goes with everything as /var/www/html/yahoo/index.html because it has to pass through web channels. that is, in apache webser, the default file html file is inside a web directory and the web must be inside other web directories in order to function. so when removing the file, you must edicate everything /var/www/html/yahoo/index.html. Also, when removing the google and yahoo directory, you much enter the whole /var/www/html/yahoo and /var/www/html/google.

To access the script on your web page via apache webserver, copy and paste serverIPAddress:80 to read the content of apache default file index.html, severIPAdress:80/google to read the content of the index.html file inside google directory and severIPAdress:80/yahoo to read the content of the index.html file inside yahoo directory

Note: index.html is the default file of apache webserver.

EQUALITY AND INEQUALITY SIGNS IN LINUX; This are arithmetic signs that are accepted in linux environment

1. x -eq y  means x is equals to y
2. x -ne y  means x is not equals to y
3. x -gt y  means x is greater than y
4. x -lt y  means x less than y
5. x -ge y  means x is greater than or equals to y
6. x -le y  means x is less than or equlst to y

CONTROL COMMANDS
These are condition commands/statements in the form of if, elif, else and then. In conditional statement script writing, the content must start with "if" and should always end with "fi"

Note you can use the above accepted equlity and inequality signs that are accepted in linux on condition commands statements if the need arises.
Example;

1. Let us write a script about the accepted school fees need in LandmarkTech
# Let the scriptName be called fees.sh
#!/bin/sh
echo "Please enter your legal name"
read name
echo "Please enter the fee amount that you are willing to pay"
read fee
if [ $fee -eq 3500 ]
then:
echo "$name, you are admitted"
echo "welcome to LandmarkTechnology"
elif [ $fee -gt 3500 ]
then:
echo "$name, you are admitted and your balance would be refunded in due course"
echo "welcome to LandmarkTechnology
else:
echo "$name, sorry you are not admitted and refund would be made to your payment card in a short while"
fi
date

Copy-past the script above in vim fees.sh and save-quit
run sh fees.sh

You can decide  to rewrite or edit the fees.sh and make it more in lesser words by copying fees.sh content to fees2.sh by running cp fees.sh fees2.sh

# Let the scriptName be called fees2.sh
#!/bin/sh
echo "Please enter your legal name"
read name
echo "Please enter the fee amount that you are willing to pay"
read fee
if [ $fee -ge 3500 ]
then:
echo "$name, you are admitted"
echo "welcome to LandmarkTechnology"
else:
echo "$name, sorry you are not admitted"
fi
date

Copy-past the script above in vim fees.sh and save-quit
run sh fees.sh

2. Let us write a script for real estate business for the sale of a house.

Let the scriptName be price.sh

#!/bin/bash
echo "how much was the first customer offering to pay for this property?"
read price1
echo "how much was the second customer offering to pay for this property?"
read price2
if [ $price1 -gt $price2 ]
then
echo "the first customer should be sold the property"
elif [ $price1 -lt $price2 ]
then
echo "second customer should be sold the property who happens to be a better deal"
else
echo "$price1 of the first customer is equals to $price2 of second customer, the property would be opened for any other customer that will pay higher"
fi
date

Note: always put "fi" at the end of the "if-condition statement"

[ ] is used for a condition statement  which are called square bracket 

( ) is called curved bracket
{ } is called a curly bracket


3. let us write a script for a sale of a product
let the scriptName be called amount.sh

#!/bin/bash
echo "How much do you want to pay?"
read amount
if [ $amount -ge 3000 ]
then
echo "The product would be sold and delivered to you"
else
echo "Sorry, what you are offering is not enogh to purchase the product. Thank you"
fi
date

let us write a script for the purpose of command line arguments in Linux. ScriptName be cla2.sh

#!/bin/bash
if [ $# -eq 3 ]
then
echo "The proposal is finallized"
else
echo "Please enter the 3 arguments in order to run this script"
fi
date

If you run sh cla2.sh, it will tell you that "Please enter the 3 arguments in order to run this script"  This is so because there are no arguments on the script. You need to pass or infuse an argumement to the script in order to be able to proceed.
Let us pass "Jude db def" as the 3 arguments required to tye script,

You can now run sh cla2.sh Jude db def
It will now proceed and tell you "The proposal is finallized"
That means that Jude is the person to execute database db script in def environment.

Let the scriptName be called job.sh

#!/bin/bash
echo "Please enter your performance(p)"
read p
if [ $p -gt 8 ]
then
echo "Congratulations, you are hired"
else
echo "Sorry, you do not meet up with the required performance. Another job seeker would be considered. Thanks"
fi
date

Let the scriptName be called tdbk.sh

#!/bin/bash
echo "Please enter your username"
read username
if [ $username -eq 917812345677 ]
then
echo "Please enter your pin"
read pin
echo "You are successfully logged in"
echo "Clifford Egharevba, you are welcome to TD bank"
else
echo "Sorry, you entered an incorrect username. Thanks"
fi
date


Note: httpd means hyper text transfer protocol daemon. httpd is called apache webserver

GIT/GITHUB

As a DevOps engineer, you must know what is expected from Developer to do that will easy collaboration in order to build a good and sophisticated software.
When a Developer has finished coding and writing a code, he is expect to take th good to Githug where other Developers and team members to see.

It there means that Github and its IDEs installation is neccessary here and all DevOps engineers is expected to have a Github account.

IDE stands for Integrated Development Environment
An integrated development environment (IDE) is a software application that helps programmers develop software code efficiently.

Common examples of IDE are: 
VS Code
Intellij IDEA
PyCharm.
PhPStorm
WebStorm
RubyMine

To install git in linux. you run (sudo yum install git -y)

To check the version of git that is running in your linux system. run (git --version)

To install git on your windows by downloading, run  (https://git-scm.com/downloads)

IQ: What is the role of DevOps engineers in terms of Github.

Answer: DevOps Eng. role is baiscally for administration and security. Administration and security here simply means the following;

1. Creating a github account. It is the work of DevOps eng. to create a secured github account. If you newly contracted to engage in a fintach project. Fintech project simply means financial technology from a fintech client link banks, financial institution e.g BOA, TDBANK, VISA, MASTERCARD etc. The possibilty of you creating a github account simply because of that project is about 0.5%. It means you may not be required to do so. Another senior DevOps eng. must have done that before you joined the project.

To create a github account: https://github.com

2. Creating an organization inside the github account in a secured manner: The possibility of you creating an organization is 5% because you may have joined when an organization has already be created.

To create an organization: https://github.com/organizationName

Your organization name is the name of your account in Github

You can also create another organization inside existing organization if you like to and link it to the existing organization and can interswitch anutime. The two organization who have separate team members and repo but can have same thing if you add what is in first organization to the other and vice-versa.


Example of an organization: https://github.com/landmarktechnology

https://github.com/Osas-DevOps  This organization was actually created inside cliffosa22 organization

To access Osas-DevOps: https://github.com/Osas-DevOps
To access Cliffosa22: https://github.com/cliffosa22

3. Creating a team: The possibility of you creating a team is 20%. If you did create a team, you are required to add other members of your team by sending them request and if they accept your requested, they will become members of your team. It is most possible that a team has already been created for the project before you were contracted and what the team lead will do is to add you to the team.

You can add someone to your team by searching for the person default github organization name or github username:

i.e  https://github.com/username

To create a team: https://github.com/organizationName/teamName

Example of a team: 
https://github.com/landmarktechnology/teamName
https://github.com/osas-devops/visa_team30


4. Creating a github repository. The possible of doing this is 90% and so it is expected that you create a github repository where Developers can push the codes they have written to it for further collaboration
You role may entails;
a. Assigning roles to team members for the project
b. You may also be adding external collaborators.
Sometimes, you may be assigned a role also by another team member or team lead.

To create a github repository: https://github.com/organizationName/GithubprojectRepo.Name

Example of a team: https://github.com/landmarktechnology/visa
My newly created repo link:  https://github.com/Osas-DevOps/VisaRemoteRepo

visa is the RepoProjectName which the code that has been written by the Developers would be pushed to.

How do i create a repo.? 
Click on the logo or face of your organizationName.
click on "create a new repository"S.
write in your repo.Name: This must partially reflect your projectname for best practise.

After the repo has been created, you can decide who you want to grant access to see whats in your repo. You can grant access of your repo to collaborators and teams by simply adding them if need be.

Note: The github repository is the remoteRepo that the Developers written codes would be pushed to.

IQ: In your own word, What do you understand by Repository?

Answer: A repository is like a folder or directory where files and other IT contents are kept for current and future usage.

Commands In Git.

Before you start running any command in git, 

1. first of all know if git is installed in the system by running "git and when it says "command not found" it means tha git is not install but it give detailed info about git, it means it is install.
2. Creat a directory that partially reflects what your project is all about, for best practise and also create a file containing the code, inside the dir.
3. cd dirName so that the system will now be in the environment of the new directory that was created.

Running a command in git, it usually starts from git before any other. That is, git status etc.
4. run ls -a   to list files that are hidden. Hidden files begins with a duct sign. example .fileName (.deploy, .cv, etc)

5. Run Git status; If you run git status and it says fatal: not a git repository or parent directory, you know that git is not the one that is managing and controling any of the file or directory or repository.

For you to bring any repo. or directory under the management of git, you would have to initialize it by running git init

Git is very important because it has the capacity to role-back to old version. This is true because git has keeps records of any activities or any modification that happens to a file or code.
Git keep tracks of happenings in the environment.

Git triggers system integrity and trust because git can trace anyone that does a commit.

Examples in git command

Let us create a file called case. run

1. echo "DevOps is such a lucrative and stressless type of job due to its automation 	tendency" > git.txt

Note: 
> means redirect: 

a. it is used to create and add content to an empty new file. As in the case above.

b. It can also be used to create a file without adding content. That is, 
echo "" > test
case and test here are they file names

c. it can also be used to replace the content of a file with a new content in the 		same file.
	This means that when you run

	echo "DevOps is cumbersome to learn" > case

	You are redirecting "DevOps is cumbersome to learn" to an existing file called case. It will replace "DevOps is such a lucrative and stressless type of job due to its automation tendency" with "DevOps is cumbersome to learn"

	You can verify this by running

	cat fileAname

2. echo "This is why I love DevOps" >> git.txt

>> is called append: 
a. it means to add more content to an existing content in a file as in the case of example 1 and 2.
b. it can also be used to create an empty file by running 
echo "" >> test.git
c. it can also be used to add content to an exiting file with no content

The >> and > almost does the same thing but the major difference between >> and > is > can replace the content of a file with a new content if the same file but >> cannot replace the contents for a file but only add more content.

Let us now proceed. Let us create a directory called "mscard" and and cd into master to make it a mscard directory and create a file called case that we have already created:

run ls -a to check if there are hidden files and if there is no hidden file, we can move to running a git status. Follow the explanation  have outlined earlier in the above.

Git status; If you run git status and it says fatal: not a git repository or parent directory. You know that git is not the one that is managing and controling any of the file or directory or repository.

For you to bring any repo. or directory under the management and control of git, you would have to initialize it by running 

git init

After you have run git init, run git status again and it will tell you 


fatal: detected dubious ownership in repository at '/opt/maven-web-application'
To add an exception for this directory, 

run below command to clear this error

git config --global --add safe.directory /opt/maven-web-application

and when you re-run   git status, it will tell you

"on branch master, no commit yet, untracked files etc"

WORKING AREA
When a Developer starts or begins to write code, the area or spot in which code been written at that point is said to .be in the WORKING AREA: This is an area where developer starts writing codes.

git add fileName  (if you want to add just one file to the staging area)
git add .  or  git add * (if you want to add all the files present in the dir. to staging area)

Notes; files in staging area appears red

STAGING AREA: This is a place where developer move the code that was written into it. When developer are done with written code, the move it to the staging area to reaccess the code that he has written.

Note: before a developer commit, it is very neccessary that the developer must configure what is called "Git Global user's details".

Git Global user's details: 
is a configure that would be done by developer in the form of authentication. This is very important because when this is done, it would be easy to track whoever commit the project by the time it gets to remoteRepo. This triggers some level of integrity
GGUD is done by running:

git config --global user.name "CLIFFOSA22"
git config --global user.email "cliffeg1@gmail.com"
git config --global --list

and then run  
git status to know if the configuration was successful. If it is, it will display the username and address of the developer.

This is why you cam easily know whoever has commit a file.

The next thing to do is to run

git commit -m "my first commit" 

-m means you are passing a message

The above command would move the code from staging area to localRepository
Local Repo. is your physical computer repository. If you are satisfy with what you have done.

The next thing to do is to run

git log     This will bring out details of the developer interms OF the name and email address based on what you have configured in GGUD and also show you the CID (commit ID) of the commit that just took place.

Note: To exit git log, type q

For you to know what really take place after the project file has reached the remoterepo and to know who indeed run the commit, you run the command

git show commit-ID

Just copy only 7 digits of your commit-ID and run the command, it will give you details of the person that commit the project. This is why GGUD configuration is very important for any developer to perform before he commence to commit the codes that he has written to localrepo

IQ: what does it mean when you run git status and it shows: 
On branch master
nothing to commit, working tree clean

Answer: It means that there no more codes/files left in the staging area and working area. it is empty. It is a best practise for developer to leave the working tree or environment clean

Note : Files in staging area appears green

LocalRepository:
If the developer he is fully satisfy that the code that he has written is good and ready to go ahead. run 

git push

The above command would move the code from localRepo to remoteRepository
Local Repo. is your physical computer repository. If you are satisfy with what you have done.

RemoteRepository: This is the place where codes are pushed for other collaborators like other developers and other team of work can brainstorm and delibrate on the code that was written by running.

git push

From the localrepo., the developer  would push the codes to the remoterepo in github by running

git remote add <name> <url>

and then push using the remote name

    git push <name>

Note: RemoteRepo. is hosted by github or gitlab. We have repo inside github where written codes can be pushed

Example 1:
We already have a file called git.txt that has been initialized and now become git repo which means that the file git.txt is now been managed and controlled by git.

It is now presently in working area with red color and when we run 
git add git.txt, it moved to staging area where it turned green.

Note: When you want to git add to move the code from working area to staging area, You do the following;

git add . or git add *   or  git add (fileName or scriptName) 

git add . or git add *   is used to git add all the files moving them from the working area to the staging area

You can also move a file/codes from the working area to the staging area and from the staging area to the localrepo in just a single command by running

git add. && git commit -m "content of the message here"
git add fileName or scriptNmae && git commit -m "content of the message here"

git add (fileName or scriptName)  is used to git add a particular file moving it from the working area to the staging area.

Before we went further to commit, We first of all  performed the GGUD configuration called git global user's details configuration i.e

git config --global user.name "Osas-DevOps"
git config --global user.email "cliffeg1@gmail.com"
git config --global --list

Then we run 
git commit -m "my first commit"  

To move the codes/file(git.txt) to localrepo

Example 2

Let us create another file call in the directory git1
#let the file be a shell script called tdbank.sh
#!/bin/bash
# Let us create a script that can be reusable
# Create a script with scriptName td.sh
echo "Please enter your name"
read name
echo "Please enter your pin"
read pin
echo "$name, welcome to TD bank"
echo "$name, your $pin pin is valid"
echo "Enter an amount to withdraw"
read amount
echo "Cash withdrawal of $amount is completed. You can please remove your card and cash"
echo "$name, thank you for banking with us"
date
whoami

vim tdbank.shS and paste the aboce script and save-quit

run sh tdbank.sh

let is initialize the shell script tdbank.sh so that it can now be managed by git, by running

git init

It will initialized the file/script tdbank.sh because that is the only file that is left in the working area

if you run git status, it will let you know that it has been initialized

Move the script to staging area by running

git add tdbank.sh

Before we went further to commit, We first of all  performed the GGUD configuration called git global user's details configuration i.e
git config --global user.name "Osas-DevOps"
git config --global user.email "cliffeg1@gmail.com"
git config --global --list

to move the script to localrepo, run

git commit -m "my second commit for practise purposes"

run git log to know your commit-ID 

run "git show commit-ID" to know the details of the developer that git commit the written code 

Note: Coding is the process of writing a code

Git push

The DevOps engineer will share what is called PROJECT REMOTE REPOSITORY in Github to the developer and ask the developer to git push the code he has writtehn to it.

In my github, the name of the projectremoterepo is VisaRemoteRepo because i am working on tdbank visacard

Let us now push all the commits to projectremoterepo for further collaboration by other team and developer for review you run

git remote add (the link of your projectremoterepo)  i.e

git remote add https://github.com/Osas-DevOps/VisaRemoteRepo

Note: (the link of your projectremoterepo) is called the BRANCHNAME also known as the MASTER

Alias:
You can decide to add alias name to the branch name to replace the branch name because of the fact that the branch name which is the link of the project remotre repo may be difficult or cumbasome to use to avoid mistake. i.e

let the alias be va  the run

git remote add va https://github.com/Osas-DevOps/VisaRemoteRepo

when you run above command, https://github.com/Osas-DevOps/VisaRemoteRepo is now va

the link of your projectremoterepo is now va

For you to know if the alias name us did worked, run

git remote -v      this would list all the aliasname you have in your repository

run  git push aliasname master  i.e

git push va master

when you do that, it will tell you to enter your github username and then ask you for password.

Your github username would be your organization name where your projectremoterepo was created. this is Osas-DevOps

To get our password, you would have to generate what is called PAT ie personal access token. To do this,

Click on your photo id or logo at the upper right side in your github account and the scroll down and click settings, then scroll down and click on developer setting at the left down and then click on personal access token and it may ask you for your password of your githug account. Put in your regular github account password and it will now allow you to go further. It is like authentication, and then click on token classic and the click on generate token and then copy the token link.

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


and paste it on the place they were asking password in linus git environment. once you put in your password, and press enter, you have finally push the writen code to the remote repo which is your projectremoterepo in github

You can now go to your github account to verify this. You will see all the commit done and who did the commit. Since the code.files are now in remoterepo, other team members and developers can now collaborate or review.

Note: When you want to git add to move the code from working area to staging area, You do the following;

git add . or git add *   or  git add (fileName or scriptName) 

git add . or git add *   is used to git add all the files moving them from the working area to the staging area

git add (fileName or scriptName)  is used to git add a particular file moving it from the working area to the staging area.

You can also move a file/codes from the working area to the staging area and from the staging area to the localrepo in just a single command by running

git add. && git commit -m "content of the message here"
git add fileName or scriptNmae && git commit -m "content of the message here"

Example 3.

#Let us create a python script with scriptName and finally move the script to remoterepo for further collaborations.
#Let us call the python script release.py
print ("this script is for practise purpose")

First of all check if python3 is up and running in your system by running 

python3
python3 is the version of the python we are looking for here

if it says "command not found"
You will know that python3 is not running in your system

Then, run

sudo yum install python3 -y

it will automatically become running in your system

vim release.py

copy and paste the above script to it and then save-quite

If you run 

sh release.py   it will not run or it is wrong because sh is bourne shell which makes it a shell script but we are dealing with a python script here.

To run a python3 script, you run

python3 release.py

The next thing to do is to move the file from working area it is now to staging area and from staging area to localrepo in just one simple command. That is

git add . && git commit -m "this is a python3 script commit"

First of all run git status and it will tell you that there is untracked file release.py with red color. That means that the file is still in staging area.

Since we have already run the GGUD configuration before, it means that the developer's details is already on the git repository or git directory and so there is no need to perform the GGUD configuration, we can ahead.

Note: once you have initialized git in your working area or the directory you created that made it to become git repository, any file or code henceforth would automatically be initialized and you do not have to be initializing at anytime you have a new file on that particular git repo or directory that was initialized by git.

run

git add . && git commit -m "this is a python3 script commit"

and then run git log to know who actually did the commit. 

Note: if you want to leave the page after running git log. You just type
q  and it will leave the git log page

The next thing to do is to push the code to remoterepo for further collaboration by running

git push aliasname master

aliasname here is called va

git push va master

The code will be pushed to the projectremoterepo in your github called VisaRemoteRepo

Note: The va replaced (the link of your project remote repo which is https://github.com/Osas-DevOps/VisaRemoteRepo) and this same https://github.com/Osas-DevOps/VisaRemoteRepo is the branchName also called master

The project name here is VisaRemoteRepo

Note: You can only run the command
git remote add (link of your project remote repo) and then, git remote add aliasname (link of your project remote repo)  when you create a new project remote repo in your github account

When you finally run git push va master, it will ask you for username and password.
Your username is the exact organization name that has the project remote repo. Here, mine is Osas-DevOps and your password would be the same personal access token called PAT you created on your github account.

You can always run 

git show commit-ID 

to know the details of the developer that commit a particular project or written code so long you know the commit-ID of such code.

You can actually know the commit-ID by running git log

IQ: How do you edit your github organization name from private to public?

Answer: If you want make your github account public if it was a private mode before. You do the following:

Under your repository name, click Settings. scroll down below Under "Danger Zone", click Change visibility. Select a visibility. To verify that you're changing the correct repository's visibility, type the name of the repository you want to change the visibility of.

IQ: what do you use for versioning in your projects?

Answer: We use git/github for versioning in our projects.

IQ: In your own word, what is versioning?

Versioning is a process of improving and upgrading on an old operating systems, software artifacts and web services by a developer to make it more sophisticated and perform better though targeted doing almost the same task but in a more better way.

Note: It is at the locarepo that versioning is created because that is actually where you have the commit-ID, the details of the developer who comitted the codes and so on.

Are developers considered software engineers?
Software developers design specific computer systems and application software. Software engineers work on a larger scale to design, develop, and test entire computer systems and application software for a company or organization—software development is a subset of software engineering.

	VIDEO 45. GIT/GITHUB CONTINUATION

Looking at the diagram in my note. Landmark Software Solutions LSS was contracted by Tesla to deveolope an application. 
Tesla is the client here. and LSS is using Github as the source code manager SCM

LSS has team of engineers like DevOps engineers, developers and software engineers they are working with. 

In the diagram, We have about 3 developers called Simon, John and Tolu and each of these developers has installed gitbash in their system.

These developers started writing code individually in their working area, and then add the code to their staging area where they review the code they have written by running git add . and if convinced is good, would commit the code their local repo where versioning is created by running git commit -m "content message here" and from their push the code to project remote repo called Tesla in Github by running git push aliasname master

We have about 2 DevOps engineers in the diagram and their aim is to onboard the project by creating organization, create teams and create a projectRemoteRepo called Tesla in Github and grant developers access to push their respective codes where other developers or teams members can review and collaborate.

Note: Versioning in the localRepo as soon as a code/file is committed to the localRepo. This is so because it is at the local repo that a commit ID is create for each file/code that is moved to localrepo. It is also at the localRepo you can actually determine or know the person who committed the code running git show commit-ID.

In LandmarkTechnology, we use git for versioning.

GIT PULL, FETCH AND CLONE

#Let us create a java file called list.java inside our git1 directory by runing 
cd git1 and the create the file
#vim list.java and write 3 lines of names 
Simon
Peter
James
#save-quit
#Run (git add . && git commit -m "list of developers") to move the code that took place in the working area to staging area and to localRepo
#Let us create a projectRemoteRepoName as Tasla in your github account under your organization
#We will create an aliasname by running (git remote add ta (link of the tesla remoteRepo) ) because we have created the project remoteRepo as Tesla since Testa is the client which is git remote add ta https://github.com/Osas-DevOps/Tesla
#To know if the aliasname ta was successful done run git remote -v
#Run (git push ta master) to push the codes to The Github projectRemoteRepo called Tesla.

After you have git push, it will ask your github username which is the exact organization name where the exact remoteRepo is. It is in Osas-DevOps and then ask of your password which is your PAT i.e personal access token which is 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  and if you paste the token, it will not show because password do not show in git environment just like linox

If you run git log, you will see the commit ID of tesla and also the details of the person who did the commit. and then tap q to leave the page 

In a situation whereby other developers has also done some work and commit them to Github. Since you are compiling lists of developers, you can include the developers names to the once you have before by

1. click on tesla and then click on the fileName called list.java it will show you the names of developers we currently have
2. go to your right side and click on the inverted triangle sign 
3. click on the edit this file and then type in the names of developers you want add to the list you have before like Loveth, Osas. let say loveth and Osas on separate line.
4. click on commit changes and the changes would be activated

This mean that changes has been carried on the the file in the remoterepo call Tesla

If you cat list.java you will notice that the changes made was not captured in your local environment or localRepo.

For the changes to be captured in your local environment, there are 3 methods that can be used to capture the changes that took place in remoteRepo to be shown in the working environment and local environment. These are

1. git pull: This will move changes in the codes/file in your remoteRepo directly to your working area.
You can pull or move the changes in the remoteRepo by running 

git pull ta master  you can confirm by 
cat list.java   then the  changes will show

If you run git log it will tell you that list.java updated

2. git fetch: This will move changes in codes/file in your remoteRepo to your local Repo for you to examine the changes by running 

git fetch ta master 

and then cat list.java but you would not see the changes in the code from remoterepo in your local enevironment. For you to see the changes, you can now run 

git diff ta/master  

and then you can now see the changes that is missing from your localrepo which usually starts with a minus sign. And if you are satisfy with the changes and willing to apply the changes, you can now decide to merge the changes by running

git merge ta/master

If you run git log it will tell you that list.java updated

3. git clone: This will move all the codes/files in your remoteRepo to the working area of another new directory/working environment.

git clone LinkOfRemoteRepoName NewDirtory i.e git clone LinkOfTesla NewDirtory which is 

git clone https://github.com/Osas-DevOps/Tesla NewGitDirtory
NewGitDirtory is a working environment of git

Note: To remove a directory, run

rmdir -v dirName or rm -rv dirName

To create a directory, you run

mkdir -v NewDirName

dir is a short for for directory.

IQ: Why do we create files or task done using CLI and not GUI?
Answer: It is better to run task or create files using CLI (command line interface) because it makes work done faster, easier, better and more accurate that using GUI (grapgical user interface). In DevOps, it pays better when you use CLI.

IQ: Why is it not recommended for DeVops or Developers to create file using GUI?
Answer: It is not advisable for developers to create fils using GUI because when developing, there is what is called IDEs (integrated development environments) that is been achieved by CLI when you are creating files which cannot be achieved by GUI and this IDEs make work to be quick and fast for developers to run more than one line of code.
Using GUI would make developers to be writing one line of code
Developers always use IDEs when developing or writing codes.

Some Examples of IDEs are as follows:

1. Vscode = Visual Studio Code also known as VS Codium is fully open-source software binaries of VS Code licensed under the MIT license. With VS Codium, developers do not need to download and build from the source. This is an open source IDE
2. Pycharm: PyCharm is a cross-platform IDE that provides consistent experience on the Windows, macOS, and Linux operating systems. PyCharm is available in two editions: Professional, and Community. The Community edition is an open-source project, and it's free, but it has fewer features
3. Eclipse = This is an open source IDE. Eclipse is an open source community whose projects are focused on building an extensible development platform, runtimes and application frameworks for building, deploying and managing software across the entire software lifecycle
4. MyEclipse = This is not an open source IDE. You gave to pay
5. Intellij = This is an open source IDE. IntelliJ IDEA is a cross-platform IDE that provides consistent experience on the Windows, macOS, and Linux operating systems. IntelliJ IDEA is available in the following editions: Community Edition is free and open-source, licensed under Apache 2.0. It provides all the basic features for JVM and Android development.
7. Atom: Atom was a free and open-source text and source code editor for macOS, Linux, and Microsoft Windows with support for plug-ins written in JavaScript, and embedded Git Control. Developed by GitHub, Atom was a desktop application built using web technologies.
8. NetBeans: This operating system is very heavy. Throughout its history in Sun Microsystems and Oracle, NetBeans has been free and open source and has been leveraged by its sponsor as a mechanism for driving the Java ecosystem forward. In 2016, Oracle donated the NetBeans source code to the Apache Software Foundation.

Note In git, When developers are developing using IDEs, there are some files and directory that are automatically created which was not intentially created by the developer. The unwanted files and directory that are automatically created are:

.classpath, .project and node_modules  respectively. 

These above files are usually empty files.

node_modules is a directory while .classpath, .project are files

In git, If the above files are directory are created automatically when developers are using IDEs and these files are directory are not part of the project that the developer is working on. Such files would be moved to .gitignore file

IQ: What is .gitignore file?
Answer: .gitignore file is a file that records or store all files that developers do not want git to track. So any files that you do not want git to track should be move to .gitignore file which you can create if you do not have one.

Now, let us assume that we are using IDE and so let us manually created the files and directory  .classpath, .project and node_modules respectively as if it was IDE that created them by running

touch .classpath && touch .project     to create the two files or you can just run 
touch .classpath .project     or  without && to create the two files since it is the 								   same the same command "touch"

You can remove the two files at once by running 

rm -v .classpath touch .project

v = verbose mode = that is, it is letting you know what just happened in a verb

then to create the directory by running

mkdir node_modules  

You can create both files and directory using one command line by running

touch .classpath .project && mkdir node_modules

Note: Any files that starts with duct sign before the file is called an hiddden file and it does not show or seen when you run "ls" but the file is secretly existen

If you run "ls" ,  you would not see  .classpath  and  .project  that was created because it started with a duct sign

Note: Any fileNames that start with a duct sign is called a hidden file and it does not show or is not viewable when you run "ls" but the file is secretly existen

In git, for you to see a hidden file, run git status and you would see the hidden file starting with a duct sign and are they are usually red in color indicating that it is an untracked files in the working area. run

git status     and you will see all the hidden files in red color

Since the files and directory .classpath, .project and node_modules are not part of the project we are currently working on, we have to move the files/directory to .gitignore file

Let us create .gitignore file since we do not have any, by running

touch .gitignore


and then move the files/directory one after the other to .gitignore by running 

mv UnwantedFileName .gitignore  i.e

mv .classpath .gitignore
mv .project .gitignore

If you run 

git status

You will notice that you no longer see the unwanted files and it is only the .gitignore file and maybe some other files that are untracked that are now present which is in red color.

You can go ahead to move these files from their working area where they are now to staging area and then to localRepo by running the command with just one command line below

git add . && git commit -m "git ignore file"

Note; if you want to delete a file/code from the localRepo. You run

git rm fileName/code  where rm means remove

As soon as you do this, the file will leave local repo and come to staging area.

If you want to unstage the file/code or remove the file/code from staging to working area. You run

git restore --staged fileName

Note: You can decide to leave any file/codes on your .gitignore file if you do not want to push them to remoteRepo where other developer can collaborate.

ATLASSIAN TOOLS
Atlassian is a company that has some sophisticated softwares that are been utilized by DevOps engineers in running task.

Examples of Atlassian tools are:

1. BitBucket: This is a source code manager where developers push codes for further collaborations. BitBucket functions like Github but we usually use Github as our SCM
2. Jira: Jira is an Atlassian tool using for Ticketing. Jira is used to raise ticket to assign task to team members.
3. Bamboo: This is used for DI/CD automation, for automating workdone. It function like the Jerkins. We usually use Jerkins for automation
4. Confluence: This is used for documentation

IQ: What Atlassian tools have you been using to run your Projects in landmark?
Answer:  We have been using the following Atlassian tools below:
1. Jira
2.Confluence

BRANCHES IN GIT
IQ: what are branches and how many branches do your company supports?

Answer: Branches are used to create lines of developments. Therefore. a branch is a line of development.

Our Company, Landmark supports 3 branches. These are:

1. Development branch aka Dev branch
2. Staging branch
3. Master branch

To check the branch that you are currently on in your git enviroment. run

git branch

If what you have in your git environment is master? You can decide to create another branch from the master branch.

You can be able to know the branch that you are currently on in your git environment if the beginning of the branchName starts with a star or asterick and the branchName is green, it means that the green colored branchName with asterick is the branch that you are.

Example 1.

In this master branch, Let us create a file called deploy1.java by running

vim deploy1.java

and add contents with separate lines
i.e

This is a jave file
Deployment would be carried out here

and then add and commit the new file in order to move the file from working to staging and to localRepo by running

git add . && git commit -m "java deployment"

Note: Always add and commit anytime you create a new file with contents or codes

Let us create another branch or line of development called development branch (dev branch) from the master branch since you are currently in master branch with asterick sign at the beginning of master. Before you create another branch, run

ls   and take note of the files and directories your have in your master branch.

Then run

git branch NewBranchName i.e
git branch dev

If you run

git branch   to know how many branch you have now, you will notice that the master is green and starts with asterick and the dev is white but no asterick. This means that we are in the master environment.

If you run 

git log

You will see the last commit that was carried out and you will discover that it now has master and dev environment or branch

Note: The last branch is always the branch or environment that we are. 

Note: Since the dev was created from the master, the dev would have every contents, files or directories that is in master. 

If staging branch is created also from the master, it will also have everything that is in master.

Switch branches: that is, the tendency of moving from one branch to another. if you want to move from one branch to another. for example, let us switch from master to dev. We run

git checkout dev

As soon as you do that, you are now in dev branch.

Since we are now in dev branch,  ls to see all the files you have.

First of all, let us cat deploy1.jave  you will notice that deploy1.java in dev has the same content with the same deploy1.java that is in master.

Example 2

let us add more content to what you have before in deploy1.java in a separate line in dev environment  by running

echo "deployment3 in progress" >> deploy1.java

if you cat deploy1.java, you will see that line 3 has been added

if you run

git status

It will tell you that file modified

You would need to add and commit the changes made in deploy1.java in order to move from working environemnt to staging and then to localRepo by running

git commit -a -m "deployment 3"   			in branching. Take note of this commands.

Tha above command will automatically move file.code from working area to staging and to localRepo. This only applies to git branching.

If you run git status, it will say clean tree.

If you run git log, it will show what has just been committed with commit-ID

If you switch back to master by running

git checkout master  and then cat deploy1.java

 you will notice that the changes that was made on deploy.java in the dev branch is not captured on the same deploy.java in master branch.

 You can actually would be able to make it captured or see in master by running

 git diff dev

 If you run the above command, it will show you what is missing in deploy1.java in master with a minus sign at the beginning of a red colored line of content

 If you are ok with the changes that was made and would like to proceed with the changes, you can run

 git merge dev

 That would merge the changes in the file in dev to master

 After doing the above, the changes would now reflect in deploy1.java in master

 Note: To delete a branch. run

 git branch --delete branchName  i.e

 git branch --delete dev

To verify if the branch was deleted successfully, run

git branch -a

Example 3

Let us go back to dev by running

 git checkout dev

 Let us add another line of content to deploy1.java  i.e

echo "deployment4 activated henceforth" >> deploy1.java
and then add and commit by running

git commit -a -m "deployment 4"

if you switch to master and run ls, you would see the file but you would not find new contents that was added when it was in dev. Then run

git diff dev              to see the changes that is in dev that was not capturd in the master branch

and the run 

git merge dev    

to merge the changes made on dev to master so that the changes can now be captured in master.

Example 4

Let us remain in master branch and add more content to the file by running

echo "deployment5 just commenced" >> deploy1.java

WHen you run

Cat Deploy1.java and then run git status

You will see tha content that was added and the status would say that there is a file modification untracked but you would need to add and commit the changes or modification made by running

git commit -a -m "The 5th deployment"

Note "a" in the above command represents add

If you run git status, it will say working tree clean

Let us switch to dev branch once again my running

git checkout dev

cat the deploy1.java 

You will discover that the modification that was made on the file/code in master was captured in dev

Note: Any file content modification made in master environment would be captured in dev and staging environemnt as long as the dev and staging environment was created from the master.

MERGING CONFLICT
Example 5

Right here in dev branch, Let us add another content to the file by running

echo "deployment6 has been trigered" >> deploy1.java

WHen you run

Cat Deploy1.java and then run git status

You will see tha content that was added and the status would say that there is a file modification untracked but you would need to add and commit the changes or modification made by running

git commit -a -m "The 6th deployment"

Note "a" in the above command represents add

If you run git status, it will say working tree clean

Let us switch to master branch my running

git checkout master

cat deploy1.java 

You will discover that the modification that was made on the file/code in dev is not captured in master. This is simply because dev was created from master and so any changes in dev would not automatically reflect in master but any changes in master would automatically reflect in dev.

If you run 

cat deploy1.java

you would not see the content that was added to the file/code while in dev Unless you run 

git diff dev

to show you what is in dev that is not capture in master which appear as a minus sign at the beginning of the content

Before we go further to merge the changes or modifications, let us add another content in the file while in master to establsh what usually lead to MERGING CONFLICT.

echo "deployment8 on the run" >> deploy1.java

WHen you run

Cat Deploy1.java and then run git status

You will see tha content that was added and the status would say that there is a file modification untracked but you would need to add and commit the changes or modification made by running

git commit -a -m "The 8th deployment"

Note "a" in the above command represents add

If you run git status, it will say working tree clean

Let us now merge the files changes made in dev to master. You may have to check the difference in content between them to know and review the changes to determine whenther you would accept the changes on not by running

git diff dev

After running git diff, you would discover that it will show you a minus sign red content and a postive or plus sign green content. That tells you that the content with the minus sign is the missing content in master that is in dev while the positive green content is the already committed content that was added to the file in master. 

If you go ahead to run

git merge -m "message" dev

It will say Merge conflict in deploy1.java

As a DevOps engineer, the best practise is to avoid merging conflict


What is a Merge Conflict?

Merge Conflict as the name implies is the conflict or error that arises in git when you are trying to merge the difference in the content of one branch to another branch.

How to resolve Merge Conflict

You can manually resolve merge conflict by

vim deploy1.java

delete  "head", "arrows or greater than sign" and "the double dash long line"  completely

and then, save-quit

If you run

git status

It will tell you modified file untracked. You know that if any modification is made on a file, that file must undergo git add and git commit by running

git add . deploy1.java && git commit -m "merge conflict resolved"

If you now run   git status, it will tell you working tree clean.

Note: You can only run the below command in git

git commit -a -m "required message content here"  

when you want to add and commit a modified file/code to staging and to locaRepo

You can only run the below command in git

git add fileName && git commit -m "required message content here"  

for a particular newFile/newCode

git add . && git commit -m "required message content here"  

for all the newFile/newCode

VIDEO 46, GIT 3

Software or applications are developed by the writing of codes by team of developers. In most cases, application contract acquired is always a long term deal and it is required that, such application would be experiencing updating, upgrading, fine-tuning and changing of features as time goes by, in order to enhance user's experience and more effective functionality. This is what is called versioning. Where, you have something like old and new version of an application/software.

For example, The way yahoomail displays it features in 2006 is not the same user experience as it is now in 2023. A lot of updating, upgrading and feature changes has taken place over the years.

Yahoomail log in and every activities that goes on there, is a collection of written codes that was put together by some team of developers. 

Yahoo may contract Landmark Software Solution LSS to develope an email application for them. The email application that would be developed by LSS will called yahoomail application and so as a DevOps eng. It is required that you craete a secured enabling environment for the developers to commit a good and quality written code to.

Let us assumed that you have just been hired as a senior DevOps Eng. by a company. It is very important that you know the versioning tool that the company uses. If it differs from the versioning tool you know about, you can advise them about the versioning tool called GIT. 

Let them know the excellent quality of using Git for versioning. For example, 

1. Git has the tendency to track and keep records of all the happenings/activities that took place when developing the appplication. Git keeps records of old and new version of any developed application 

2. Just incase the Client change his/her mind and ask for the old version of the application to be deploy, Git would be able to roll-back all old application and deploy them if need be. 

Git can roll-back and roll-out any version of the application.

3. There is data integrity using git for application development in the sense that before a developer commit a code, it is imperative that he should run a Git Global User Details configuration (GGUD) in order for the developer's information like email, name etc to be captured and so it would be very easy to Identify the exact developer that wrote/committed a particular code anytime, anyday.

Question:
How many branches do you supports?

Answer; In Landmark, we support a minimum of 3 branches namely:

Development branch
Stage Branch
Master branch

but a branch can also be created by a developer in our team if the developer as a ticket for a task like "bug remover" If the developer is tasked to remove a bug from a code, the developer can create a new branch called bug_fixed branch from the master branch in the production environment and once he is done, he will create a pull request and assign the pull request to members or obligated members of the team for review, test and if the team is satisfied with the bug removal, they will commit the changes to remoterepo master for production deployment.

The developer cam delete the bug_fixed branch as soon as the codes has been deploy to production because the branch would no longer be useful.

The same process also happen when a developer is tasked to add features to the code. He will create a new branch called feature_(name of the client). After he has written/modified the code that would add more features to the code. he will create a pull request and assign the pull request to member of the team for review, test and if the team is satisfied with the features added, they will commit the changes to remoterepo master for production deployment. The developer cam delete the features branch as soon as the codes has been deploy to production because the branch would no longer be useful.

Question:
How many environment do you supports?

Answer; In Landmark, we support a minimum of 4 environment namely:

1. Development env
2. Stage env
3. Testing env
4. Master env

Note

1. Development env:  Development branch deploys development env
2. Stage env:  Stage branch deploys stage and testing env
3. UAT/QA/BA env:  stage branch also deploys to UAT/QA/BA env.
4. Master env;  master branch deploys to production env or master env. also known as customers env. where customers can access and take advantage of the application

UAT= User Acceptance Testing: This is also called application testing or end-user testing. It is a phase in software developement whereby an application is been tested by end users or client to see if it is actualkly what they bargained for. This is customer friendly.

QA= Quality Assurance: This is a phase in software developement where the QA team assures clients that the application is up and running, free from error after been and of good quality as required or demanded by the client after being tested by the QA.

BA= Business Analyst: This is position in software development where a person or a team responsible in contacting end-users or people for the testing of the new or upgraded application to see if it is good enough for the market. 

BA is also responsible in pointing out the application requirements and stick closely to ensure that the application developed, reflects what it is been set as system/application requirements. The BA work directly with stakeholders. The BA writes the overall documents for system requirements of the application and ensure that the new developed application is inline with the system requirements.

LET US RUN SOME EXAMPLES.

Let us log in to our system, switch to git1 where I did have some git files in my system and run git branch to know the branches you have in your system.

If you have dev and master branch, you can delete any of the branch if need be. for the sake of example, let us delete the dev branch by running

git branch -d branchName  that is,
git branch -d dev

if you now run git branch, you will see that dev has been deleted. And so you only have 1 branch called "master" that is now left.

If you are in master and you want to create another branch,

If you want to create more branch, you can run

git branch branchName

Let us created 2 branches called development and stage by running

git branch development
git branch stage

When you run git branch, you will see that stage and development has be created.

Since the 2 branches you created were created from master, it therefore means that evcerything files/codes that is in master is captured in stage and development branch.

To justify this, let us run "ls" and take note of the contents in master and run "git log" to know that last commit that was carried out here, then let us switch to stage by running

git checkout development               and run "ls"
git checkout stage                     and run "ls"

You will see that they both have all the contents and commits in master branch.

If you are in stage branch. run 

git status

You will notice that there is a modified file called .gitignore file with red color that has not be committed to localrepo.

Let us created another file called info.dev by running

touch info.dev

If you run git status, you will notice that we now have we now have 1 modified file and 1 new untracked file that is being track by git.

If we do not want git to track the new file that was created if it is not part of our project, we can move or record the new in .gitignore file.

Note: Any file that you do not want git to track should be recorded in .gitignore

vim into .gitignore and type in info.dev and save-quit by pressing shift+zz in you key board.

if you now run git status, you would notice that you can no longer see info.dev and so git would not be able to track the new file but iit was recorded in .gitignore

Let us vim into the new file info.dev  and add the content "information needed by developers" and save-quit

if you cat info.dev   you will see the above new content  but when you run git status, you will not see the new file still.

Note, any modified file you see is waiting for add-commit to localrepo. let us commit the .gitignore file to localrepo by running

git commit -a -m "info_developers"         -------------commit 1

a= add to staging
m= commit message

if you run git status, it will say working tree clean

We have a file called deploy1.java

Let us add more content to the file by running

vim deploy1.jave and type in

deploy to stage env
deploy to dev env
deploy java application
deploy python application
grafana

save-quit

if you run git status, it will say modified files, which being tracked by git.

You would have to add and commit the changes to localrepo by running

commit -a -m "deployment to stage env"   			---------------commit 2

Let us modify the deploy1.java once again my adding more content to the file

echo "this is a great job" >> deploy1.java
echo "tested and accepted" >> deploy1.java

save-quit the file

cat deploy1.java to confirm

when you run git status, it will say modified file

To add and commit to localrepo, run

git commit -a -m "tested and accepted by the client's QA team"  ----------commit 3

Let us switch to master by running git checkout master

if you run git log, you will notice that the commit that was created in stage is not captured in master. For is to be captured, you need to run what is called git merge or get rebase or git cherry-pick.

In real situation, after a developer has finished modifying a code as tasked by the company in the stage branch, he would create what is called git pull request where team members can compare and review by running

git diff stage

If the team are satisfied with what they see and want to merge or integrate it or them to master for possible production deployment, you do 3 things: git merge. git rebase. git cherry-pick

1. Git Merge: This will merge the all the commits in stage branch to become one commit in master branch. This means that if you have 1000 commits in stage, git merge will mmake them to become just 1 commit in master. By running 

git merge stage

Diadvantage of git merge is that it make one not to be able to know thedifferent versions that took place in stage. because each commits is a version of the code

2. Git Rebase:  This will integrate/merge individual commits in stage or any other branch to an individual commits in master. This means that if 1000 commits in stage, it will also form 1000 commits in master. By running

git rebase stage

3. Git Cherry-pick: This is the process where team member picks or choose any of the commits from stage or other branch and merge it to master if the commit-ID of the code in stage is known. you run

git cherry-pick commit-ID

first of all, if you run  git diff stage since am in master, a red minus sign at the beginning of a red content/s would appear indicating that the file needs to exeperience add and commit,

NOTE: Any correction/modification to be made to any code is usually done at the stage branch by the developer and create a git pull request where other team member can review and if satisfied would merge the changes to master branch for possible deployment to production or to end-users. It is not a best practise or not advisable for Modification to take place at the master branch.

It is only a new projects that takes place at the development branch.

QUEST ASKED IN CLASS: 
What if there are more than one modified file?
Answer:
Let us switch back to stage branch.

We have a file called monitor.app  with contents

prometheus
grafana
appDynamics
newrelics

vim monitor.app   and add 
dynatrace
save-quit

if you run git status, it will say modified file.

Add and commit by running

commit -a -m "dynatrace addED to APM"    -------------commit4

APM= Application Performance Monitory

if you git status, it will say working tree clean.

Let us go back to master by running

git checkout master

if you run git log  you will notice that the 4 commits we just did is not captured in master branch. This means that the modifications will did in stage is not captured in master. 

for clarity. cat monitor.app   you will notice that dynatrace is not included.

For team members to review what was modified on the code is by running

git diff stage

it will show you what is in stage that is not in master. It starts with a minus sign and the content/s is always red in color.

After the tema's assigned person/s has reviewed and accepted, they can decide to merge or rebase or cherry-pick the modifications made.

Note: during git merge, if any conflict message arise, you can resolve the conflict manually by vim into the file and delete anything that is not part of the content or codes. something like a long greater than sign, long dash lines etc.

If the modification is what is expected to be done, a team member can do any of the following:

1. Merge the modification
2. rebase the modication
3. cherry-pick the modification

Since we have done merging before in one of our examples, let us consider rebase or cherry-pick the modications done in the stage to master.

Remember Rebase will make individual commits or integrate the modifications in stage to be created as individual commits in master But Cherry-pick would only pick the one you prefer among the modifications made to form a single commits.

Lets us first of all cherry-pick any of the modifications on stage by running

git cherry-pick (commit-ID of the preferred commits)

Note: to leave git log page, you press  q 

Note: to remove the last commit from a branch, you run

git reset --hard HEAD^   it will now go back as an untracked file

To push the codes/file from locarepo to remoterep in github, you run the following

git push alisaname branchname

Note: Since we have codes in the 3 branches in our localrepo and we only have 1 branch called master in our remoterepo, that is active or existing in github. We can decide to create development and stage branch in github also when we push all the codes in the 3 branches in our localrepo to our remoterepo in github by running

git push aliasname--all

aliasname here is ta

this is the reponame link: https://github.com/Osas-DevOps/Tesla

PAT         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

If you execute the above, 2 new branch with contents would be created in your remoterepo and master already existen in your remoterepo but its contents modified or added. You can verify by going to your remoterepo at github and click on your repoName and click on branch , you will see that 2 more branch is created and you can also check the contents. It will be the same content with the once in localrepo.

The above command can only be possible if you already have aliasname.

Aliasname is also called origin.

You can be able to know your aliasname to your remoterepo by running

git remote -v

Note: it will ask you for username and password. Your username is your organization name and your password is your generated PAT in github. PAT= Personal access token

For example

Before we go ahead with the GUI processes after 2 new branches with content has been created in remoterepo from localrepo. Let see another example below.

Cliff is a developer who was hired by Tesla and Cliff has been working on a an application called monitor.py Tesla contracted LSS to collaborate with Cliff.

Since Cliff has been on the project, 

If LSS would be approaching the project for the first time, what LSS would do in his environment is to clone the remoterepoLink of Cliff by running

git clone GithubRemoteRepoLink

Let us say LSS is using windows as in local environment, using git bash

ls if you will found a file called tesla by running

ls tes*     this will bring all file that starts with tes
* means ycap

before cloning the remoterepo of cliff called tesla

if there is no file called tesla, then you can go ahead to clone by running 

git clone GithubRemoteRepoLink


Run Git status; If you run git status and it says fatal: not a git repository or parent directory, you know that git is not the one that is managing and controling  the directory.

For you to bring any repo. or directory called Tesla that you cloned, under the management of git, you would have to initialize the repo by running.

Since you are using git bash the first time, you have to initialise the folda or directory or repository called tesla by running

git init

Note; You must be inside your project directory called tesla before running git init

if you decide to create a branch, it will tell you

fatal: not a valid object name: 'master'

This means that you have to run git commit command before you can create a branch from master. Run

Remember that you already have a directory called Tesla that is already untracked. You have to add and commit that file/directory to stage and to localrepo by running 

git add . && git commit -m "my first commit in windows using git bash"

if you tap enter key, it will tell you

*** Please tell me who you are.

Run

  git config --global user.email "you@example.com"
  git config --global user.name "Your Name"

to set your account's default identity.

This means you have to set up what is called GGUD called git global user's details by running

git config --global user.name "Osas-DevOps"
git config --global user.email "cliffeg1@gmail.com"
git config --global --list

It is same process with what you did in linox server when you were initializing and registering.

You can now add-commit the directory tesla by running

git add . && git commit -m "my first commit in windows using git bash"
  if you run 

  git staus

 working tree cleam

If you wish, You can now create other branch here by running

git branch development
git branch stage

It is not a good practise to be working while in master branch but you can it is neccessary to always clone remoterepo to master and then go to stage to do any neccessary modification and then merge or rebase or cherry-pick to master.

Note: Downloading and Installing gitbash to your windows makes your windows to function like linox

Then let us clone our the remoterepo of cliff by running

git clone GithubProjectRemoteRepoNameLinkOfCliff   i.e

git clone LinkOfTesla
git clone LinkOfTesla

If you execute this, and run ls

you will notice that LSS's developer has cloned the project Tesla.

Note: This project called Tesla is a directory or repository. It is not a file and so you would have to cd into tesla before you can cat any of the file inside the Tesla repo or directroy.

Another example

Let us assume a ticket was raised in one of LSS developer's Jira account with ticket number 555 to fix a bug in a code/file called  monitor.app

The LSS developer will have to create a branch while in tesla repo environment by running

git branch bug_fixed_555_monitor.app 

from master branch and then run 

git checkout bug_fixed_555_monitor.app 

and the Directory or repo called Tasla would appear. run
 
 ls

and you will see that all the files in tesla in master would be captured in the new branch here and then fix the assigned file/code called monitor.app

vim into monitor.app  and let us just add one line to assume we have indeed remove the bug

#bug removed   

and then save-quit

add-commit the modification by running

git commit -a -m "bug successfully removed"

And then push the codes to remoterepo in github be running

git push aliasname branchname      for a particular branch

In this case, you will push to bug_fixed_monitor.app_555 which will automatically create bug_fixed_monitor.app_555 in your remoterepo by running

git push aliasname new branch

git push ta bug_fixed_monitor.app_555

it will say 

$ git push ta bug_fixed_monitor.app_555
fatal: 'ta' does not appear to be a git repository
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.

To know if we have any aliasname here. run

git remote -v

it says 

origin  https://github.com/Osas-DevOps/Tesla (fetch)
origin  https://github.com/Osas-DevOps/Tesla (push)

The above means that you do not have an aliasname that would replace the origin (https://github.com/Osas-DevOps/Tesla)

To register an aliasname just as we did in linox server by running

git remote add aliasname https://github.com/Osas-DevOps/Tesla

git remote add ta https://github.com/Osas-DevOps/Tesla

Then,

git push ta bug_fixed_monitor.app_555

sINCE YOU ARE USING WINDOWS, IT WILL ASK YOU TO LOGIN TO YOUR github ACCOUNT and click on authorization before the command you did above will complete it running.

LSS would go to his github account and would click on branches and would notice that the new branch was indeed created and can also see all the files and the modification that was carried out in the branch using CLI.

USING GUI FOR MERGING IN GITHUB

MERGING
After LSS has pushed the bug_fixed_monitor.app_555 into his remoterepo, he will have to merge or integrate the changes in bug_fixed_monitor.app_555 to master because the modification made is not yet captured in master and other branches. You can verify that by clicking on master or other branches and verify by checking the file/code monitor.app

To merge, 

1.LSS would switch to master because that is where deployment usually take place and 
2. the click on compare pull request, 
3. click on new pull request 
4. and then click on compare, to compare the modification 
5. and then highlight the new branch
6. write on the message box if need be. (bug removed, please review)
7. click on reviewers and then tick or select the name of the members of you team that is obligated to always review modifications
8. Finally click on create pull request. All the members you selected would automatically be sent an email regrading what you just did.

If Cliff was one of the team member that was selected, he would receive an email about the pull request. He would do the following

1. click on the link that was sent to him and he will see where it stated please review which is the pull request message. Under that, you will see the branchName that was created for the possible of fixing the branch. Click on the branchName and scrolll down, you will see the modifications in green color that was done to the file called monitor.app
2. He will review the modification carried out on monitor.app and when he is satisfy and want to adopt the changes, he will
3. click on merge pull request
4.He will leave a message behind like (reviewed done, good job)
5. click on confirm merge.

After Cliff has merged the pull request, if LSS go back to his github accout, he will notice that the a new line is now visable or in real time, he would notice that the bug has been indeed removed.

Since LSS has completed his task for removing the bug, he can go ahead to delete the new branch because his aim has been accomplished. 

By clicking on the branch and then click on delete located at the end right side with a sign that look like a covered basket.

Note; To delete a branch created from master using CLI, you would have to switch to master and then run

git branch -d branchName

I.Q

What are your branching strategies?

Answers;

In Landmarktech, we maintain a minimum of 3 branches. Nmaely

1. Development Branch and codes in the developmenmt branch are further deploy to development environment
2. Staging branch and codes from staging branch are further deployed to staging and uat/testing/qa environment
3. Master branch and codes in the master branch are further delpoyed to production or customer environment.
4. We can also create what is called a bug_fixed branch when we are tasked to remove bug from a code
5. We cam also create a feature_branch whyen we are tsked by those who hired us to add features to any code/software.
6. Also, in each of the branches, when using CLI, we also maintain a minimum of 4 areas to arrive at a branch, these are:

-Working area; When a developer starts writing/developing codes and when he is done, he will push to the code to the staging branch.
-staging area: This is Where a developer begin to test the codes he has written when it is goopd enough, he can the go ahead to commit the code to his localrepo
-local area: This is where a written/modified code that was committed stays in a form of version. the written code in the local environment which has details of the developer should incase a rollback is needed. This is the place where versioning of the code is created with the evident of a commit-ID

-remote repos: This is the place where codes are finally pushed. Remoterepo is the scm where developers finally pushed codes to for further collaborations. Example of remoterepo are github, gitlab, bitbucket, aws commit etc.

These are our branching strategies.

Note;  It is not a good practise to modify a code from master. If you are tasked to modify a code for exanple, to fix a bug or add features to a code, it is best practise that you should create another branch from the master so that automatically the new branch will definitely have the code you want to modify, modify the code and then push the branch with the modified code to the remoterepo where your team can see and review the modification and if they are satisfy with the modification, they can merge or integrate the modified code to master.

CLASS QUESTION

Why do we have to fix bug in a code in the production environment. Why not in the staging environment?

Answer.

This is because the code that has the bug that is to be removed is already being deployed to customers for usage and so customers are alreading using the code. Knowing fully well that whatever that is in the master branch can easily be deployed to production and easily be accessed by customers. The code or application that has the bug is still running and so we do not want to cut off the running of the application, So therefore, there is no need for you to start fixing the bug or remove the bug from the staging branch. and if you do, It will take more time for the entire process because in staging environment, you have the uat,QU team and also the BA. They would have to test and discuss and agree about the modification before pushing again to master. That usually takes time and since this is an inhouse technical issue, there is absolutely no neeed for any uat or qa to test the modification.

For best practise, it is better to fix the bug in the production environment but since you do not want customers to see what is being done whyfixing the bug in the code, you would have to create another branch from master where you can fix the bug and then push it to remoterepo where your team or other collaborator can review and if satisfied, they would integrate the the modifications to master for further deployment to production.

Note: Once you craete a new branch from master, you are automatically out from production enevironment and so, any modifications you are doing would no longer be visible by customers until your team integrate it to master.

Please refer to the drawings in your note book which diagraphically illustrate the branching strategies.

ADDING FEATURES TO A CODE/APPLICATION

In real time, When a developer wants to add features to a code, after he has created a feature branch from the master, he can then run a git pull command to move it to staging because this is a very important aspect where every stakeholders need to see, review and test the new features. In staging, every kinds of testing is carried out like uat team, qa team, ba team. After this is done and everyone is satisfied, the new features can now be push or integrated  to master for production deployment.


Let us assume that a ticket was raised witha ticket number 123 sent to LSS jira box to add features to a code, monitor.app in  Atesla project.

The LSS developer already has tesla repo in his envuironemnt in windows, he will have to create a new branch while in tesla repo environment by running

Note: If the repository you want to work on e.g tesla has not be clone please clone and cd into the repo you just clone and then initialize it.

git branch new_feature_123_monitor.app  from master branch and then run 

git checkout new_feature_LSS_123_monitor.app 

Note, as soon as you create a branch in master, you have left the production with a copy of everything that is in master to be seen in another branch while the exact code that is to add features is still running in production but a duplicate is now in the new branch.

ls

and you will see that all the files IN master would be captured in the new branch here and then do the assigned modification by adding new features

let us vim into monitor.app  and let us just add two lines to show that we have indeed added feature to the code.

print( "car servicing automated )
print( "hello $user, your car is due for servicing" ) 

This is a java script

and then save-quit

add-commit the modification by running

git commit -a -m "car features added please review"

And then push the codes to remoterepo in github be running

git push aliasname branchname      for a particular branch

In this case, you will push to new_feature_LSS_123_monitor.ap which will automatically create new_feature_LSS_123_monitor.ap in your remoterepo by running

git push aliasname new branch

git push ta new_feature_LSS_123_monitor.ap

To know if we have any aliasname here. run

git remote -v

it says 

origin  https://github.com/Osas-DevOps/Tesla (fetch)
origin  https://github.com/Osas-DevOps/Tesla (push)

The above means that you do not have an aliasname that would replace the origin (https://github.com/Osas-DevOps/Tesla)

To register an aliasname just as we did in linox server by running

git remote add aliasname https://github.com/Osas-DevOps/Tesla

git remote add ta https://github.com/Osas-DevOps/Tesla

Then,

git push ta new_feature_LSS_123_monitor.ap

LSS would go to his github account and would click on branches and would notice that the new branch was indeed created and can also see all the files and the modification that was carried out in the branch using CLI. LSS can now run a pull request which has already shown on the github account and then write to message like, "please review the new features" and go to reviewers and the slect the people that he wants them to review the changes so that people can reviem the differences and then go ahead to merge or integrate the changes to master if they are satisfied. Note, anytime you run a pull request and highlight the people that you want them to review the changes made, a message would be sent to the people in form of a link in ther email box and they will click on the link and review the changes and if satisfied would merge the changes

USING GUI TO CREATE A NEW FEATURE BRANCH AND MERGE

1. You can go to yourn github account, and create new_feature_111 from master by clicking on the master
2. the type new_feature_111 in the box that we display.
3 click on create new branch 
4. click on view file while you are in new_feature_111
5. click on the inverted triangle on the right hand and click on edit
6. and them add new feature that you want to add. Like

print( "email security automated )
print( "hello $user, someone is trying to log into your Gmail box from another ip addtess" ) 

Scroll down and you can go ahead to commit the change and write in a commit message like

email sercurity added

and then commit the change and 

7. click on pull request and you will see that it will say compare pull request and leave a comment like please review the features added. thank you. 
8. Click on the reviewers and select those you want them to review the changes and click on confirm. an email would be sent to the people and they can review and the leave the satisfactory message and then go ahead to merge on integrate the changes to master if they are satisfied.

You can go ahead to delete the feature branch since is no longer useful for now.

TO DELETE BRANCH GUI

1.By clicking on it says number of branches. since it is 5 branches now. it will say 5 branches. click on the 5 branches
2.  and then click on delete located at the end right side with a sign that look like a covered basket.

TO DELETE BRANCH IN CLI

git branch -d branchname

Now if you go back to your windows. You would not see the two last changes that was done did not reflect in CLI IN WINDOWS because it was done in GUI. if you want to see the changes. just simply run

git diff command i

git diff new_feature_LSS_123_monitor.app

git merge -m "merging to master" new_feature_LSS_123_monitor.app

This merging was possible because new_feature_LSS_123_monitor.app with the modified file monitor.app is still in this gitbash window but has been deleted in github. 

I.Q

WHAT IS THE DIFFERENCE BWTWEEN A GIT AND A GITHUB
answer; 

A git is an open source versioning tool used to record changes made on file/code and can also be able to track file and rollback if need be. It also guarantee data integrity and speed.

A github is an open source repository that host git. It is an open source code manager that host git. It is a scm like gitlab, aws code commit, bitbucket etc.


GIT TAG

Tag is like almost the possite of git branch in character but very alike in terms of running the command

1. TAG cannot be modified after craetion
    but branch can be modified.

2. TAG are created when you are about to do a product release
   but branch are created from the beginning of development

3. a new TAG can only be created from the master
  but a new branch can be created in any branch

4. Tag can be created by running
   git tag tagname
   but git can be created by running
   git branch branchname
5. Tag can be push to remoterepo by
    git tag aliasname tagname      for a particular tag to remoterepo  or
    git tag aliasname --tags       for all tags to remoterepo

   but branch can be push to remoterepo by
    git branch aliasname branchname     for a particular tag to remoterepo  or
    git branch aliasname --all       for all tags to remoterepo

    Note: Tags are created when you want to do a product release.

Let us create a tag in windows or in linux

git tag tagname

git tag teslav1.0.2

1= higher version
0= lower version
2= patch
v= version

git tag teslav1.0.2

run git tag to know if the tag was created

When this is done

move the tag to your remoterepo by running

git puah aliasname tagname

git push ts teslav1.0.2

If you go to your remoterepo, clig on tag and you will see that the tag teslav1.0.2 has been created.  The tag file is a zip or tar file of what is in tesla repo and these files and folder cannot be modified.

If you want to created a new release, click on release and create a new release 

 click on release, name the release like teslavi.0.2 release  and leave a comment like Initiating teslav1.0.2  and select the tag you want to release e,g teslav1.0.2  and the publish pre-release

You can create a high version of a software and the create another tag that will reflect that you created a higher version like teslav2.0.2

You want to create a lower version, it will be teslav1.1.2

let us create a higher version by created teslav2.0.2 in github by using CLI to create the tag and the push it to github tesla repo

You can also go ahead to release and publish  and click pre-release


VIDEO LINK FOR GIT 4. LAST PART OF GIT

https://www.youtube.com/watch?v=3YgutlExHRo


GITHUB is a tool that is being used by many developers no matter where they are located, they would be able to collaborate with each in software development toi access their code they are developing or modifying. So Github is very important because github repo is what we are going to be using till the end where we have different repo for any task our clients obligated us to do and where other collaborators can also be able to collaborate.

Understanding how git andd github works is very important because that is what is going to be used at work.

FRom now on, we are going to be looking at github from the Devops eng. point of view.

Note:

Codes in the developement branch are deployed to development env.
Codes in the stage branch are deployed to staging, QA, uat or testing env.
Codes in the master branch are deployed to production or customer env.

Please note that github is a remote server

GIT STASH COMMAND

This is a command used in backing up or saving an uncompleted code writing in order to attend to other urgent task and you cam easily come back and begin from where you stopped without any lost of information in the unfinished code writing.

During the process of writing code as a developer or you are writing a code in your working area and you suddenly received a ticket in your Jira account from your senior Manager maybe a DevOps eng. that you should stop whatever you are doing and that you should quickly resolve a very pressing urgent task like maybe to fix a bug in a code.

In this case, you have no choice than to stop whatever you are doing and quick resolve that situation. In doing that, you should be able to back-up the codes that you have started writing so that you do not start all over when you are now ready to continue writing the code.

Example

Let us go to our linox server or window gitbash server.

Assuming that you received a ticket in your Jira box that you should fix a problem about the inablilty of end users to access the data base that you designed as a developer which is very urgent. 
    
    FOR NEW FILE/CODE 

If you are already writing a code in your code in your gitbash window server when the urgent message came. Code in your server Like example

vim into names.sh

#!/bin/bash
echo "100 names of engineers needed for the task are below:"
echo "Charles David Osas"

You are not done with the 100 names you are typing and some other features you want to input on the code and you need to backup what you have started so that you can continue from there when you are done with the urgent task.

What you need to do is to

1. save and quit what you are doing by pressing   ctrl + zz in your keyboard to save and quit
Run git status and it will tell you that a new untracked file
2. add and commit the code to staging and locarepo by running  

   git add . && git commit -m "names to deploy"   to move to locarepo in order to have a commit ID

3. Go back to to the file or code by running  vim  names.sh

4. Modify the file by adding one more name or anything to the code so that the file/code can replect as a modified file in order to be in the working area and then save and quit.

5. Run   

git stash

As soon as you execute this, your uncompleted code/file will be backed up but when you run git status, it will tell you working tree clean

So you can now switch to order branch if need be to resolve the problem" to resolve the issue

FOR A CODE UNDERGOING MODIFICATION 

If an existing code with commit ID is already undergoing any form of modification when ticket was urgent for an urgent mattera to be resolved by you.

1. Just save and quit and Run git status to know if the code that you are modifying would deplay as modified file in your working area

If it did not, you have to add and commit to move it to staging and to localrepo area. and try to remodified by adding anything and then save and quit so that it will now desplay as modified file in your working area.

But if it shows as modified file in your working area,

For both cases, the last thing to do is for you to run 

git stash       for back up

If you run git status, it will tell you that mothing to commit, working tree clean

This became so because i have backed up the uncompleted file/code. WIP file meams work in progress file/code


Note: To be able to back up a file/code by running a git stash command,

1. The code nust be in the working area
2. Th code or file must appear as a modified file/code in the working area which means the file must have had commit ID before modification began

TO RESUME WORK WITH THE BACKED UP FILE

If i am now ready to resume work with the code/code that i backed up before, I will switch branch to the place where the code/file is located. Maybe in the development or stage branch or other branch you created that has the file and then I WILL SIMPLY RUN THE COMMAND

git stash list

This will list all the files/codes that i backed up

If you run git stash list  it will show only one back up we did, it will show like

stash@{0}


TAKE NOTE OF THIS

It is normal that you are going to have lots of backup files up to 100 or even more with about 100 lines of codes or more  for each files/code in realtime but each of the file has an Indices number that will appear like the following bellow if you run git stash list

stash@{0}    
stash@{1}
stash@{2}    
stash@{3}
stash@{4}    
stash@{5}   and more

Looking at the bove backups, stash@{5} is the most recent backed up file/code

1.  If you want to resume work with the backup file stash@{0}  You can run

git stash apply stash@{0}

2. If you want to resume work with the backup file stash@{3}  You can run

git stash apply stash@{3}  

This is applicable to others

3.  If you want to resume work with the most recent backup file,  You can run

git stash apply

Looking at the bove backups, stash@{5} is the most recent backed up file/code

DELETING A BACKUP FILE

4. If you want to delete a particular backup file after resumption and done with the work with of the backup file,  You can run

git stash drop stash@{index number of the backupfile}

If the index number of the file is stash@{3}   You run

git stash drop stash@{3}     that will delete the backup file  stash@{3}

5. git stash drop    will drop or delete the most recent backup file


APPLYING OR RESUMING AND DROPPING OR DELETING A BACKUP FILE AT THE SAME TIME

Restoring the backup and at the same time deleting the backuo, you run

6.  git stash pop stash@{index number of the backupfile}

If the index number of the file is stash@{1}   You run

git stash pop stash@{1}     that will apply and drop the backup file  stash@{1} at the same time

7. git stash pop    will apply and drop the most recent backup file


When you run the above command, it will bring all the modified file/code-name to the working area where you can continuing working on the file/code by vim into the file/code  e.g vim names.sh

In realtime, there are ticket that are of higher priority. The higher priority tickect would have to be attended to before the one with less priority. P1 ticket is of high priority than P3 ticket. So backed up codes/files would be very common in realtime working env.

P1- PRIORITY 1

P2= Priority 2

NOTE: You cannot be able to backup a secret file or a hidden file. That is a file that is inside .gitignore  file. That is because it can not be tracked by git and so git stash would not be able to backup such file. If you do not want a file to be tracked by git, the file should be recorded or typed in  .gitignore file

Note: {}  means curly bracket
      <>  angle bracket
      []  square bracket
      ()  simple bracket
      #!  means shebang

 Note: For the sake of CPU effective utilization,  it is good that you  drop or delete some backups file/codes that is no longer needed in your system because your cpu space maybe overloaded or occupied with little space left. You need to drop some files to free some space in your cpu so that it can have more space for other important projects.

 IQ

You maybe asked what command do you use to backup or save work when you want to switch to another branch to urgently execute a task.

Answer: The answer is,  git stash command


OTHER IMPORTABNT THING YOU NEED TO KNOW IN GIT THAT ARE VERY USABLE FREQUENTLY

GIT CHERRY-PICK

This is used to merge or apply a particular commit in one branch to another branch if you know that commit-ID of such commit

If you are obligated to merge a particular commit of a task that took place in another branch like stage or developemt or etc to master branch. You can run a git cherry-pick command so that you can only copy only the exact commit of the branch to master. 

Example

If you are obligated my your senior colleague to create a list of newly employed DevOps engineers.  What you will do is that you would

1. create a new branch like database branch from master if you feel like creating a branch with such name and at the same time switch to the new branch by running

git checkout -b database

The above command will create a new branch and also switch to the new branch at the same time.

Note; If you want to create a new branch and at the same time switch to the new branch, you run the command

git checkout -b newBranch

2. vim list.sh  to create a file called list.sh  and type in

  echo "List of newly employed senior devOps engineers in Landmark Software Solution'
  echo " Andrew "
  echo " Angela "
  echo " Wilson "
  echo " Cliff "
  echo " Uche "


Then, Save-quit

also, 

vim love    and paste in the below message.

echo "I love DevOps Engineering work"
 

3. add and commit the file to staging and to localrepo by running

Since it is a new file and not an existing file undergoing modificatiobn, you run

git add . && git commit -m "newly employed Senior Engineers"

If your team lead has reviewed the names listed and asked you to go ahead to merge only that commit to master,  What you will do is that you would

1. swich to master by running  

git checkout master

2. then run 

git cherry-pick (commit-ID of the exact commit required that is in database branch)

if the commit -ID ia 7e7e444eee992ff5, pick only 7 digits  and run 

git cherry-pick 7e7e444

That will merge only that commit in database to master

If you run git log   you will see the exact commit that was in database branch now in master branch

Note; This implies that, An engineer can be able to merge another COLLEAGUE'S "commit", to the commits in his OWN branch if he knows the commit ID of his colleague

Note also that if you run 

git merge -m""     

it will merge all the commit in one branch to become one commit in another branch


Note;  If you want to clone a remoterepo to window with gitbash installed and you do not want the repo to go inside other folda or directory but want it to be on Desktop, you run

cd ..                 cd  means change  directory

NOTE; If you clone any remoterepo to your system directory or desktop, you need to cd into that repo as soon as it is in system so that you can be able to access or see all the codes or files in that repo.
  NOTE; if you run 

  git remote -v    to know the aliasname of the remoterepoName you are working on, and it says

  orgin: link of the remoteReponame

  The origin is the default aliasname


  NOTE: ls -ltr   means list all files/directory in the order of the latest or most recent file/directory.

  GIT CLONE

  Git clone is used to copy the a remote repository with the entire content in it from github to another working area of a system like windows with gitbash installed and linox with git installed.

  Assuming a new developer was employed and the developer is expected to take part in a project that has been existing and he has been obligated to modify a code.

  Since it is an existing project, the new developer would have to clone the remote repo of that project and so the team lead may have to send the new developer the URL or link of the project remote repo

  NOTR; URL = UNIFORM RESOURCE LOCATOR; this can be called a link or an address that we take you to a webpage.

  As soon as the new developer receive the url of the project remote repo, he would first of all clone the repo by doing the following;

  1.   According to Prof., if you are using windows, make sure clone the repo to your window desktop by first of all running

  cd desktop

  2. git clone linkOfProjectRemoteRepo

  3. run

     ls -ltr    to locate the most recent file/directory

  3. cd into the project     if the projectName is ebay, then 

     cd ebay/

  4. ls            to locate or see the file/code that you want to modify

  5. vim into the file that you want to modify

  	NOTE; 	Where ever git is installed, you can be able to download or clone a remoterepo to it. Be is solary system, linox system or windown and others.

  6. modify the file and the save-quit 

  7. add and commit the modified file to move to localrepo in order to have a clean working tree. by running

  git commit -a -m "comment here"   since we are modifying or it is a modification file

  if it is a new file, we run   git add . && git commit -m "comment of what you did here"

  8. The new developer want to push the modified code to remoterepo of the team lead say in github by running the command but would want to know the alaisname of the remoterepo of the team lead. Without asking the team led about the remotrepo, if he run

  git remote -v   it will say

  Origin   URLOfTheProjectRemoteReposOfTheTeamLeadThatHeCloned
  Origin   URLOfTheProjectRemoteReposOfTheTeamLeadThatHeCloned

  NOTE; Origin here, is the default aliasname of the cloned projectremoterepo
        URL is the link of the of the cloned projectremoterepo

git push aliasname master

git push origin master

Note;   Origin   is equivalent to URLOfTheProjectRemoteReposOfTheTeamLeadThatHeCloned   so either you use origin or URLOfTheProjectRemoteReposOfTheTeamLeadThatHeCloned

You also put master at the end because the clone was done on master branch ans so it should go back to master branch. 

You can also push it to stage branch of the team lead for further review and testing before the team lead or any team member with escallated privileges can merge or integrate the changes to master.

 NOTE;  

 If the new developer runs that above command and it says, remote: permission to   URLOfTheProjectRemoteReposOfTheTeamLeadThatHeCloned denied to the new developer and URL error 403. 

This means something. This is an IQ. 

 IQ

What does error 403 means?

Answer; 403 is an http status code that means, FORBIDDEN.

In respect to the new developer, it is possible that the teamLead has not granted thE N.D permission to push the modified code to his github remoterepo. He has to be granted the permission or the privilege to push the code.     N.D here mean new developer.                                                                                                                        
We need to know the meaning of some error code generated from our system anytime we run any command.

This http status code is many atimes asked in IQ.  IQ means interview questions

SOME HTTP STATUS CODES

100 AND ABOVE= INFORMATIONAL

100= CONTINUE
101= SWITCHING PROTOCOL
102=  PROCESSING

200= OK
201= cREATED
202= aCCEPTED

200 AND ABOVE= SUCCESS

200= OK
201= cREATED
202= aCCEPTED

300 AND ABOVE= Redirection

300= multiple choices
301= moved permanestly
302= found

400 AND ABOVE= Client's error

400= bad request
401= unauthorized
402= payment required
403= Forbidden
404= Not found

500 AND ABOVE= server error

500= server error
502= Bad Gateway
503= service unavailability
504= Gateway timeout

NOTE; The projectremoterepo URL simply means the gateway or the link to the projectremotrepo that was created.

8. Since the N.D was unable to push the code to teamlead showing error 403 meaning access forbidden. This simply means that the teamlead need to grant the N.D access or grant him some privileges to be able to push code to his remotrepo.

Note; tHE TEAM LEAD for the project might be a senior devops enginerer. We as devops eng., it is not our duty to write code but we make sure that we grant a developer or the right person access to pushing any code. We make sure that the security and classified info of the project and the organization is well kept and mannaged. security here is our top priority so that nothing would go wrong.

What the teamlead would do is to go to his github account and do the following;
a. TEAMLEAD would go to the projectremoterepo in his github account
b. click on settings, that looks like three duct signs at the right top side
c. click on manage access
d. enter your password
e. click on invite team or people. In this, enter the person or people"s organization name you want to grant access
f. It will ask you to choose a role. Choose a role means what type the access do you want to grant the team member you want to add to the projectrepo. Any role or roles you choose for the team member (the new developer) is the permission you have given to such team member to be able to do on the project.
In the case of the N.D, since you want him modify the code, it there means that you would aloow him "write access" then click on "write"
g. As as the write access is be highlight and click allow access, an email would be sent to the N.D and once he except the invitation, it will take him to the remoterepo of the teamlead and he would be able to access the teamlead remoterepo.

USING GUI

NOTE: If your team member or teamlead obligated you as a developer to modify a code and he went on to grant you an access to write and a link was sent to you to accept the invitation. As soon as you accept the invitation, let say it was a github account, the link will take you to the projectremoterepo of the teamlead. In this case, cloning the projectRemoteRepo would no longer be neccessary because you can now have access in GUI to modify the exact code in the PRR by clicking on the exact code in the PRR and then click on edit and then edit the code.

PRR here means project remote Repo and it would be used henceforth for abbreviation purpose.

1. As soon as you have modified the code in github, you can scroll down and commit the change by by typing in your commit message in the commit box and hen click on commit change.

Note; Since you have a "write access" to the teanlead PRR, you can also be able to create a new file in the PPR and write some code or script writing.

IQ
In what situation is it right for you as a developer to use git clone, git pull and git fetch?

Answers

1. Git clone: would clone the project remote repository of a team or teamlead to another system environment of a team member working area for any form of modification to be made if the url of the PRR is given to him. that is, it will copy the PRR AND ALL THE CODES/FILE IN IT TO THE working area

2. Git fetch: only copy the modification or changes that was made in the remoterepo to its local repository.  and not in the working area. This is done by running

git fetch aliasname master

git fetch wm master

and this will be on the local repo with commit ID and commit comment when you run git log

if you run ls 

after this git fetch command, you not be able to see the changes or file in your working area.

You can now go ahead to run a git merge if you want the changes on a file in the remoterepo to reflect in your working area by running 

git merge wm/master

If you run 

ls    you will now be able to see the file or changes that was made

3. Git merge: this would copy the changes in your local repository to your working area. by running

git merge aliasname/master

4. Git pull will copy the changes in your PRR directly to your workinbg area by running

git pull aliasname master.

For Example.

Let us onboarding another project by creating another PRR called WALMART in the github account.

1. After i have done that, go to your system environ like your linox or window system using CLI to create a drectory and name it walmart just to reflect the name of your project.

cd in to the directory by running

cd walmart

2. create a file by naming it say data.py and write a code inside the file by running

vim data.py 

write:  "this is a python file"     and save-quit

Note; we do not want to write any code because we are Devops. we just want to understand how developers write and commit their code so that we can be abke to manage them. You cannot manage what you do not understand.

3. run 

git status   

 and it will tell you that the the directory is not a git repository.  just initialize the directory by running


4. run git init

5. run git status    and it will tell you that untracked file

6. run   git add . && git commit -m "first commit"   to your local repo

7. To push the code to your PRR, create an aliasname of your PRR by running    

git remote add wm UrlOfYourPRR

8. Run  git push wm master   since your PRR is in master

9. Go to your PRR and you will see the code/file that was created in via CLI in your virtual cloud system called linox or in your window. Let us use windows here

To illustrate Git Fetch

Let us create a code or a file using GUI in your PRR by clicking on add file and click on create a new file.

1. Let us call the file note.sh

and then write inside the file    "deploy to tomcat. this is just for understanding purpose"

scroll down and write in your commit message and then click on commit a new file

2. Since this was done on GITHUB, the new file would not be in the prOject directory in linox or windows. You can found out by running  ls  and  git log

For the changes that was made in PRR to come to linox of window environment, we can either use git clone or git fetch or git pull.

Since we already know what git clone. let us use git fetch

Using GIT FETCH  and GIT MERGE

run     git fetch aliasname master = git fetch wm master

3. run 
    ls     

    you will notice that the file called note.sh is not see or view. This is because the file is not in the working area but in the local environment

 4. if you run

    git merge wm/master   it will now take the file from the local repo to working area.


  if you run  ls   you will see that the file is now seen.

  LET US ILLUSTRATE GIT PULL

  1. Let us go back to our PRR and create another file and name the comment.sh  and write inside the file   "comment is a metadata or a data about a data"

  and scroll down and write in your commit message and then click on commit a new file

  2. go to your windows system and run ls or git log, you will not see the new file that was created in github

  For it to be see, let us employ the service of GIT PULL

  USING GIT PULL

  Run

  git pull wm master

  3. run
    ls

   You will see the new file called comment.sh   This is so because git pull will move file from your PRR directly to your working area.

   If you still run git log, it will still reflect the commit made on github to be seen in windows or linox.

 Note; Git clone will copy your PRR with all the codes and commits inside to both your working area and local repo.


    SSH KEYS CONNECTION TO GITHUB PROJECT REMOTE REPOSITORY

This is process of using SSH keys to access our github repository apart from using a password in your normal working system environment like windows or linox. 

NOTE;
1. https supports password and PAT (personal access token) authentication

Note;  PAT is used to acess and manage github API resources. it is also used to control other permitted user privileges or access on what is expected any user to be able to do. like, read acces, write access etc. It can also be used to access github instead of password

2. ssh supports rsa key in a way of encrypted algorithm 

3. We use https connection to create aliasname that uses a password and PAS (personal access token)

4. We use SSH KEYS connection to create aliasname that uses rsa encrypted algorithm


Remember that, for we to communicate with our linox server, we were using SSH keys. This can also be applicable by using ssh keys to communicate or access our github project remote repo.

How DO I KNOW IF I HAVE SSH KEY ON MY SERVER OR NOT

To know if you have any ssk key in your server, run

ls ~/.ssh

~   =  $HOME

This, you can also run    ls $HOME/.ssh

If there is no ssh keys that was configured to YOU connect to your PPR, it will say no such file or directory.

TO CONNECT SSH KEYS TO GITHUB PRR, we run

ssh -T git@github.com   it will take you to your github account wherever the ssh key has been configured.
The above command can only be used if you have already created and configured an ssh to your github account before.

HOW TO CREATE AN SSH KEYS TO GITHUB

ssh keys can be created using CLI by running

1. ssh-keygen

Above command will create an ssh key using a particular algorithm. That, if you run that command, it will create an ssh key with RSA encrypted algorithm by default

as sson as you enter the above command, continue to press enter key like four times until the system will show a box full of some kind of algorithm.

To find out if it has been created, run

ls ~/.ssh   or ls $HOME/.ssh

You will see that Two RSA algorithm were created.

1.     id_rsa        = private RSA algorithm key pair

2.     id_rsa.pub    = public  RSA algorithm key pair


So I need to put or configure public rsa key in my github account so that i can be able to access the resources I have in my github PRR.

HOW TO CONFIGURE OR PUT THE PUB. KEY IN YOUR GITHUB

Becuase  I want to create a passwordless means to my github, I do not want github to be asking me to enter password anytime i want to carryout any task using my system server like linox or windows.

1. cat the public rsa key by running

cat ~/.ssh/id_rsa.pub

2.  cope the encrypted rsa algorithm

3. Go to your github account and click on the menu near where image of the account owner is click on settings,  
click on ssh or gpg keys,
click create a new key
enter the name you want to name the key on the title bar
paste the key you copied and click on add ssh key

This is very important to create an ssh key to your github account because in realtime, companies uses more of ssh keys than using https password because shh keys are more secured than https password because of the encrypted algosithm which is very difficult to memorize except copied. But is more easy to memorize https password.

Even if an unwanted user is able to know the public key, he would also need to know that private key before he can gain full entrance because the public keys needs the private key for authentication.

The public key is the lock while the private key is the open or key

 CREATING ALIASNAME THAT USES THE SSH KEY

4. since i have created the ssh keys in my github, go to your PRR PROJECT REMOTE REPO that you are working on like walmart 
click on walmart, 
click on code   it will show you  https, sshkey and github cli
click on sshkey    since we want to copy the link or url of ssh key and not the url of https
click on the copy sign

the url of the sshkey is in the form like  git@github.com:organization/ProjectRemoteRepoName

i.e git@github.com:Osas-DevOps/WALMART.git

5. Go to your project directory in your widows or linox and create an aliasname that would be using this ssh key by running

git remote add aliasname  ssh url or link of PRR

let the aliasname be ss

git remote add ss sshLinkOfPRR   

6. run  git remote -v to know if the ssh aliasnames was created

You will see that you now have ssh and https aliasname.

Example;

Let us implement the rsa encrypted keys to our github account that we have just created.

Let us modify a file using CLI in our windowa

ls

let us modify  note.sh  by adding anything to the script in our Walmart project directory.

save and quit

add and commit by running

git commit -a -m "deploy to jboss"    to move it to locarepo

the push the script to pRR by running

git remote -v     to know the exact aliasname of the url you want to use to push the script 

The rub

git push ss master to push the script to PRR in github

Note; yOU CAN ALSO PUSH THE SCRIPT TO stage or development branch if you want to by running

git push ss stage   os git push ss development.

If you execute the above command, it will ask you if you want to proceed. Just type in yes.

As sson as you do that, the push will be successful and henceforth, you window computer or you aws linox server would now be recognozed by your github account and so subsequent push from localrepo to remoterepo will no longer ask for authentication or validation anytime you want to execute or push a code.

If you run ls

You will notice that another file called known_hosts  containing known credentials hosts. This is like a validation that your github account now recognize your server or computer system

if you cat the known_hosts by running   

cat ~/.ssh/known_hosts   you will notice that the known_hosts  is also an rsa encrypted key 

NOTE: TO delete exiting ssh key that connect you top your git PRR in your server, run

rm -rf ~/.ssh

rm = remover      rf = force remove   ~ = $HOME   / = a sign for directory

USING A WRITTEN SHELL SCRIPT THAT WAS WRITTEN BY PROF TO RUN A COMMAND IN LINOX OR WINDOWS THAT WILL CREATE SSH KEY AND ALSO CONFIGURED THE SSHKEY IN OUR GITHUB AT THE SAME TIME USING AN ADMIN PAT (PERSONAL ACCESS TOKEN) THAT ALLOW ADMIN ACCESS OR FULL ACCESS TO OUR GITHUB.

In our last class, ssh key was created via CLI and was now configured in our github account in order to establish trust and relationship between our linox server or our windows computer and our github account so that any task done between them will not ask for authenication or password.  

But in this case, ssh key can be created and be configured to our github account at tyhe same time by running running a scrpt command in our computer server or windows and then authenticate the destination of that command by puting in an admin access PAT as a form of password.

Note; For the script that does both the creation ssh key in our seerver and configuration of ssh key in our github account via admin access PAT can only be possible if ofcourse you have a PAT that has an admin access or have permission to do eventually everything in your github account and so before you ru  the command, make sure that you have granted your PAT admin permission. This can be achieved by going to where you can created a PAT in your github account and tick the access you want. You can just create another new PAT that you can grant full access by ticking everything. 

NOTE; The shell script that was written that does create and configure ssh key in our github is in Prof running note. He said he would share the script. Please take note.  He said he will put it in our resume that you have written a script that we create and configure ssh key in github using PAT token that was generated in github API

EAXAMPLE

Let us use our linox server for this example. login to your linox server.

1. Go to your github account and create a PAT that has an admin permissions or access and then copy and save the token.  Note; TOKEN IS AN ENCRYPTED ALGORITHM KEYS THAT VERY DIFFICULT TO MEMORIZE.   

3.  copy the shell script that was written by Prof

4.  create a file/scripting in you linox by running

   vim sshscriptupload.sh

5. paste what you copied and save-quit

Check if you already have private and public rsa ssh keys by running

ls ~/.ssh

If you noticed that you have it before, you may want to delete it. If you want to delete the, run

rm -rf ~/.ssh/id_rsa
rm -rf ~/.ssh/id_rsa.pub    

If you do not want to delete them. First of all, You need to see all the users in your system by running

   /etc/passwd      this shows all the users you added
   tail /etc/passwd   this shows up to the last user you addded
   tail -5 etc/passwd   this shows the last 5 users you addded

Let say you choose clifford as a user in your linox.

Copy the script/file that is in root user here to the home of clifford user that does not have private and public ssh rsa key by running

cp sshscriptupload.sh /home/clifford

Also, for chuka to have full permission in executing the script to was copied to him. You need to run  chmod 777 of the script to the home of chuka and the script. 777 means full permissions. that is

chmod 777 sshscriptupload.sh /home/clifford/sshscriptupload.sh

Let us switch user to clifford by running

su - clifford

And you will see the file that you copied there. You need to check if clifford has ssh ras keys by running

ls ~/.ssh   if there is nothing

cat the file to see if the script/file content is what is needed.by running

cat sshscriptupload.sh

So we will be executing that script as clifford user because in real time, it is not a good practise to execute the script as root user. The best practise is to execute as other user.

6. Run the shell script by running
    
    sh sshscriptupload.sh

 It will tell you to enter your PAT token. Enter the PAT you generated in github and continue to press enter until it tells you that the sshkey has successfully be copied to github.

 If you go to your github account and refresh the page, you will see that ssh key is now found there.

 Company would value and pay more if you can work with them or run task via CLI than GUI. They prefer the CLI than the GUI.  It is generally preferrable to manage resources from the command line than from the Graphical user interface. This is because someone can easily hack into the GUI but in command line interface CLI. You need to know commands for you to be able to do that.

 Note;  

1. PAT is used to acess and manage github API resources.
2. it is also used to control other permitted user privileges or access on what is expected any user to be able to do. like, read acces, write access etc. 
3. It can also be used to access github instead of password

HOW TO AMEND OR CHANGE A COMMIT MESSAGE;

In realtime you maybe contacted by your teamlead or member to update your commit message so that other developers can understand what your commit is all about. This can be done by running


git commit --amend -m"updated message"

If you run the above command, it will only update the last commit or most recent commint that you did.

HOW TO KNOW ALL THE COMMIT THAT WAS DONE BY A PARTICULAR DEVELOPER OR AUTHOR.

In a situation whereby different developer is added to a team and each of this developer has done a lots of task or written several codes and now, there seemed to be a problem whereby a particular commits is causing our build to fail. The teamlead may want to know the developer that did that commit so that he can be ask to fix the problem if need be. They might decide to be checking all the commits each of the developers has done.

If you want to know the log of commits or all the commits that was done by a particular developer or author named Clifford or simon legah etc, you can run

git log --author="the name of the developer or author"   if the developer's name is clifford, run

git log --author="organizationName used"

IQ

What is the difference between git merge, git rebase and git cherry-pick?

Answer; 

In terms of commits,  

1. git Merge; the git merge will copy all the commits that is in one branch like development branch to become a single commit in another branch like master branch. 

For exmaple, if there 4 commits in development branch i.e C1, C2, C3, C4. If you are in master and run a git merge command i.e  

git merge developement

If a file come up and tells you that you should write a commit message. Know that the commit has been merged with a comment message like (merge brancg, development).  it means it has merge all the commits in developement to become one commits in master

2. git rebase; In the same context,  git rebase will copy individual commits of developement branch to be seen also as an individual commits in master. So if you are in master, you can run

git rebase developement

3.  git cherry-pick:  	In same context, git cherry-pick will only copy just one particular commit that you have chosen in development branch to be seen in master branch when you know the commit ID of the chosen commit in your development branch. This is achieved by running

git cherry-pick commit-ID of the particular commit in development branch.


NOTE; All the releases are always done in the master branch. Releases means deploy to production. Every release goes with a tagName

What is a README  file

As a Project manager because DevOps eng., are most of the time project managers. You are going to be manager the project from the technical point of view. 

README file is a file that a DevOps eng. can add to the Project Repository that would tell people about the project/software that have been built and why it is very useful. It explains what the project can do and how the people can be able to use the project.  It contains information like;

1. what the project is all about
2. why is the project useful
3. how users can use the project.
4. where users can get started with the project
5. where users can get help with your project if they are lost or have issue with your project
6. Prerequite: How users can install your project and what they would need for the installation


ANOTHER THING THAT IS VERY IMPORTANT THAT THE INTERVIEWER MAY ASK YOU OR YOU ARE EXPECTED TO ADD DURING INTERVIEW IS

WHAT IS IT THAT YOU HAVE INTRODUCED INTO YOUR PROJECT WHICH WAS SIGNIFICANT?

Answer:  I have recommended to developers the git best practises like  

1. the use of git branching strategy and pull requests.
2. Commits once you are done with the coding of any task
3. Avoid merge commits
4. Do not commit half-done work or unfinished work by using git stash to backup your unfinished work Except if it is a new file, you do a commit and then do a little modification before you can be able to backup the work using a git stash command.
5. make sure you test your code/work before you commit, by using VScode to test your code
6. Endeavour to write a good commit message when you are committing
7. Endeavour to use git command rather than GUI tool so that your project can be more secured and not easily hacked.

NOTE:  Code in the stage branch can be deploy to staging/uat/QA environment where you are staging your project for customers review, testing of the project, uat testing team, QA team .


VIDEO 49

   INTRODUCTION OF MAVEN

Mavin is a build tool that is used to build an application/software or project via a pplication server like TOMCAT/JBOSS from the code that has been written by developers.

Mavin can also be used to manage, report and document the software or application that was built by Mavin.

One of the perculiar feature of mavin is that, it has the ability to download the Project/application/software dependencies libraries automatically from the mavin central repo, mavin remote repo or mavin localrepo that are need for the build to be successful

MAVIN is a java based open source build technology

It is been runned or owned by APACHE WEB SERVER

Foe example;

There a Project called Visa and this project already has a remoterepo in github called VISA. Some group of developers has written a code and commit the code to github projectRepo called VISA.

Inside the ProjectRemoteRepo(VISA) in github, we have what the following below:

1. code-src           src= source code
2. buidScript
3. UnitTestCases

This is to say that the developers write the code or source code, write the unit test cases and write the buildscript. That is, the project called visa must come with the source code, buildScript, and the unit test cases.

Note; Anytime developers are done with written code and commit the code to remoterepo, the destination of the code that was written is to be running in the project application server. That is, Visa would be running in application server.	

Note: For your Project like visa to be functional and accessible to customers, the visa must be runned by an application server or must have an apllication server like TOMCAT or JBOSS in order for customers access

BUT, The code that has be written by developers cannot be interpreted by the application server. So it is not a wise decision to deploy the code or raw code to the application server. It has to pass through testing and codebuild

HOW THIS WORKS

1. The developer will first of all commit the written code to PRR via a PUSH from localrepo.  the code must have src, unit test cases and buildscript in the PRR.  The code that are written may be a java code like app.java. The code that has be written by developers are raw code.

2. The DevOps engineers will PULL the raw code from PRR or clone the PRR by running a git clone command to a build tehcnology called MAVEN for interpretation, testing and codeBuild.

3. The code would be tested and if the code passes the review and testing process, a code build would be done by the MAVEN tool.

4. The codeBuild by Maven, would create packages that ends with; .jar, .war, .ear e.g app.jar, app.war, app.ear

This is the language of packages that the application server like Tomca ND jBOSS UNDERSTANDS.

5. Once the packages are created by Maven, The packages would be deploy to the application server for customers access or use.

NOTE: To move the code from github PRR to MAVEN server, you run a git clone or git pull command but most of the time, it is going to be a git clone.

IQ

What is Maven?

Maven is a build tool that interprets, test and build on the raw code that was cloned from the PRR for the purpose of creating a deployable packages like jar, war, ear that would be deployed to application server like TOMCAT OR JBOSS which the customer can access.

In a simple sense, 

Maven is software that is used to create packages like .jar or .war or .ear packages for deployment to application server like TOMCAT OR JBOSS.

NOTE; Anytime a raw code passes through a build tool like Maven, a deployable packages which the application server can interpretes are created

Examples of the pacakages are .jar or .war or .ear packages

Project code are written in the form of a programming languages like

1. java code     .java
2. python code   .py
3. nodeJS
4. JAVA SCRIPT
5.  .NET
etc


SOME BUILD TOOL FOR DIFFERENT PROGRAMMING CODE LANGUAGE AVAILABLE TODAY

1.  For JAVA project, the build tools are  MAVEN, ANt, GRADLE

2. For .NET project like microsoft project, the build tool are  MSbuild, NANT

3. FOR pYTHON project, the build tool IS PYbuilder  

4.For JAVASCRIPT project, the build tool are GULP, GRUNT

5. For RUBY project, the build tool is RAKE

3. nodeJS project, the build tool is NPM= Non Package manager


WHAT IS EXPECTED FROM DEVELOPERS

It is the duty of the developers to make sure that every project has what is called src= source code, unit test cases and the buildscript in the PRR.

UNIT TEST CASES

If there are 30,000 lines of code, it means that there would be 30,000 unit test cases

For example:

Assuming the code written is a java code, that looks like below

print("welcome to BOA")      Assuming the java code is like this. Though this is a python code.

The unit test cases would be written if form of a condition like below in the following app.test file

app.test

if [ welocme -e ]
then:
print("is ok")
else:
prin("not ok")
 Above is the unit test case to the code

It is always the job og developers that whenever they write a line of code, they should also write its corresponding unit test cases.

nOTE;  moving a file or repo from one server to another need a git clone command to do that. That is, to move a file or repo from the projectRemoteRepo (PRR) to maven server, we use git clone or git pull to execute that.

MAVEN  is a java based open source BUILD software, that create a deployable packages from a raw java code.

Java code or file always end with .java   app.java

example of deployable packages that maven creates are  .war   .jar  and .ear  packages  like app.war, app.jar, app.ear

Maven is a BUILD software that was original developed in java and also for executing on java code.

The vendor of Maven  or the company that own Maven is called APACHE

Just like the vendor for iphone is APPLE

Generally, any software you see today is available is 3 options;

1. FREE:  This are software that the services or the software itself is free but the source code or the recipe cannot be revealed

2.OPEN SOURCE: These are software that both the services or software itself and the source code are free.  E.G=  Linux OS  MAVEN   etc.

3. LICENSED: Theses are software in which the software is for sale.  E.g, windows, the source code cannot be given or revealed.

Just like coca-cola, Mac donald. Their food is for sale and their recipe is never told or revealed.

RECIPE here signifies source code.

IQ

Explain your experiences in open source technology.

ANSWER: Say something about

MAVEN
LINUX OS

lINUX AND ITS DISTRIBUTIONS ARE ALL OPEN SOURCE
Redhat
Centos
Ubuntu
Amazon Linux

WHAT DOES BUILD MEANS?

The word BUILD in maven means the tendency for maven software to compile and create a deployable packages from a raw code.

Example of raw codes/file are  app.java, app.py, app.sh

Since we are dealing with a java based BUILD technology, we will use app.java

 codeName or fileName= app.java

 app.java contail a line of written code below;

 print("hello world")

 When the code gets to maven server, the code or file has to undergo a compiling process by running

 javac  app.java

 javac means  java compilation

 aS SOON AS YOU RUN THE ABOVE COMMAND, the file/code it will create what is called java classes and so the file would now be 

 app.class which also has a binary number in the form of 101243244.

 This binary number is called java classes= This is what is called the machine language. binary numbers are the language that the machine can understand and interpretes.

 The machine that understands and interpretes java class/file like app.class is called JVM

 JVM= JAVA VIRTUAL MACHINE

 Generally, there are 2 types of languages in a computer or server;

 1. Human readable language: This is the language you see in your system that human beings can understand and interprets
 2. Machine readable language; This are languages that only the software machine or computer maachine can understanbd and interpretes. E.G  JVM

For example;

LOG IN TO YOUR linux or windows and run ls to know the files and directory that you have.

cd into any of the directory and ls the directory to see if there are files in it by running

ls /directoryName

if there are files in the directory, then run

ls -h    this human readble and so it will show normal files

ls -i    this is machine readable command

You will notice that the beginning of the file has a binary number. This is called inode. That is the machine language.

i means inode. it is a binary number which a computer atatched to a file when a file is created. inode which is a binary number is a machine language.

If a compilation is not done, a BUILD cannot be successful

MAVIN INSTALLATION

Maven can be installed in any platform like

	Linux (redhat, ubuntu, centos)
	windows (window xp/7/8/10/11, server 2012/20016/2019)
	Mac os
	Solaris
You must install java and its dependencies in a system or server before you can install maven. This is because Maven cannot run without java. Java is a prrequisite for Maven.

When you install java, other dependencies of java will automatically be installede


READme.md   THIS IS THE FILE FOR THE MAVEN INSTALLATION PROCESS

Landmark Technologies, Ontario, Canada.
Contacts: +1437 215 2483
WebSite : http://mylandmarktech.com/
Email: mylandmarktech@gmail.com

AWS EC2 Redhat Instance BEFORE THE INSTALLATION BEGINS
Prerequisite

AWS Acccount.
Create Security Group and open Required ports.
22 ..etc
Create Redhat EC2 T2.medium Instance with 4GB of RAM.
Attach Security Group to EC2 Instance.
Install java openJDK 1.8+
Java openJDK 14 will be good
java openJDK 1.4  is not advisable

Install Java JDK 1.8+ and other softares (GIT, wget and tree)
# install Java JDK 1.8+ as a pre-requisit for maven to run.

sudo hostnamectl set  hostname to maven

sudo su - ec2-user

cd /opt

sudo yum install wget nano tree unzip git-all -y

yum install java-11-openjdk-devel java-1.8.0-openjdk-devel -y 

java -version   to know ther version of java or if java is running

git --version   to know ther version of java or if java is running

Maven server need git plugin. Git needs to be running in Maven server because Maven would run a git clone to be able to pull the PRR or raw code from github

2. enter the below link  

https://maven.apache.org/download.cgi 

in your internet browser and press enter, so that you can download the latest version.

then copy the link opposit Binary zip archive.

after you have copy the link, run

sudo wget linkOfWhatYouCopied    in the form of

sudo wget https://dlcdn.apache.org/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.zip

and press enter

to see the file that you have zip, it will be in red color. run

ls 

copy the file

Then you have to unzip the file, to do that, run

sudo unzip zipFile       that is


sudo unzip apache-maven-3.9.1-bin.zip   or  sudo unzip apache*

if you run  ls    you will notice that you now have zip file with a red color and unzip directory with a blue color

then, you have to delete or remove the file that was zip since it is no longer necessary. Remember that you zip that file because it was a very large file. Since it has been comprised by sip the file, and now that you have unzip it, you have to delete or remove the zip file by running

sudo rm -rf zipFile    that is

if you run  ls   you will notice you now have one unzip directory.

by a good naming convention, rename the file to just maven by running

mv unzipFile maven    that is 

mv apache-maven-3.9.1 maven

.#Step3) Set Environmental Variable - For Specific User eg ec2-user

vi ~/.bash_profile        #and add the lines below

copy the 2 below commands

export M2_HOME=/opt/maven
export PATH=$PATH:$M2_HOME/bin

paste them inbetween bash_profile and get aliases   and then, save-quit

#Step4) 

For maven to start running, you have to refresh your sever by running 

source ~/.bash_profile
mvn -version

Then run
mvn -version     to see if maven is running because the maven commands starts with mvn


MAVEN HOME DIRECTORY (MHD)

This is the directory where maven software was extracted.

Rember that the zip maven url was download inside the opt directory (/opt) and then unzip also inside the opt directory. This means that the order would start from /opt 

Therefore, MHD is represented as 

/opt/maven    which is also represent as  [ M2_HOME ]

to verify this in your linux or window server, run

in your linux or window,  run

cd /opt      to go to the /opt where you downloaded and extracted your maven software

run   pwd       you will see that it will say  /opt

That means you are in opt home directory

To be on maven home directory  (MHD), you need to pass through /opt before you get to /maven

That is, you can run

cd /opt/maven    to go to maven home directory. to verify this, run


pwd          meaning  current working directory, it will give you


/opt/maven 

There are other components or directories and files that are in MHD. Some of the directories that you will see are

1. bin: This is a bin directory that contains binary files like command a command file. The main file in binary directory is the mvn

2. conf:  This is a config directory that contains configuration files like settingsxml. the main file here is the settings.xml

2. lib:  This is a lib directory file that contains libary files like settingsxml. the main file here is the settings.xm

3. readme: This is a file and not a directory that contains what the project is all about. What the project can do to make life easy for the subscriber or customer and how you can subscribe or register depending on what the project is.

and etc


To verify all this, run

ls /opt/maven    to show you the directories inside MHD above

You can aslo run

ls /opt/maven/bin     to see the files inside bin directory

ls /opt/maven/lib     to see the files inside lib directory

ls /opt/maven/conf     to see the files inside config directory
  

BUILD AND MANAGE PACKAGES

As a deveops eng., you have just receievdd a ticket in your jira box to build and manage a package using Maven

Note; Maven software is used to build and manage packages. These packages are of 3 types, namely

1.  .jar packages
2.  .war packages
3.  .ear packages

Build and managing of any of this packages depends on the code/application that the developers have written.

TYPES OF MAVEN APPLICATION

1. MAVEN STAND-ALONE APPLICATION: This is the types of application written by developers that contains only java codes that creates java classes after compilation to form .jar package which the application server can interprete.  

The project file name of this project must end with .jar

Example of the name that is being given to the project file of a stand-alone application are

maven-standalone-application.jar 
ebay.jar
app.jar
 etc

 etc

 2. MAVEN WEB APPLICATION: This is the type of application that has been written by developers that has both java codes and web contents on the application. 

 Examples of the web contents are JS=JAVESCRIPTS, HTMI, CSS

The project file name of this project must end with .war

Example of the name that is being given to the project file of a web application are

maven-web-application.war 
ebay.war
tesler.war
etc

3. MAVEN INTERPRISE APPLICATION: This is the type of application witten by developers that contains multiple modules of java codes and web contents. that is ( jars and wars)

.ear= .jars +  .wars

The project file name of this project must end with .ear

Example of the name that is being given to the project file of a interprise application are

maven-web-application.ear 
td.ear
tesler.ear
etc

Note: most banks runs on web application and mostly enterprise application.

Each of these packages created by maven like archives. This archives are deployable packages that can be understood by the application server

IQ
   WHAT APPLICATION DO WE BUILD IN LANDMARK TECHNOLOGY

Answer: In landmark, we build from a java based application like stand-alone application, web application and interprise application but most often, we build from a web application.

JAVA BASED APPLICATION

Java based application are application that has been written with java codes by developers.

The Java application/code that has been written by developers comes with the following 3 parameters

1. Source code (src) : this contain the raw code that has been written by developers

2. unitTestCases: The unitTestCases for a java application is called J-unit TestCases, The The unitTestCases for a  .NET application is called N-unit TestCases and The unitTestCases for a C++ application is called CPP TestCases

But we, in landmark only support or build from a java based application.

3. BuildScripts: BuildScript are witten in xml

xml= extensible mark-up language 

The buildScripts are found in pom.xml file which contains the project's;

a. Dependencies: 	This are things that the project would depend on which you can find in pom.xml

b. plugins: this are other softwares that is needed by maven in order to boost of increase the functionality of maven software

Examples of maven plugins (Maven Core Plugins) are:

compiler:     Compiles java source code.
deploy:	      Deploys the artifact to the remote repository.
failsafe:     Runs the JUnit integration tests in an isolated classloader.
install:      installs the built artifact into the local repository

c. tags:  These represents the version of the code that developers have written. For exmaple, it may be V1  or V2 or V3 or V1.0.0 etc. 
etc

whats the difference between dependencies and plugins

Plugins are software components that extend the functionality of another software application. Dependencies are external libraries or modules or repository used to build and run a software project.

what are the plugins for maven software

Maven Core Plugins
Plugin	Description
compiler	compiles java source code.
deploy	deploys the artifact to the remote repository.
failsafe	runs the JUnit integration tests in an isolated classloader.
install	installs the built artifact into the local repository.

what are the dependencies for maven software

Dependencies are external libraries or modules or repositories used to build and run a software project.

Examples of this external libaries or repo. are

1. maven Localrepo
2. maven Remoterepo
3. maven central repo

d. package name: Developers would also know and specify the packages that is expected to be created at the end by Maven software. examples of the packages that can be created by java based projects are  * .jar  or  * .war  or  * .ear

note:  *  called ycap  in this context means (anything or any name) like app or web or visa or td

what is the differences between xml and html tags

xml=  extensible mark-up language:

It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.

Example of file with xml is pom.xml used by developers to show or display their BuilsScripts

html= hyperTest mark-up language. 

This is a text-based approach to describing how content are contained in a file. They are text, images and other multimedia that are not written in a plain language. They are text that can only be understood by a web browser. 

formula for written a  html and xml tag is

tag= <tag>code</tag> 

example is  settings.html in github

html tags are tags that are pre-defined. They are existing tags by default.

xml tags are tags that are custom. Which means, they are be customize or created to suit your purpose. They can be defined the way you want it.

Example

Let us assume that a developer has written a source code with python

python= print('welcome to landmark')

1. using a html as a file on the above code

in html, we have the heading, represented by h and the paragraph, represeneted by p

heading maybe hi, h2, h3, h4  depending on how many headsing are contained in the source code

From the above python code, let us say its a heading. just one heading= hi

this can be written in html as

h1= <hi>code</hi>    h1  here is the tagname

hi= <hi>welcome to landmark</hi> 

2. using a xml as a file on the above code

You can customise your tag to be any name like Kim. let us uye Kim as a tag name to represent the above python code in xml

this can be written in xml as

kim= <kim>code</kim>    kim  here is the tagname

kim= <kim>welcome to landmark</kim> 

Other examples of source code written in xml format may looks in form of

1120= <1120>new recuit</1120>

server= <server>maven is running in this system</server>

user= <user>Simon, please review. Thank you</user>

NOTE; Try to clone maven stand-alone application and it dedependencies to your github account. the url of prof is on the running note.

IQ

EXPLAIN MAVEN LIFE CYCLE

The life-cycle of maven is of 3 phase

Each of this  life-cycles have a specific goal

1. Clean  life-cycle:  The goal of this, is to delete or remove old builds that are no longer needed, if there are any by runnin

mvn clean

2. Site  life-cycle:  The gola of this, is to create java classes called byte code which the JVM can interpretes.

3. Defaults  life-cycle: This is the most valuable and most important  life-cycle because its has all the complete goals that is required to have a successfully build and stored packages. 

The default  life-cycle has 5 goals. These are:  

1. Validation goal: This goal is used to do a code validation. That is, validate or correct the project structure and the resouirce files to make sure that they are free from syntax error by running

mvn validate

SYNTAX ERROR are common errors like spellings, wrong punchuation, wrong labells that if found in a source code can make the compiler to begin to generate error message when compiling the source code or raw code

2. COmpilation goal: The gaol of this, is to compile the java code to becomes java classes like app.class which the JVM can understanding by running

mvn compile

3. Testing goal: This gos, is to run a unit testing of java claases

4. package goal; This goal, is to create or build packages in the target directory

Target Directory is the directory that house all the output of the build. That is, the directory in which everything that comes out of te build are found. E.g, The directory you want all th the artifacts or the build packages to come to. Your intensional directory. It is called target directory

5. Install goal; this goal of this, is to store the build package called artifact in the maven localRepo by running

mvn install

Maven default LocalRepo    is   .m2/repository 

6. deploy goal: The goal of this is to uploads the build package called artifact to the Maven remoteRepo by running

mvn deploy

NOTE

MVN PACKAGE. If you run

mvn package

it will do the same think like default goals without being deploy. That is, if you run the above, it cal

1. can validate: code validation in order to make sure it is free from syntax error.
2. can compile
3. run a unitTesting
4. build package or create packages in the project target directory

mvn install:

  1. validate
  2. compile 
  3. RunUnitTesting 
  4. create packages in the project target directory  
  5. create packages im maven local repo 

So therefore, instead of running all the 5 goals in default maven life cycle, you can just run

mvn install               and them run

mvn deploy                

for it to upload the build packages or artifact to Nexus (maven's remoterepo)

Note; 	MAVEN USES PLUGINS AND DEPENDENCIES FOR THE BUILDING PROCESS

IQ
Where do Maven gets it plugins and dependencies needed for the building process?

Answer: Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. If dependency is not found in these repositories, maven stops processing and throws an error.

Maven gets its plugins and dependencies from the 3 repositories below:

1. Local Repository: It startes from Maven local repository. Maven localrepo can be access as

~/.m2/repository

if you run

ls ~/.m2/repository  to see this plugins

2. Maven Central Repository: This is a repo from the internet.  If maven does not get the dependencies needed for the build from locarepo, maven will move on to search its central repo

2. Maven Remote Repository: The Maven remoterepo is found in Nexus. If maven does not get the dependencies needed for the build from centralRepo, maven will move on to search its Remote repo which is the last option and If it does not get the dependencies needed, maven will stop processing and erro will occur.

Some examples of dependecies

1.  Open-JDK1.8+ : Maven would depend on Open-JDK1.8+ which is a dependency for the compilation of the java code to java classes for a java based project. 
Note: Open-JDK1.8+ is a dependecies which also have a compiler inbuilt inside of it. Though a compiler like javac is a plugin,

JDK means Java Development Kit

2. Junit: Maven would depend on Junit which is a dependency to run a unit testing of a java based project

3. Log4J: Maven would depend on Log4J which is a dependency to create packages

3. Selenium: Maven would depend on SELENIUM which is a dependency to run a text automation


VIDEO 51

HOW TO BUILD IN MAVEN

Let us start with Maven stand-alone application

Note: if you want to set up a permanent host name in your server, so that anytime you login, the hostname would be there, instaed of running

sudo hostname mv      and then 

sudo -su ec2-user 

you can run

sudo hostnamectl set-hostname NameOfTheHostaname

sine we are now in maven, i want to set my permanent hostname for mavin as mv, run

sudo hostnamectl set-hostname mv



Please try and clone prof stand-alone application in github so that you can have access to the stand-alone application and do a build.

First of all make a directory inside in the home of ec2-user where the msa project repo will be cloned to, by running

pwd    to know the home that you are. if it says   /home/ec2-user, and call the directory jp  , then run

sudo mkdir jp

cd jp   

and run below to clone prof standalone application repo

git clone https://github.com/Landmark-Technologies/maven-standalone-application

maven is the build software running in your system.

if you run below command

git clone https://github.com/Landmark-Technologies/maven-standalone-application

it will tell you  permission denied.

add sudo to it to grant you escallayed or admin privilege by running

sudo git clone https://github.com/Landmark-Technologies/maven-standalone-application

if you   ls   you will see maven-standalone-application  directory that was cloned from prog githup repo in your server

For best practises, maintain a good naming convention by renaming the directory by running

mv dirName/fileName newdir/filename

sudo mv maven-standalone-application msa

note. You can only use mv to rename a directory

You use cp to copy a file to another new file

if it says permission denied, then add sudo by running

sudo mv maven-standalone-application msa

msa=   stand for maven standalone application 

cd msa    and then

ll

if the root user does not have full permission like   rwxrwxrwx

it means mvn package will not work

cd ..     to go back to jp directrory    and run

sudo chmod 777 -R nameOfTheDirectory

sudo chmod 777 -R msa/

cd msa    and run

ll
You will see that the root user now have  rwxrwxrwx and the pom.xml becomes gr

You will see pom.xml file  and src directory

cat pom.xml

You will see the dependencies that says

  <dependencies>

                <dependency>
                        <groupId>junit</groupId>
                        <artifactId>junit</artifactId>
                        <version>3.8.1</version>
                        <scope>test</scope>
                </dependency>

        </dependencies>

You will see that tag above is Junit and the version of the Junit is 3.8.1 which is the dependency

<!-- Build an executable JAR -->
              <groupId>org.apache.maven.plugins</groupId>
              <artifactId>maven-jar-plugin</artifactId>
              <version>3.1.0</version>
              <configuration>
                <archive>

<groupId>com.mt</groupId>
        <artifactId>maven-stanalone-application</artifactId>
        <version>0.0.1-SNAPSHOT</version>
        <packaging>jar</packaging>


You will see that tag above is org.apache.maven.plugins and the name of the artifactId which is the name of the actifact is maven-stanalone-applicationi and the package it will build is an executable JAR package and version of the Junit is 3.8.1 which is the plugin. so apache is the plugin here

let us ls source code file/directory
to see the scr of this project from prof, 
cd src  and keep cd into next directory until you get to a file called HelloWorldTest.java and then cat the file to see the src

1. make sure that you are in the home of msa 
that is    /home/ec2-user/msa
by running pwd  to know the home you are

Since we already have the above standalone application src and its pom.xml file that has both the plugin and dependencies cloned to our server or system localrepo, let us run the comand  mvn package and see how long it will take to build or create a jar package. run

mvn package

You also decide to add more depemdencies manually to your  pom.xml by vim into it and add other dependencies.

since you know that maven needs log4J dependencies to be able to build a package for a standalone application, you would have to cat  pom.xml and then add log4J dependencies

go to your internet and type log4j dependencies for maven, click un the first version number, scroll down and then copy the dependencies 


<!-- https://mvnrepository.com/artifact/log4j/log4j -->   

The above link, this is a comment. Dont not copy it along.

only copy the below one is the dependencies that you need for the build


<dependency>
    <groupId>log4j</groupId>
    <artifactId>log4j</artifactId>
    <version>1.2.17</version>
</dependency>


and paste it in your pom.xml where dependencies are and make sure it is inside the dependencies.

vim pom.xlm   and then add

<dependency>
    <groupId>log4j</groupId>
    <artifactId>log4j</artifactId>
    <version>1.2.17</version>
</dependency>

and add selenium dependencies also that you copied from the internet same way. Selenium is used for text automation.

NOTE: If you want to remove or delete a directory and you run

rmdir diretoryName

and it says directory not empty and failed,  then force-remove the directory by running

rm -rf directoryName     it will delete the directory but if it says permission denied, run

sudo rm -rf directoryName


NOTE; when you create a new aws ec2 redhat linus server, you need to connect the aws server to your linux server by running

ssh -i keypegenerated ec2-user@yourIpAddres

if key is not recognise. copy the content of your key in your desktop and vim to create file that would contain the content of our key and then save

run 

sudo chmod 400 fileNameOfYourKey

RE RUN

ssh -i keypegenerated ec2-user@yourIpAddress


and then it will work. 

You can also create a password by first of all updating and activating password authentication by running

sudo vim /etc/ssh/sshd_config

it will take you to the ssh configuration file

type i, to put it in insert mode or writing mode.

scroll down and go to where it is writing password authentication

go where they wrote 'NO' eras it and type in "YES"   and then save and quit

To activate or apply the changes that you have just made, run

sudo systemctl restart sshd

To change or create password now, you run

sudo passwd userName

if the username is ec2-user  or Andrew, you run

sudo passwd ec2-user   or sudo passwd andrew

to create a user, run

sudo useradd userName

it will ask you, change passwd. put in your password twice and it will be up and running



and also create new users by runnin

sudo adduser userName

and then follow the instructions


NOTE: If you want to set a permanent hostname in your server, run the below command

sudo systemctl set hostname


To sent time zone on your server:

timedatectl list-timezones

It will list Zones and the name of the countries

Example

To set time in Doula in Africa, you run

sudo  timedatectl set-timezone "Africa/Douala"

To set time in Doula in Africa, you run

sudo  timedatectl set-timezone America/New_York

NOTE: 
MavenLocalRepo is  .m2/repository 

.m2/repository  is in the home of ec2-user     this can be written as

/home/ec2-user/.m2/repository

i.e    
This is the place where maven can first of all search dependencies/plugin from, when running a build, and if it does not find the needed dependencies/plugins from here, Maven will go for the second option called MavenRemoteRepo and if what is needed not found, would go to the third option called MavenProxyRepo and if not found, would now go to the fourth option which is the MavenCentralRepo (internet search). 

If maven gets the needed dependencies/plugins for a build to be successful from any other source, afater the build, the dependencies gotton from other source would now be saved in mavenLocalRepo 


Note That, If maven search for its dependencies/plugins from just it localRepo, the build process would be the fastest but when it goes outside to search, the build process will reduce.

For example, if a build of an application (MSA or MWA or MEA) is done for the first time, by running

mvn package

there is every tendency that Maven may not have all the required dependencies/plugins needed for the build to be successful. and so maven can go sort for dependencies/plugins from another source.

if Maven gets its dependencies/plugins from MavenCentralRepo only, the build time is going to take about twice the build time, if it got the dependencies/plugins from just MLR alone.

In this case, if you you clean or delete the first build pacakge by running

mvn clean

and then re-run the maven build command by running

mvn package 

You will notice that the build time would less, thi si because maven has saved the dependencies/pugins gotten from the first build in its localRepo called MLR   .m2/repository 

Note;

If validation and compilation is passed and unitTestCases is not completely passed, the build would not be successful



# mount -o remount,size=6G,noexec,nosuid,nodev,noatime /dev/shm

For a build to be successful, the unitTestCases must be 100%

Example:

RunTest: 100%  Failure: 2  Error: 2  Skipped: 0 

RunTest passed (OK) 98%

With the above readings, maven will not be able to create or build packages because the RunTest which is the unitTest is 98% due to the 2 errors that appear. The code has to be error free for maven to build.


IQ

If the Run test passed is 100%  but 2 failed. What can be done for maven to build?

Answer: In as much as there is no error, Maven can skip the unit testing for maven to build, by running

mvn package -DskipTests   or    mvn package -Dmaven.test.skip=true

IQ
Some packages are taking so much time to build, what would you do to resolve this?

Answer; You can resolve this by skipping the test goal or unit testing of the application  

Iq

What problem have you faced in your project recently?

Answer: The problem we have recently faced was that some package was taken so much time than usual to build, we troubleshoot and discover than a Junior Engineer has mistakenly deleted the MLR and so maven where now getting its dependencies and plugins from another source.

What we did was to create  a customize MLR in settings.xml in order to prevent delays or to normalize the time that would be needed for a build to take place when such thing happen again in future.

setting.xml is a file containing MLR which you can modifile to create another MLR called custom LR.

Knowing that maven is in /opt    then run

ls /opt/maven

You will see conf  which is a configuration file. Then 

ls /opt/maven/conf

You will see setting.xml   then run

vim /opt/maven/conf/settings.xml

It will not allow you to modify the file because it is owned by a root user

For you to modify it, add sudo by running

vim /opt/maven/conf/settings.xml

This is where you cam modify the MLR to a customize LR

Let us call the customize LR   fintech

Knowing that, MLR is in the home of ec2-user

We are trying to make sure any maven future build, maven will search for its dependencies.plugins from fintech and no longer the default MLR 

Scroll down and go to where localRepository  is, and then go below  arrow sign  and paste the below 

<localRepository>/home/ec2-user/fintech>/<localRepository>

save-quit

if you run

ls home/ec2-user

You will find  .m2  which is MLR but you will not find fintach

when you run

mvn package

After the nvn package is done, run

ls home/ec2-user

You will see that fintech would appear.  The custom LR called fintech will now be visble home/ec2-user when the first build is done of which the build time would be longer because maven will sort for dependecies and plugins from other source and dependecies and plugins gotten from other source would be saved fintech.

If you delete the old build and re-run 

mvn package

build time will now be faster because maven will now get its dependecies and plugins from fintech 

IQ

what is the difference between mvn package, mvn install and mvn deploy?

Answer: if you run the command

mvn pacakage

it will validate, it will do code compiling, code unit testing and lastly create packages or artifacts like .jar or .war or .ear inside the Project target directory

if you run the command

mvn install

it will validate, it will do code compiling, code unit testing, create packages or artifacts like .jar or .war or .ear inside the Project target directory and lastly also store or install the artifacts in MLR, If you are using custom LR like fintech, the artifacts will be store or saved in fintech

if you run the command

mvn deploy

it will validate, it will do code compiling, code unit testing, create packages or artifacts like .jar or .war or .ear inside the Project target directory, store or install the build packages or artifacts in MLR, If you are using custom LR like fintech, the artifacts will be store or saved in fintech and lastly, it will also save or store the artifacts in MRR in Nexus


IQ

How do you see or view or access the artifacts that is stored or saved in MLR OR CUSTOM LR?

LR means LocalRepo

Answer: First of all, MLR or customise is always find in the home of ec2-user. Then run

ls /home/ec2-user/.m2/com/mt/ProjectRepo

   or

ls /home/ec2-user/fintech/com/mt/ProjectRepo

If you run the above command, you will see a VERSION OF SNAPSHOT, that snapchat is where the artifacts is saved.  

Let us say, the version of the snapchat is  1.0.2.3 snapshot, therefore That is

ls /home/ec2-user/.m2/repository/com/mt/ProjectRepo/1.0.2.3 snapshot

   or

ls /home/ec2-user/fintech/repository/com/mt/ProjectRepo/1.0.2.3snapshot

IQ

How do you deploy MSA



Answer; Knowing that the build artifacts is inside the target directory, first of all 

ls /opt/maven/ProjectRepo/target

and then you will see the artifacts which is a red color words that end with  .jar

and then

java -jar target/TheNameOfTheBuildPackagesInRedColor

TheNameOfTheBuildPackagesInRedColor of MSA is in the form of ProjectRepo-1.0.2.3SNAPSHOT.jar

java -jar maven-stanalone-application-0.0.1-SNAPSHOT.jar 

For best naming convention, you can always rename your project to more shorter and message driven name. 

if the ProjectName of the MSA is renamed to be app.jar by running

sudo mv maven-stanalone-application-0.0.1-SNAPSHOT.jar app.jar 

the build artifact is always red in color

stand alone application deployment

Then you can deploy the build package of artifacts by running

java -jar app.jar

If the above command, you have deployed the stand alone application it it will show


Hello Engineers, Welcome to Landmark Technologies DevOps Master Class. You are sudying Maven Build Tool. !! Today Date is:  Sun May 07 20:12:47 UTC 2023


NOTE;

If you run

mvn clean package

 or 

mvn clean install 

 or 

mvn clean deploy

Two goals would be initiated in the 3 categories of command above. Thease are


1. The first and end result here is that it will clean or delete old builds
2. The end result here is that it will build the package in the target directory or The end result here is that it will save/install the BUILD package in MLR or The end result here is that it will SAVE the BUILD package in the MRR in Nexus

IQ

What are the files that is neccessary from a developer for maven to initiate a build process

Answer: tHE TWO FILES THAT ARE VERY improtant for a maven to start a build are

pom.xml and source code (src)

NOTE; iF you run

mvn install

and delete the builds or artifacts by running

mvn clean

Note that, your target directory including the artifacts inside would all be deleted but the same artifacts would still be seen in MLR. For you ro confirm, do the above and then investogate by running

ls /home/ec2-user/.m2/repository/com/mt/ProjectName/1.0.2.3snapshot

or

ls /home/ec2-user/fintech/repository/com/mt/ProjectName/1.0.2.3snapshot

projectname maybe MSA or MWA or MEA

You will see the arifiacts backed up there


Note; Build artifacts is always red in colors

If you want to see the artifacts inside target directory.



ls /home/ec2-user/ProjectRepo/msa

You will see the target directory, then if you

ls /home/ec2-user/ProjectRepo/target

You will see the artifacts of build packages that has build by maven

IQ

Among the 4 means maven gets dependencies and plugins from, which is the most secured and why?

Answer: Build artifacts security is very important to avoid unauthorize person having acces to the build artifacts and other important contents. 

This is why MLR is the most secured if maven got its build dependenciues/plugins frpm MLR searching for dependencies/plugins during maven build was done in MLR. This is because, there ia a form of secrecy of the build without any external awareness that such thing happened. No external search aside MLR was carried out, that would expose the process to hackers. 

Examples of other search that maven will employ are like MRR, MPR and MCR.

But when the dependencies and plugins are searched exwhere, the secrecy is broken and therefore the process would be exposed to unauthorized IT hackers.

Among the 4 means for maven dependencies/plugins search,  MavenCentralRepo would be the most unsecured.

IQ

WHAT IS THE ONE OF THE PROBLEM YOU ENCOUNTERED RUN MAVEN APPLICATION.

ANSWER: One of the problem we faced was when i was trying to fix the problem that a junior colleague faced when trying to buld on a maven web application. He shared his screen and i did try to fix it but it was showing the same result about compilation problem.

I knew that there was something wrong with his java installation. I discovered that he only have one java installation of a lower version and that would not be able to run the latest maven installtion. I had to install the latest java version 11 on his maven server but still, maven refused to build.  I cd back and fault and found out that my colleague cloned the project remoterepo from the developer to the /opt/maven directory. That is one of the reason maven refused to build.

Maven build from the home of ec2-user. i had to cd back to /home/ec2-user and the create a directory and then, run the move command to move the projectrepo to the new directro /home/ec2-user/newDirectory by running

sudo mv jp ~

When i ran   mvn package

Maven successfully made a build on the application


Note: You can move a file from one directory to another in the same server by running

sudo mv destinationDirectory ~     

~         means   /home/ec2-user   

The execute this, you have to be directory you are copying from and the 2 direcories must be on the same home.

MAVEN WEB APPLICATION (MWA)

clone prof github remoterepo  to jp diretory in the home of ec2-user. make sure you are in jp directory, and run

sudo git clone https://github.com/LandmakTechnology/maven-web-application

rename the repo to mwa

This is the type of maven application that contains java codes and web content that will create a .war build package when a maven build process is carried out.

cd mwa 

mvn package

You will notice that its says compilation error. Maven refused to build

The remoterepo came as a root user without full permission.

make sure you are in jp directory and change the permission to full permission by running

sudo chmod 777 mwa/

You have now change the permission in mwa to full permission so you can execute on the repo

when you re-run

mvn package

it will build successfully

when you 

cd target    you will see that it created tesla.war  package whgich is red in colour

DEPLOYMENT OF A MWA

IQ

How can you deploy java based MWA?

Answer: MWA can be deploy in TOMCAT or JBOSS and so, you must make sure tomcat or jboss must be up and running is in your system for mwa to be deployed successfully

IQ

How can you deploy a javabased MSA?

Answer: To deploy a Java based MSA. first of all 

make sure you are in /home/ec2-user     by running   pwd   if you are, then run

ls /projectDirectory/ProjectRepo/target   that is 

ls /jp/msa/target

and the you will see the artifacts which is a red color words that end with  .jar  that is 

app.jar

and then

java -jar target/app.jar

wHAT ARE THE FOLLOWING 

MVN CLEAN PACKAGE

When you run the command

mvn clean package

It means that you are running 2 maven goals in just one command. You will be running mvn clean and mvn package in one command. It will do the following

1. It will clean or delete old builds on your server by removing and deleting the target directory and its contents
2. it will run validation of the codes to make sure it is free from syntax error
3. It will do code compilation to java classes called byte-code that the JVM can understand and interpret
4. It will run a unit testingde
5. It will create a new package in the project target directory


MVN CLEAN INSTALL

When you run the command

mvn clean install

This also means that you are running 2 maven goals in just one command. You will be running mvn clean and mvn install in one command.  It will do the following

1. It will clean or delete old builds on your server by removing and deleting the target directory and its contents
2. it will run validation of the codes to make sure it is free from syntax error
3. It will do code compilation to java classes called byte-code that the JVM can understand and interpret
4. It will run a unit testingde
5. It will create a new build package in the project target directory
6. It will install or save or store the new build packages in the MLR

MVN CLEAN DEPLOY

When you run the command

mvn clean deploy

This also means that you are running 2 maven goals in just one command. You will be running mvn clean and mvn deploy in just one command. It will do the following

1. It will clean or delete old builds on your server by removing and deleting the target directory and its contents
2. it will run validation of the codes to make sure it is free from syntax error
3. It will do code compilation to java classes called byte-code that the JVM can understand and interpret
4. It will run a unit testingde
5. It will create a new build package in the project target directory
6. It will install or save or store the new build packages in the MLR
7. It will install or save or store the new build packages in the MRR in Nexus

IQ

WHAT ARE THE 2 FILES NECESSARY FOR MAVEN TO INITIATE OR DO A BUILD?

Answer; The 2 files needed for maven to do a build are

1. pom.xml  which contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. 
2. source code (src)  which contains the codes written by developers.

MAVEN ENTERPRISE APPLICATION

This is also a java based application that has multiple modules of applications.  Like a MSA and MWA or a MWA and MWA that would produce a .ear build packages.

For example.

If you clone prof MEA

git clone https://github.com/mylandmarktechs/maven-enterprise-application 

 you will notice that it has 2 modules of application. That is MavenEnterpriseApp-ear directory, MavenEnterpriseApp-web  directory and pom.xml file

If you cd into /home/ec2-user/jp/mea/MavenEnterpriseApp-ear  and /home/ec2-user/jp/mea/MavenEnterpriseApp-web
pwd

ls /home/ec2-user/jp/mea/MavenEnterpriseApp-web    then

ls /home/ec2-user/jp/mea/MavenEnterpriseApp-ear

you will notice that there is a src in each application and a child pom.xml inside each of the 2application 

if you run 

mvn package

It will build on the individaul child pom.xml on each applications directory and create the target directory containing the build package inside each applications directory

cd target   and you will see the buld .ear artifacts

Many atimes, MEA project can contain about 100 modules of application 

and if only one has been updated and you are told to build on only the updated module, you can simply run the command

mvn package -P TheUpdatedModule

The -P is a cap latter

How do you deploy a mea

you deploy mea using jboss only

IQ

HOW DO YOU TROUBLESHOOT A FAILED BUILD IN MAVEN?

Answer; You can troubleshoot a failed build in maven by

1. check or read the logs to understand the error
2. run the command

  mvn -x package 

It will build on debugging mode

IQ

HOW CAN YOU BUILD ON A SPECIFICA MODULE IN AN ENTERPRISE APPLICATION?

Answer: You can BUILD from a specific or particular module in an Enterprise application using by running the command

mvn package -P TheSpecificModule

VIDEO 56



TOMCAT

APACHE TOMCAT

Introduction
Tomcat or Apache Tomcat is a light weight, open source web container used to deploy and run java-based web applications. It is developed by Apache Software Foundation (ASF).

What is Webserver?
A Web server is a program that uses HTTP (Hypertext Transfer Protocol) to serves web content (HTML and static content) to users.

IN MY WON WORD

Application server also know as deployment server is a server that deploys build packages or artifacts from build server to the clients application via different protocols like http in an n-tier architecture like (3-tier architecture, 2-tier architecture, etc. n= 2, 3, 4 etc)

Examples OF Web servers are:

Apache HTTP server
Nginx (pronounced engine X)
IBM HTTP server (IHS)
Oracle iplanet web server
Internet Information Server (IIS)

What is an Application Server?
An application server is a container upon which you can build and expose business logic and processes to client applications through various protocols including HTTP in a n-tier architecture.

Examples of Application servers are

Apache Tomcat --Apache
JBoss/WildFly - RedHat
WebLogic - Oracle
WebSphere Application Server - IBM
WebSphere Liberty Profile - IBM
Galssfish


Note: Tomcat by default will run in 8080 port number.
---------------------------------------------------------------------------------------------------------------------
Tomcat Directory Structure
Tomcat installation provides these directories:
bin: for Tomcat's binaries and startup scripts.
conf: global configuration applicable to all the webapps. The default installation provides:
One Policy File: catalina.policy for specifying security policy.
Two Properties Files: catalina.properties and logging.properties,
Four Configuration XML Files: server.xml (Tomcat main configuration file), web.xml
(global web application deployment descriptors), context.xml (global Tomcat-specific
configuration options) and tomcat-users.xml (a database of user, password and role for
authentication and access control).
The conf also contain a sub-directory for each engine, e.g., Catalina, which in turn contains a sub-sub-directory for each of its hosts, e.g., localhost. You can place the host-specific context information (similar to context.xml, but named as webapp.xml for each webapp under the host).
lib: Keeps the JAR-file that are available to all webapps. The default installation include servletapi.jar (Servlet), jasper.jar (JSP) and jasper-el.jar (EL). You may also keep the JAR files of external package here, such as MySQL JDBC driver (mysql-connector-java-5.1.{xx}-bin.jar)and JSTL (jstl.jar and standard.jar).
logs: contains the engine logfile Catalina.{yyyy-mm-dd}.log, host logfile localhost.{yyyy-mmdd}. log, and other application logfiles such as manger and host-manager. The access log (created by the AccessLogValve) is also kept here.
webapps: the default appBase - web applications base directory of the host localhost.
work: contains the translated servlet source files and classes of JSP/JSF. Organized in hierarchy of engine name (Catalina), host name (localhost), webapp name, followed by the Java classes package structure.
temp: temporary files.
How to change the port number in Tomcat?
Go to the conf directory and open the server.xml and you will find below lines.
<Connector port="8080" protocol="HTTP/1.1"
connectionTimeout="20000"
redirectPort="8443" />
Replace the 8080 with any port number.
---------------------------------------------------------------------------------------------------------------------
How to stop the tomcat server?
>cd APACHE_HOME Dir\bin
>shutdown.bat (OR) > catalina.bat stop
MAC/Linux:
shutdown.sh (OR) > catalina.sh stop
---------------------------------------------------------------------------------------------------------------------
Install


IQ: 

What is the d/f b/w JBoss/WildFly and Tomcat? 

    1. The vendors of Tomcat is Apache open source java based
       The vendor of JBoss is Redhat  

    2. Tomcat is used to deploy java based web application only
       JBoss is used to deploy both java based web application and enterprise application


Knowing that MWA and MEA are deployed by an application server called TOMCAT/JBOS and JBOSS respectively.

TOMCAT and JBOSS are both applicationServers

WHAT IS TOMCAT OR APACHE TOMCAT?  find this out in prof running note

WHAT IS A WEB SERVER?  find this out in prof running note

EXAMPLES OF WEBSERVER

WHAT IS AN APPLICATION SERVER?  find this out in prof running note
examples of APPSERVER

Note; Webserver serves or displays web content or image content to users. It serves as a security or proxy or a gateman or means in which users can access the application server

WHAT DO YOU MEAN BY N-TIER ARCHITECTURE OR MULTI-TIER ARCHITECTURE?

Answer: Let us use example using facebook. Facebook has about 2B subscribers. Each of these subscribers has to access the application server to be able to login to their accounts.

It is not secured or not recommended for a user to be able to directly access the application server like Tomcat. This is where webserver comes in

Users would have to pass through webser to be able to get to application server via webserver like Nginx or apache or IIS etc and all of this webservers serves http (hyperText Transfer Protocol or web content which users can be able to access via the internet)

By above implication, webserver serves as the following to the application server

1. Proxy: This means that webserver prevent hackers or anyone to get to applicationserver directly because the applicationServer is not configured to receive traffic directly from end users.
2. They increase security. They can screen users request. they act as a security guard

3. load balancer: Webserver act as a load balancer. They perform load balancing. If you have a 2 Billion users, 2 Billions can not be managed by just one application server and if it does, the applicationServer will break down very fast. What we happen in realtime is that, multiple replical of the applicationServer would be created and webserver would act as a load balancer by routing traffic to this multiple application server in a way that each application can withstand or manage. It make sure that each application server are served base on their capacity.

Route traffic means sharing users to each of the applicationserver in accordance to their capacity.  The webserver would do some configuration, define some routing rule and that routing rule uses algorithm. Example of the algorithm webserver users is Round Robin Algorithm. It will be routing traffic to the Appserver that has less traffic. That is called load balancing. 

after the webserver has route the traffic to applicationservers, the data or info that users are entering are being capture in the database after the application. 

This type of architecture is called a 3-tier or multi-tier application deployment. These are 

1. Webserver Tier          this is called the front-end-tier
2. ApplicationServer       Tier  this is called the back-end-tier
3. DataBase Tier           this is also called the back-end-tier

The illustration above is a 3-tier application deployment

Note: Among these tiers, Our back-end tiers are expected to be highly secured. Our back-end is called MILITARIZED ZONE.  This means that no body can get to the back-end directly except via a proxy called a webserver. While our fronf end is called a DEMILITARIZED ZONE

In a 2-tier application deployment, it only contains just the weserver and the application server. The front end tier is the webserver which is in the zone called a DEMILITARIZED ZONE while the back-end-tier is the application server which is in the zone called a MILITARIZE zone.

MILITARIZED ZONE is an indication that no user would be able to have access to the application server and the database without passing through a proxy called webserver. This zone is highly secured.

MILITARIZED ZONE is an indication that no user would be able to have access to the application server and the database without passing through a proxy called webserver. This zone is highly secured.

DEMILITARIZED ZONE is the zone that users can have access to and can relate with. It is not very secured commpared to the militarized zone and so any user traffic would be screened and then redirect to the application server for an interpretation and then data or info received would be pushed to the database for storage or backup, for a 3-tier application deployment


TOMCAT INSTALLATION BELOW

installing tomcat using a bash shell script

#!/bin/bash
#use this shell script to install tomcat
#echo assign hostname to your server 
sudo hostnamectl set-hostname tmt1
#install Java JDK 1.8+ as a pre-requisit for tomcat to run.
cd /opt 
sudo yum install git wget -y
sudo yum install java-1.8.0-openjdk-devel -y
sudo yum install unzip -y

# download tomcat software form the internet and extract it.
#https://tomcat.apache.org/download-90.cgi to get the latest version
sudo wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.74/bin/apache-tomcat-9.0.74.tar.gz
sudo tar -xvf apache-tomcat-9.0.74.tar.gz
sudo rm apache-tomcat-9.0.74.tar.gz
#to rename the directory to tomcat
sudo mv apache-tomcat-9.0.74 tomcat1 
#to change permission to full permission     
sudo chmod 777 -R /opt/tomcat1
#to change ownership to ec2-user
sudo chown ec2-user -R /opt/tomcat1   
#to start tomcat1
sh /opt/tomcat1/bin/startup.sh    
# create a soft link to start and stop tomcat
sudo ln -s /opt/tomcat1/bin/startup.sh /usr/bin/starttomcat 
sudo ln -s /opt/tomcat1/bin/shutdown.sh /usr/bin/stoptomcat
#update all the software
sudo yum update -y
starttomcat
#the installation script ends here

NOTE:

anytime you log out of your system, when you come back, always start tomcat by running the command

starttomcat

copy the above script

vim tomcat.sh       in linux server ec2-user via sshkey and then save and quit and then  run

sh tomcat.sh

You will see that tomcat has started

To see if tomcat is running in your system, you do the following

1. paste  tomcatpublicIpAddress:TomcatPortNumber  on your internet brwoser
          tomcatpublicIpAddress:8080 
2. curl -v tomcatPublicIpAddress:8080 or url -v tomcatPrivateIpAddress:8080
    to check tomcat in anywhere or any server
3. ps -ef | grep tomcat
4. curl -v localhost:8080   this means that i am running it locally or from you lcal computer
          

NOTE: to uninstall java   run

sudo yum remove TheVersionOfTheJava          like   java-1.8.0-openjdk-devel -y


You can also do install tomcat step by step in the below order but running it as a script is faster and better

 sudo hostnamectl set-hostname tomcat

# install Java JDK 1.8+ as a pre-requisit for tomcat to run.
cd /opt 

sudo yum install git wget -y

sudo yum install java-11-openjdk-devel java-1.8.0-openjdk-devel -y

NOTE: to uninstall java   run

sudo yum remove TheVersionOfTheJava          like   java-1.8.0-openjdk-devel -y

sudo yum install unzip -y

# Download tomcat software form the internet and extract it. That is, paste below in the webpage

https://tomcat.apache.org/download-90.cgi

click on archive
click on the lastest version by checking the date it was release. the most recent date is the latest version
click on bin
copy the url of the bin where you used to write or type e new url at the top bar, in the form of below

Copy the link that end with ipaddress.zip  like  apache-tomcat-9.0.74.zip  and add it to the link avove  to form like below


Let us use the tar file url

#https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.74/bin/apache-tomcat-9.0.74.tar.gz


sudo wget latestVersionOfTheTomcatApplicationServer  #Some like below

sudo wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.74/bin/apache-tomcat-9.0.74.tar.gz


#use a tar or unzip command to unzip the file depending on which of the file formet you #downlaoded. If you downloaded a zip file, the use unzip command to unzip it but if you copied # # or downlaoded a tar file, then use tar -xvf command to unzip it

sudo tar -xvf apache-tomcat-9.0.74.tar.gz

sudo rm apache-tomcat-9.0.74.tar.gz

sudo mv apache-tomcat-9.0.74.tar.gz tomcat      #to rename the directory to tomcat

sudo chmod 777 -R /opt/tomcat     #to change permission

sudo chown ec2-user -R /opt/tomcat   #to change ownership

sh /opt/tomcat/bin/startup.sh    to start tomcat

# create a soft link to start and stop tomcat

sudo ln -s /opt/tomcat/bin/startup.sh /usr/bin/starttomcat 

sudo ln -s /opt/tomcat/bin/shutdown.sh /usr/bin/stoptomcat
sudo ln -s /opt/tomcat/bin/shutdown.sh /usr/bin/stoptomcat


#Once the above instruction is follow, you can start tomact by runing

starttomcat

#Run the below command to move to 

sudo su - ec2-user

#amd then run

starttomcat

THD: The Tomcat Home Directory 

if you cd into tomcat and run pwd, you will see the THD

This is where the tomcat software is extracted which is /opt/tomcat


Where us your THD located in your environment?
     /opt/tomcat9          is what we used in landmark
      /app/tomcat  
      /usr/local/tomcat 

 $HOME_TOMCAT:  /opt/tomcat9    /app/tomcat  /usr/local/tomcat 

 Note; When tomcat is full installed and running. there are so files in the BIN can be used to start and stop tomcat

THD:
  bin
    startup.sh    LINUX SYSTEM
    startup.bat   ----> WINDOWS OS
    shutdown.sh
    shutdown.bat     ----> WINDOWS OS
    catalina.sh start
    catalina.sh stop | start | restart | 
    version.sh
    version.bat
    sh /opt/tomcat9/bin/startup.sh
    sh startup.sh
    sudo 
    ln -s /opt/tomcat9/bin/startup.sh /usr/bin/tomcatstarting

  conf:
    server.xml    This is found under the conf meaning configurattion file. Should incase you want to configure your tomcat app, like maybe you want to change the port to say 6000 


       default port for tomcat   = 8080 
       changing the default port = 6000
       autoDeploy
       webapps. The server.xml will tell you that deployment is taking place in the webapps directory.

    Tomcat-users.xml   This is another form of configuration file where you can create tomcat users and assign roles or what their office or duty would be using tomcat app. Users that would have more right than others. Let say you are managing a banking tomcat app. Creating users like

       simon    - Manager 
       landmark - Developer 
       paul     - deployment engineer 

  	Lib:  This contains the libray files. They contain jar files

  logs  This are logs files that contain the following

    catalina.out  : If you tomcat is not starting, you can check the logs file by checking the catatalins
    host-manager.log  if something is wrong with your application, check the host-manager.log
       IQ:  What are the log files available in the log dir?  
  
  webapps: This is where deployment take place. This is the default directory in tomcat. we effect deployments in Tomcat in the webapps Directory. If You wantr to deploy any application ,it is done in the webapps directory
  This is a very import directory in the THD
    This is default deployment Directory in Tomcat 


    war files   This file contains build packages of mavin web application. Files like
        maven-web-app.war  
        maven-web-app
DEPLOYMENT
    This is copying the war fils into the webapps directory. If you want to deploy the war files like  maven-web-app.war  
, you will copy the war file into the webapps directory in the form of

   THD = /opt/tomcat9/   
   webapps home directory will be = /opt/tomcat/webapps

  let us assume that the war file is maven-web-app.war  

 Therefore,

 deployment = cp maven-web-app.war /opt/tomcat/webapps/

       That is  cp *.war /opt/tomcat/webapps

       cp means copy into

       *.war means anything that end with .war


Note: tO UPDATE YOUR INSTANT LINOX SERVER, YOU CAN RUN

sudo yum update 

Accessing Tomcat:

 	Go to your aws and copy you public IP address or run the command

 if your ipaddres is 18.218.154.78   and tomcat default port is 8080  


  the copy  18.218.154.78:8080  and past it in your web internet and tap enter key

  You will see that you are able to access tomcat web page

  If you paste and it says  404 not found

  stop tomcat and start tomcat 

  check if tomcat is running by the 

  ps -ef | grep tomcat

 If it says it is running

 then, 
   the copy  13.57.212.115:8080  and past it in your web internet and tap enter key

   if it stll says, 404 not found

   Delete the tomcat and install the old or lower version of tomcat from the tomcat web page by following the tomcat install procedures and rename it to tomcat, change the permision to be executable by everyone (sudo chmod 777 -R tomcat), also change the owneship of the app to ec2-user (sudo chown ec2-user -R tomcat)

cd tomcat and start tomcat  by running

starttomcat

ls     to see if bin directory is present   then

ls tomcat/bin   and you will see startup.sh   then run

sh /bin/startup.sh 

The go back and copy  13.57.212.115:8080  and past it in your web internet and tap enter key

if it works, it will tell you that, you have successfuly installed tomcat, congratulations

Tomcat server configuration:

On the web browser of tomcat in order to upload artifacts,  to be able to manage application. go to manager app and click, and it will say access denied. it means the user does not have access.

How to solve access denied problem

vi /opt/tomcat9/webapps/manager/META-INF/context.xml  This is the context.xml file

 Tomcat server configuration:

 vim into context.xml following the absolute path to context.xml

 By default, you can only run tomcat from the local host. This means you can only access tomcat from the same computer that you are using to run tomcat from an external system, so we have to comment on the line that allows only the local host to view

  vi /opt/tomcat9/webapps/manager/META-INF/context.xml 
 #vi /webapps/host-manager/META-INF/context.xml  

 Remember # is a sign of comment

 scroll down until you see the line like below, 

 strict"/>  and end with    <valve classNAME

 and then make the line a multi-line comment by opening the line with 

<!--   the next line after   strict"/>  

and close the the line after <valve classNAME  with   -->

after you have done it, it will now appear as

<!--
  <Valve className="org.apache.catalina.valves.RemoteAddrValve"
         allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" />
-->


Then save and quit

If you click the manager in your tomcat web page, it will ask you for username and password

configuration of user name and password om tomcat web page

To configure users or add users, password and their roles, you need to vim into the users file called tomcat-users.xml which is in conf directory. Following the absolute path, it is 

vim /opt/tomcatName/conf/tomcat-users.xml>   that is

vim /opt/tomcat1/conf/tomcat-users.xml

Then copy THE FOLLOWING BELOW

<user username="landmark" password="admin123" roles="manager-gui,admin-gui, manager-script"/>

user landmark, with password and ASSIGNED roles which is manager-gui, admin-gui, manager-script

then 

sudo vim /opt/tomcat1/conf/tomcat-users.xml

scroll down and you will found where it says  ROLES, USERS etc.

Let us add the 2 users.         paste 

<user username="landmark" password="admin123" roles="manager-gui,admin-gui, manager-script"/>

<user username="cliffosa" password="admin123" roles="manager-gui"/> 

between -->  and </tomcat-users>

and then save-quite

Note, We now have 2 users with different permissions.

if you notice, cliffosa permissions that was configured it different from landmark permissions. Landmark permission is more.

If you click on manager on the tomcat web application, it will ask you for username and  password, if you put in clifford details, it will say access denied. This is because clifford do not have managerial permissions according to the configuration but only Landmark do. If you put in landamrk details, access will be granted to the backend of the tomcat web application server

It is in this back-end where you can upload and deploy application

if you have any .war package in your local desktop computer, You can deploy any .war application by clicking on deploy  where .war deploy is, it will take you to upload ypur .war package, upload and deploy and you will see it on your tomcat web page. if you click on the file, the application you deploy will appear

USING CLI TO DEPLOY APPLICATION

 Example of Application Server: 
  1. IBM WebSphere Application Server (WAS)  --> IBM
  2. Apache Tomcat   --> Apache  --> OS
  
  3. JBoss/WildFly   --> Redhat/IBM 
    1.0.0 to 7.0.9
    1.x to 7.x --> JBoss
    from 8.x-->    WildFly
         versioning  
    git branch and git tag 

  WebLogic --> Oracle  


  WHO CAN DEPLOY TOMCAT APPLICATION?

  For you to be able to access and deploy Tomcat application, you need to be 

  1. added to or you are ne of the tomat users  which can be found in tomcat-users.xml (following the absolute path i.e  /opt/tomcat9/conf/tomcat-users.xml> )

  2. You need to have role of a manager-gui and admin-gui permission. If you only have manager-gui, you will to be able to deploy tomcat and also have access to tomcat back-end

NOTE:

  MAVEN is our build server

  TOMCAT is our deployment server

  NOTE:  To know the private ipaddress of your server, run

  hostname -i

Note;

tHE COMMAND THAT CAN BE USED TO COPY A FILE FROM ONE SERVER TO ANOTHER IS THE 

scp 

EXAMPLE

Let us do a build in maven and do the deployment in tomcat application SERVER

1. Perform an a build once again with one of the web application in prof projectRepo by running

mvn clean package

This will delete old build and create a new build package in the target directory

2. cd into the target directory in order to see the build package
   let us assume that the build package is  

  tesla-webapp.war   which is always in red color

  USING CLI TO DEPLOY WEB APPLICATION IN TOMCAT

3. copy the build package from maven server to tomcat server by running in maven server

scp -i sshkeyPem tesla-webapp.war  ec2-user@privateIP absolutePathToWebapps

This is because deployment take place in the webapps directory    in tomcat.   that is

scp -i maven.pem tesla-webapp.war ec2-user@privateIP /opt/tomcat/webapps

We are using the private ip because they are in the same environment or using the same local computer

NOTE. The password used here is the maven sshkey pem while the private IPaddress used here is the tomcat private ipaddress

scp -i maven.pem tesla-webapp.war ec2-user@privateIP /opt/tomcat/webapps

If you run the above command,

It will copy the .war red colour file to tomcat webapps.

Go to tomcat server

ls opt/tomcat/webapps

1. You will notice that the tesla-webapp.war file is prersent is red because it is a zip or compressed file
2. You will also notice that the tesla-webapp.war file has be unpacked or unzip to because tesla-webapp directroy which is blue color. It is called an unpacked file.

If you go to your tomcat web application manager and refresh the page, you will notice that a new unpacked file called    tesla-webapp

NOTE:

If you want to deploy in tomcat via CLI in maven, you can a run a command in a way that it name of the .war in maven will be changed to another name.war when it get tomcat server.

Let us say the build artifacts or package in maven is tesla-webapp.war and you want it to change to td.war when it gets to tomcat server, you can run

scp -i maven.pem tesla-webapp.war ec2-user@privateIP /opt/tomcat/webapps/td.war

What is deployment in Tomcat. Using CLI

Deployment in tomcat using CLI is simply copying the .war file build package from the build server into the tomcat webapps directory

Note: To check if the application you deployed in tomcat is running of not, in verbose mode by running

curl -v tomcatPublicIpAddress The-unpacked-deployed-application

The-unpacked-deployed-application here is tesla-webapp   without  .war because it has been deployed and tomcat only record the unpacked or unzipped application in its tomcat manager site.

That is

curl -v tomcatPublicIpAddress:8080 tesla-webapp

the port for tomcat is 8080


VIDEO 57

Note;

You can also install tomcat using the aws configuration pattern by addding the script when trying to create tomcat server in aws
This can be done while creating a tomcat server in aws, go to advance details and paste the below shell script and then save before you launch the server.

#!/bin/bash
#use this shell script to install tomcat
#echo assign hostname to your server 
sudo hostnamectl set-hostname tmt2
sudo -su ec2-user
#install Java JDK 1.8+ as a pre-requisit for tomcat to run.
cd /opt 
sudo yum install git wget -y
sudo yum install java-1.8.0-openjdk-devel -y
sudo yum install unzip -y

# download tomcat software form the internet and extract it.
#https://tomcat.apache.org/download-90.cgi to get the latest version
sudo wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.74/bin/apache-tomcat-9.0.74.tar.gz
sudo tar -xvf apache-tomcat-9.0.74.tar.gz
sudo rm apache-tomcat-9.0.74.tar.gz
#to rename the directory to tomcat
sudo mv apache-tomcat-9.0.74 tomcat2 
#to change permission to full permission     
sudo chmod 777 -R /opt/tomcat2
#to change ownership to ec2-user
sudo chown ec2-user -R /opt/tomcat2   
#to start tomcat2
sh /opt/tomcat2/bin/startup.sh    
# create a soft link to start and stop tomcat
sudo ln -s /opt/tomcat2/bin/startup.sh /usr/bin/starttomcat 
sudo ln -s /opt/tomcat2/bin/shutdown.sh /usr/bin/stoptomcat
#update all the software
sudo yum update -y
starttomcat
#the installation script ends here


NOTE:

anytime you log out of your system, when you come back, always start tomcat by running the command

starttomcat

After you have launch it in aws, if you go to you linux server by connecting the tomcat server redhat that you have just configured to your mobalexterm with ssh key, you will see tomcat that the hostname and everything you cobfigured is running. cd into /opt and you see that the tomcat directory is there too. If you grep for tomcat by running

ps -ef | grep tomcat

you will confirm that tomcat is running

After tomcat has been installed, and running properly

You can now deploy the application from the build server called maven to the webapps directory of the deployment server or application server called tomcat. 

Jboss is also deployment server or application server for both MWA and MEA.

IQ

HOW DOES DEPLOYMENT TAKES PLACE IN TOMCAT?

Deployment in tomcat takes place by simply copying the web application build package or build artifacts containing war package into the webapps directory of the deployment server called tomcat.  

After deployment to tomcat from the build server, the war package will appear in tomcat server in 2 faces. The packed file that will end with .war in red colour and the unpacked directory in blue color with no .war

NOTE: You can change or rename the war package during deployment using scp command from the maven server by adding the directory name you wish the war package should appear in tomcat server after webapps. that is 

Let say you want the war package to appear in tomcat server as boa.war

but the war package is tesla-webapp.war   in maven server

You can run the command in maven and add boa.war at thye end. that is 

scp -i maven.pem tesla-webapp.war ec2-user@privateIP /opt/tomcat/webapps/boa.war

it will appear in tomcat server as 

1. boa.war          this is the packed package, red colour

2. boa              this is the unpacked package, blue in color. This what will show in tomcat web manager

instaed of 

scp -i maven.pem tesla-webapp.war ec2-user@privateIP /opt/tomcat/webapps

that will appear in tomcat server as

1. tesla-webapp.war          this is the packed package, red colour

2. tesla-webapp              this is the unpacked package, blue in color. This what will show in tomcat web manager

Note: if you rename the war package to be something else without ending with .war

EXAMPLE

In my running Note,

renaming the war package to appear as just boa in tomcat web page

scp -i maven.pem /home/ec2-user/jp/mwa/target/tesla.war ec2-user@privateIP:/opt/tomcat/webapps/boa

 the package will deploy appear as packed package in tomcat but will not appear as unpacked package. 

If it is not unpacked, then the package will not be running in tomcat. It is the unpacked package that will be running successfully in tomcat web manager

This is illustrated as

scp -i sshkeyPem /home/ec2-user/jp/mwa/target/tesla.war ec2-user@privateIP:/absolutePathToWebapps

This is because deployment take place in the webapps directory    in tomcat.   that is

Before run the command for deployment in maven, make sure that you are in the home of ec2-user 

To go to the hopme of ec2-user, run

cd ~

The absolute path of webapps in my tomcat2 installation is /opt/tomcat2/webapps

sudo scp -i maven.key /home/ec2-user/jp/mwa/target/tesla.war ec2-user@172.31.19.175:/ec2-user/opt/tomcat2/webapps


This is because deployment take place in the webapps directory    in tomcat.   that is

Before run the command for deployment in maven, make sure that you are in the home of ec2-user 


To go to the hopme of ec2-user, run

cd ~

before you run the command for deployment to tomcat by using

scp -i maven.pem AbsolutePathTotesla-webapp.war ec2-user@privateIP:/opt/tomcat/webapps

tesla-webapp.war   this is the build package resulting from MWA
ec2-user@privateIP    this is maven ec2-user and maven private ip

Let us run mvn clean package to delete and auto build the maven web app and then, let us deploy the build package to tomcat webapps directory to appear with another name in tomat manage page

If you want to deployment to appear with another name in tomcat manager page or tomcat backend, run the below

sudo scp -i maven.key /home/ec2-user/jp/mwa/target/tesla.war ec2-user@172.31.19.175:/opt/tomcat2/webapps/td

You will notice that it appears as td and td.war in tomcat server but as td in tomcat web manager or tomcat backend. Tomcat web manager only recongnise the 

NOTE

Ngynx web-server is a very good loadBalancer server. it will also act has a security and a proxy to the applicatiion server or deployment server 

WATCH COMMAND

Watch command is used to see changes every 2 second. by running

watch the abolutePathToWhatYouWantToWatch

let say you want to watch your deployment in webapps of your tomcat, you run

watch /opt/tomcat/webapps

You are watching the deployment that is taking place inside the webapps directory of your tomcat server

VIDEO 58

IQ
WHY DO YOU THINK THAT WE SHOULKD NOT HAVE ALL OUR APPLICATION DEPLOY IN JUST ONE SERVER?

Answer:  We should not because if we do, if something goes wrong with the server, the whole of your infrastucture will be down. This is called a disaster which decrease performance. decreases reliability. It is better to use multiple server to increase performers and to keep your application still up and running no matter what happened to one of the server.

IQ

WHAT ARE THE THINGS THAT CAN AFFECT TOMCAT SERVER PERFORMANCE?

Answer:

1. multiple deployment to a tomcat server can affect the server performace. 
2. Multiple requests or too much traffic being routed to a single tomcat server can affect the performance of the tomcat server
3. the resource level or the computing capacity of the tomcat server can affect the server's performance. The resource level or computing capacity has to do with what server contains. If computing capacity is of higher level. Quality is at play here. Ofcourse, the quality of the deployment server would affect performance.

For example. 

Tomcat server with 64GB or RAM and 4000GB of hard drive SSD will perform better than a server with 8GB of RAM and 480GB of hard drive HDD

A camera with 4k will have a better performance to a camera with 2k

IQ

HOW DO WE INCREASE THE PERFORMANCE OF OUR TOMCAT SERVER?

1. You can increase the heap size of our deployment or application server in order for it to be able to withstand or handle multiple deployment.

Heap size is means the amount of space or size that is assigned to our server by default

If the heap size is 64MB of RAM, you can decide to increase it to 1000MB

2. To handle the issue of mutiple user's requests on a single tomcat server is to create multiple tomcat server and route traffic to them using Ngynx web server to balance the traffic of load to each tomcat server created. Ngynx is a good load balancer.

3. You can increase the computing capacity or quality of the server to achieve a better or higher performance.

NOTE; wHEN i RUN FULL PERMISSION ON MY MAVEN SERVER. I NOTICE THAT MY MAVEN DIRECTORY BECAME GREEN COLOR INSTAED OF BLUE. i HAD TO RUN NORMAL PERMISSION TO RESTORE IT BACK TO NORMACY

FULL PERMISSION: sudo chmod 777-R dir/file

NORMAL PERMISSION: sudo chmod 755-R dir/file


WEB SERVERS

What is Webserver?

A Web server is a program that uses HTTP (Hypertext Transfer Protocol) to serves web content (HTML and static content) to users.

IN MY WON WORD

Application server also know as deployment server is a server that deploys build packages or artifacts from build server to the clients application via different protocols like http in an n-tier architecture like (3-tier architecture, 2-tier architecture, etc. n= 2, 3, 4 etc)

Examples OF Web servers are:

Apache HTTP webserver
Nginx webserver (pronounced engine X)
IBM HTTP server (IHS)
Oracle iplanet web server
Internet Information Server (IIS)

APACHE WEB SERVER INSTALLATION (HTTP)

To install apache in the tomcat server, run

1. sudo yum install httpd -y       y for auto-approve or auto-yes 
2. sudo systemctl start htppd
2. sudo systemctl enable htppd

To check if apache webserver (http) or httpd is running. run

another name for http websever i called HTTPD   Hypertest transfer protocol daemon

HTTPd stands for Hypertext Transfer Protocol daemon. It usually is the main software part of an HTTP server better known as a web server. Some commonly used ...

ps -ef | grep httpd     or   curl -v localhost.80  or curl -v tomcatPrivateIP   in tomcat server or  application server

it will show that apache webserver is running in tomcat server  but if you 

url -v tomcatPublicIP in tomcat server  and it says, trying to connect, it means you need to check the firewall by reconfiguring the tomcat server by including port 80 and select my IP so that tomcat can completely run

It runned with url -v tomcatPrivateIP because it is an in-house or from a localhost. Using the same local computer system but in different environment

If you want to acces apache or check acces apache if running on your internet browser, you only use your public IP address on port 80 because internet browser is a public place

if you want to check in apache is running on your tomcat or linux server, you only use your private IP address on port 80 or localhost on port 80 because they are local environment or the same computer system in executing the command


if you want to access apache in another computer system or external host or environment, apache port need to be included or configured in the application server host like aws ec2 redhat server in order to access it in a public domain or in another computer 

Note: apache webserver (http) runs on port 80
      Jenkins/tomcat/Jboss runs on port 8080
      https runs on port 443

  HTTPS (Hypertext transfer protocol secure) is the secure version of HTTP, which is the primary protocol used to send data between a web server and a web browser.

  A url or link should startes with https which shows that the url is secured.

 HTTPS is more secure than HTTP because it uses encryption to protect information as it is being sent between clients and servers. When an organization enables HTTPS, any information you transmit, like passwords or credit card numbers, will be difficult for anyone to intercept

 HOW DO I ACCESS APACHE WEBSERVER ON MY INTERNET BROWSER?

 Since apache has been installed and confirmed, copy the IP address of the tomcat or application server you installed apache on and add :80  SINCE APACHE RUNS ON :80 AND THEN PASTE EVERYTHING ON YOUR INTERNET BROWSER.

 That is 

 TomcatPublicIpAddress:8080

 if it shows 404  no access, then you have to check the firewall of your server. 

 Go to your aws account and reconfigure YOUR TOMCAT OR APPLICATION SERVER's security group and click on the security groupName, and click on the security group link below, edit the inbound rules, click on add rule by adding port 80 on the tcp, click on custom and select "my IP" and then save the changes.

 run the command again    TomcatPublicIpAddress:80
 You will found out that it will run

 What is the default home directory of apache webserver?

 When you install apache, it comes with a default home directory

 APACHE DEFAULT HOME DIRECTORY is 

 /var/www/html

 you can long-list what is in  /var/www/html by running

 ll  /var/www/html

 Let us write a code or text on index.html file

 The index.html file is the only default file that you can vim into, to start a new content, add content or modified anything that is to be transmitted and accessed by http webserver or apche webserver

 sudo vim  /var/www/html/index.html         and add

 welcome to landmarkTech. A very powerful and softisticated software company

 The above code or script is a normnal text code or xml code

 You can access this code on the internet browser by pasting the application server called tomcat publicIpAddress on port 80 (3.22.224.202:80) in your internet browser.

 For an html code, it starts with < > and ends with </ > with the code at the centre of the angle brackets. like the follwoing

 <hi>welcome to landmarkTech. A very powerful and softisticated software company</hi>

 the above code is an html code or file which has hi as the pre-defined tag by default.

in coding or writing code,  h1 means primary heading or heading 1

if you passing the tag as <h1> </h1>, it means whatever that is in-between the bracket would be a heading

<h1>welcome to landmarkTech. A very powerful and softisticated software company</h1>

in the above, "welcome to landmarkTech. A very powerful and softisticated software company" would appear as a heading in apache web server web page

but

if there is no tag passed, that is 

welcome to landmarkTech. A very powerful and softisticated software company  would be just a text

Soi whenever you want to assigtned heading to a code, you should include an html tag to it.

For example 1:

<h1>Welcome To Marsh Hill Homes</h1>

A place where individuals with disabilities are well supported

Example 2

<h1>Letter To My Wife</h1>

Sweetheart, I just want to let you know that I did treasure and appreciate your love, care and support you have been rendering to me and the kids through out this trying times. Though you could be very hot tempered sometimes but I understand that you are frustrated with the happenings of this period but be rest assured that all will be well very soon and we will get through this togther. Do not think about the sale of the house. I just want you to be OK health-wise because the children needs you and I need you too.

Be strong for me and the kids. I just want you to know that you are much more appreciated and loved by us more than what you can imagine.

Cheers to a better life

Cliffosa

Note. 

1. To see the PublicIpAdress of your server, you run

curl ifconfig.ca

1. To see the PrivateIpAddress of your server, you run

hostname -i

HOW TO DISPLAY IMAGES OF PICTURE IN THE INTERNET BROWSER USING WEB SERVER

You can also copy or display images to be accessed by internet broowser using webservers like apache web server.

Example:

Let us copy a picture from our local lap via git bash

Go to where you have your picture in your laptop. Copy  or move the picture to downloads.

go to your git bash and then cd into downloads

you have to copy the picture from your git bash server to your webserver by using the scp command. You can run

scp -i shhkey nameOfTheFile ec2-user@PublicIpaddressOfYourWebserver:/TheHomeOfEc2-user

scp -i shhkey nameOfTheFile ec2-user =  is the info from your gitbash server. The ec2-user here illustrate that the sshkey is from ec2-user 

@PublicIpaddressOfYourWebserver:/TheHomeOfEc2-user=  is the info from your webserver.

That is

scp -i maven.key.pem license.jpg ec2-user@18.222.177.12:/home/ec2-user

When you check your tomcat/webserver, you will see that the file has arrived.

Note: Always make sure that your tomcat server is running by

ps -ef |grep tomcat

, if it noit running, run

starttomcat 

Since it is image or pciture you want to display, you cannot create or modify the index.html file of /var/www/html because it is only meant for content display and not for image display.

You can only move the file to /var/www/html  using the move command. run

sudo mv license.jpg /var/www/html

ls /var/www/html         to confirm the move

Note: Any image must awalys end with .jpg for it to run.

When you try to access the image by pasting the publicIpAddress on your internet browser.

If the image did not come up, add   /nameOfYourFile after your IpAddress. That is

http://18.222.177.12/license.jpg    to know why it did not appear

If it says forbidden, you dont have the permission,. Then run

Fimd out why you are unable to read or access the file in the internet browser or publick domaine by running

ll /var/www/html/license.jpg    below is what it shows

-rw-r--r--. 1 ec2-user ec2-user 3910204 May  7 14:25 /var/www/html/license.jpg

The file license.jpg permission says that

rw            r         r    
owner        group     others
read/write   read    read

This means that owner can read/write. group can only read, others can only read

says that ec2-user own the file. No other user.

If you change the permission to rw all through, by running

sudo chmod 666 /var/www/html/license.jpg

and it still says no permission,  maybe it is because it is owned by ec2-user

You need to make a copy and rename it. If you make a copy/rename the copy to another name, you have make the new copy to be owned by root, then you can now be able to access the file in the internet. that is

sudo cp /var/www/html/license.jpg /var/www/html/license.image

run

ll /var/www/html/license.image

-rw-r--r--. 1 root root 3910204 May  7 15:12 /var/www/html/license.image

The file is now owned by the root user.  if you access the file in the internet now by copy/paste on your internet browser.

18.222.177.12/license.image

The image will now appear

Note: If you just paste only the publicAipAddress on the internet browser without passing any file to it like 18.222.177.12/license.image, it will read from the index.html file by default thereby showing only the content that is in index.html file.

The index.html file is the default file for apache web server  

and if you want any other file in your tomcat/webserver to show on the internet browser, you must pass or add /nameOfNewFile

Example 3.

ls  /var/www/html

You will see that we have other files like

index.html  index.xml  license.image  license.jpg  test.html  test.xml

Let us access index.xml by pasting 18.222.177.12/test.xml or 18.222.177.12/test.xml, 18.222.177.12/index.xml  on the internet browser, you will notice that all of them shows error. but when you paste 18.222.177.12/test.html file, it opened successfully.

The accepted one is the one that has .html at the end of the filename but in the case of image or pciture display, you can use any filename to save it.

Note: Before any web content or images can be displayed and access successfuly on the internet browser, it must be a root user. Before you run the command for display, check by running

ll absoluteLinkOfTheFile      to see the owner. 

if it is owned by ec2-user, just simply copy the content to another new file by running

sudo cp absoluteLinkToTheOldFlieName absoluteLinkToTheNewFlieName

CHANGING PERMISSIONS

Let us create a file called pm

sudo vim /var/www/html/pm  

add a content "I love DevOps so much'     then run

ll /var/www/html/pm        ll  means long list

it will give you

-rw-r--r--. 1 root root 22 May  7 17:55 /var/www/html/pm        this means

rw            r         r    
owner        group     others
read/write   read    read

owner have read/write, group has only read, others only have read


1. let us change the permission of the file by running

sudo chmod 400 /var/www/html/pm   

The above command give only read access to only the owner. Group and other is zero access

ll /var/www/html/pm 

You will notice that only the owner has only read access, group and others has zero access

2. let us change the permission of the file by running

sudo chmod 444 /var/www/html/pm   

The above command give only read access of the file to the owner, group and other

ll /var/www/html/pm 

You will notice that the owner, group and others only has read access to the file

Note: APPLICATION SERVER LIKE TOMCAT IS IN THE CLIENT'S ENVIRONMENT

SUMMARY OF END TO END DEPLOYMENT TO THE APPLICATION SERVER

1. A group of developers would write an application and commit/push the applicatios to a source code manager like github. The application will come with the source, unitTestCases and BuildScript.
The build script is what is called pom.xml which have the details of the type of application that the developers have written.

2. A build technology like Maven software will run a git pull or git clone command to clone the application from the github projectRepo to the maven server to do a build.

3. after a build has been done, packages like jar or war or ear packages would be created on the target directory of the application Repo, depending on what the developers has written

4. If it is a web application that developers has written, war package would be created on the target directory ready for deployment

5. maven would deploy the war package via scp command in maven server to the webapps of the application server like Tomcat. Webapps is where tomcat package deployment takes place.

6. If there is multiple tomcat server to avoid overloading of one single tomcat, a technology know as ansible would be used for auto-deployment to the multiple application server like tomcat1, tomcat2 ----even to tomcat100. It will be a tideous job for maven to deploy build pacakages to app. server. The use of aNSIBLE WILL MAKE THE WORKER BETTER AND FASTER.

7. Ansible is automation software/tool that deploys build artifacts from the build server swiftly to a multiple application server. No matter how many app. server it is, the time it will use to deply to one app. server would be the same time it will deploy to 1Million app. server.

   EXPOSING THE APPLICATION TO END USERS

8. The above set-up has be done, the app. server would have to be utilized by use of a webserver which would stand as a proxy to the app. server

9. The webser like apache webserver (http with port 80 or https with port 443 but https is more secured that http) or Ngynx will receive the traffic from end-users and then rout the traffic to the multiple app. server. It will act as a load balancer
a. the webserver will act as a loand balancer for app. server, routing traffic or sharing users to each of the app. server according to their capacity so thatthey will not be overloaded
b. It will serve as a proxy or serious guard to the app. server for securty reason. so that if there is any cyber attack or hackers, they will not have direct access to the app. server or to the backend end of the application.
The user can only access the public ipaddress of the webserver like Ngynx and so whenever they want to make do of the app., they can only access it via the webserver without direct access to the backend or to the app. server.

NOTE: The end-users can only view or see the ipaddress of the web server and not the app. server 
thery only have direct access to the webserver

SONARQUBE

Sonarqube is an open source quality management tool which continuosly measure and analyses the quality of the source code.  Sonarqube check out the quality of the source code.

Sonarqube does codeQuality analyses. It is for ciodeQuality check

Sonarqube is used to run a static code analyses.

It is used to set standard or benchmark for the type of source code that would be accepted

Sonarqube is used for benchmarking. It is used to create a standard of the type of code that would be accepted.

Sonarqube was initially called Sonar

WHAT DOES SONARQUBE CHECK IN A CODE?

Sonarqube check the following in a code

1. duplicated code. It check if there are duplicates in the code
2. code standard
3. the unitTest cases of the code. It check if the unittest cases of the code matches with the lines of the code. It re-test the unit test that was done by developers.
4. complex code
5. comments. If the comments explains the destination of the code properly
6. Potential bugs. It also check if there are bugs on the code
7. architecture and design 

For example

A developer wrote a code with 10,000 lines but wrote 4,000 Lines of unitTestCases. An mvn package was performed and maven made a build successfully after re-testing the unit test cases that developers has written.

But for Sonarqube, the unitTtestCases of 4,000 lines for 10,000 lines of code is not a good application and would not be accepted by sonarqube because every line of code is supposed to have a unit test case.

With Sonarqube, we can set-up our benchmark or standards.

SONARQUBE BENCHMARK FOR A QUALITY SOFTWARE OR APPLICATION

Sonarqube may set-up the benchmark or standard of what is expected from developers. Like the following:

1. Code coverage should be greater than 85%. This means that the percentage of the unit test cases by developers should be greater than 85%

2. Duplicate lines should be less than 5%. There should not be duplicates line that would be more than 5

3. There should be no vulnerability in the code. This is for security purpose. A situation where users can created a password on an application that allow 1234 as a complete password. This password is vulnerable for hackers to crack or guess. The application or code should not allow that type of password creation.

4. code smell should be less than 5

5. The code must be readable with ease to avoid code smell. It is not not ready, it means, it a code smell.

How can you make your code readable?

Your code can be readable by the use of comments to explain what you stand to achieve.

6. The code or application should be portable. It can easily accessed anywhere in the world. (environmental independent). If that is not the case, the code smell or it is a hard code

      HOW SONARQUBE IS STRUCTURED OR PUT TOGETHER

1. Once Developers has pushed the code or application to github, before a build is done, the code passes through a SONAR SCANNER who will scan the code and then push the code to SONAR SERVER.

Sonarqube has a sonarqube scanner and sonarqube server.

2. The Sonarqube server is like a container that has four components which are

 a.  Compute engine= This will do the calculation in terms of percentage. Claculation how many code smell that was captured, it is able to calculate that the code coverage is 70%, duplicate code lines is 30% etc

 b.  Search Engine called Elastic serach (ES)

 c. Web server: This make it possible to be able to access sonarque on the web browser or internet browser. E;g apache webserver (https webserver)

 d. Database (DB) component: this is a container of datas

 Types Of Database: There are 2 types of database namely, MySQL and NoSQL database

 1. MySQL =  My Structured Qwery Language: This is the type of data that is used to store a strutured data. A staructured data is the type of data that uses roles and columns. A data that is arranged in a table format. In the form of 

      USES ROWS AND columns  

SN    Name      DOB         MATRICULE
1001  Dominion  11-Jun-80    LT01247
1005  Rosie     20-May-82    LT04782
1006  Paul      21-May-82    LT04783
1007  James     22-May-82    LT04784

This is the way Microsoft excel arrange documents.


MySQL is called a RELATIONAL DATABASES (RDS)

If you company wants a database that can capture data in a table format (roles and colums), you would advise them to use MySQL database

Example of MySQL database are

    Oracle
    mySQL  - OS
    PostgreSql  - OS
    MSSQL (MicrosoftSQL)
    DB2
    Sybase
    MySQL, Oracle, Microsoft SQL Server, PostgreSQL


2. NoSQL = Non-Structured Qwery Language: This is the type of data that is used to store an unstrutured data. It is also called a NON RELATIONAL DATABASES (Non-RDS)

UnStructured data are in the form of a Json format and others:

 Exanple of JSON format is below as an unstructured format are: 

    user1{
      name: 'simon'
      id: 01245
    }
    user2{
      sn: 1008
      name: 'paul'
    }
    user100{
      dob: '01/05/1974'
    }

other examples of unstructued format are:

 user1: Name=Simon 
  user2: ID=01245 
  user3: dob=01/01/1980 
  user3: tel=12457861
  user100: email=100@hotmail.com

  Exampls of NoSQL database are

    MongoDB ---
    Cassandra
    CouchDB, 
    CouchBase, 
    HBase, 
    Redis, 
    Riak, 
    Neo4J

 				SONARQUBE INSTALLATION

Below are the installation procedures for sonarque

Prerequisite
AWS Acccount.
Create Redhat EC2 T2.medium Instance with 4GB RAM.
Create Security Group and open Required ports.
9000 ..etc
Attach Security Group to EC2 Instance.
Install java openJDK 1.8+ for SonarQube version 7.8

         NOTE

1. Create sonar user to manage the SonarQube server

As a good security practice, SonarQuber Server is not advised to run sonar service as a root user, YOU NEED TO create a new user called sonar and grant sudo access to the new user to manage sonar services as follows

#create a username
sudo useradd sonar
#Grand sudo access to sonar user
sudo echo "sonar ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/sonar

# set hostname for the sonarqube server
sudo hostnamectl set-hostname sonar 
sudo su - sonar

#Assign password to sonar user
sudo passwd sonar

#Enable PasswordAuthentication in the server
sudo sed -i "/^[^#]*PasswordAuthentication[[:space:]]no/c\PasswordAuthentication yes" /etc/ssh/sshd_config
sudo service sshd restart

#Install Java JDK 1.8+ required for sonarqube to start
cd /opt
sudo yum -y install unzip wget git
sudo yum install  java-11-openjdk-devel

#Download and extract the SonarqQube Server software.
sudo wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-7.8.zip
sudo unzip sonarqube-7.8.zip
sudo rm -rf sonarqube-7.8.zip
sudo mv sonarqube-7.8 sonarqube

#Grant file permissions for sonar user to start and manage sonarQube
sudo chown -R sonar:sonar /opt/sonarqube/
sudo chmod -R 775 /opt/sonarqube/

#start sonarQube server

sh /opt/sonarqube/bin/linux-x86-64/sonar.sh start 
sh /opt/sonarqube/bin/linux-x86-64/sonar.sh status

You can also know if sonarqube is running on your sonarqube server by running

curl -v localhost:9000

Note: anytime you stop your sonaque server in your redhat ec2 aws account, when you resume work, always start the sonarqube on your sonar server before running any task on your sonarqube server by running

sh /opt/sonarqube/bin/linux-x86-64/sonar.sh start 
sh /opt/sonarqube/bin/linux-x86-64/sonar.sh status

You can now access sonarQube on the internet browser by copying and pasting the below on your internet browser

SonarquePublicIpAddress:9000    that is

18.117.228.35:9000

Sonarque runs on port 9000 by default

You can know your public and private IP address by running

curl ifconfig.ca   and     hostname -i     

on your sonarque server to know the public IP address and private Ipadress respectively

When you access your sonar server on your internet browser, you can get into the account by default by running

default USERNAME: admin
default password: admin

NOTE

The home Directory of Sonarque is extracted.

/opt/sonarque

HOW DO YOU EXTRACT THE SONARQUBE?

You ususally downlaod sonarque from the internet using wget and use the unzip command to extract sonarqube when it is still in zip format.

sonarqube is extracted from the sonarqube home directory.

Some company sonar home directory is

/apt/sonarque     companies that uses ubuntu  ec2 aws server

Thi home directory is the directory where sonarque is extracted

If you want to start sonarqube, you must be on your sonar username and run the below because you actually created another user for the installation of sonarqube

sonar.sh  is used to start, stop, status your sonarqube and it is found in your bin directory.

The absolute path to sonar.sh to start sonarqube is 

sh /opt/sonarqube/bin/linux-x86-64/sonar.sh start 

The absolute path to sonar.sh to know sonarqube status is

sh /opt/sonarqube/bin/linux-x86-64/sonar.sh status

[sonar@sonar sonarqube]$ ls
bin   COPYING  elasticsearch  lib   temp
conf  data     extensions     logs  web
[sonar@sonar sonarqube]$
[sonar@sonar sonarqube]$ ls /opt/sonarqube/

Directories inside sonarqube server

 bin:
    [sonar.sh] 
    sh sonar.sh start   = using the relative path
    sh /opt/sonarqube/bin/linux-x86-64/sonar.sh start 
         start|status|stop|restart  = absolute path

 conf:
   vi /opt/sonarqube/conf/sonar.properties 
   vi sonar.properties 

 logs

 data     
 extensions     
  COPYING  
 elasticsearch  
 lib   
 temp 
 web

 Free version and languagues :

 Sonarqube supports multiple languages

  Java,  = 30,000 lines 
  JavaScript,
  C#, 
  TypeScript,
  Kotlin,
  Ruby, Go, Scala, Flex, Python, PHP, HTML, CSS, XML and 
  VB.NET

  Java, C#, JavaScript, TypeScript, 
  CloudFormation, Terraform, Kotlin, Ruby,
  Go, Scala, Flex, Python,
  PHP, HTML, CSS, XML and VB.NET

  LET US RUN A PROJECT WITH SONARQUBE AS AN EXMAPLE

Let us clone web application from Prof remotrepo from giuthub to our maven server by running

 sudo git clone https://github.com/LandmakTechnology/maven-web-application.

 If already have a web apllication cloned already in your maven server, you can use it as an exmaple. The url is below

Let us set our time zone to America time-zone by running

 sudo timedatectl set-timezone America/New_York

 Procedures for sonarqube to do a codeQuality analyses

1. Developers will commit and push code to source code manager like github 

2. Testing + Build would be done by a build software like maven. Using maven in this, run
    mvn package 

3. CodeQuality analysis with SonarQube. With sonarqube, you do not need to run mvn package, just run

    mvn sonar:sonar   

It will do the codeQuality analyses and also do a build. Sonarqube run the codeQuality analyses and if passed. it will do a build

 We have sonar in the left and right side above
  The  left handside sonar is a goal. that is, sonar = goal
  The  right handside sonar is a plugin. that is, sonar = plugin
             :    
         1. sonar = goal  
         2. sonar = plugin

If you run the  mvn sonar:sonar  command for codequality analyses, it will say sonar server cannot be reach. 

This is so because you need to change the ipaddress of the web application pom.xml to sonar private iPaddress by vi into pom.xml of the web application server and go to the properties tag and change the ipaddress to sonar Ipaddress. That is call

MAVEN-SONARQUBE INTEGRATION

To integate maven and sonar, maven has to be talking to sonarqube directly by configuring sonar private ipaddress to the pom.xml of the maven web application

     vi pom.xml in maven server and enter sonarqube server   
     credentials in the properties tag   

           userName=admin
           Password=admin
           ip-address=172.31.5.96   :


sudo vim pom.xml

change the Ipaddress to your sonar Private IpAdddress

You can also change it to sonar server public address if you are using different VPC, that is, if you are using different network or using the same vPC. If i am using aws cloud computing company and the other server is not hosted by aws

VPC means VIRTUAL PRIVATE CLOUD

Since I am using the same VPC or using the same network, I will have to use sonar private Ipaddress

EXAMPLE 2

Let us use the MSA in our maven server to run sonarqube codeQuality analyses by cleaning the build packages and do a nw build.

cd msa

integrate maven with sonarqube my changing the Ipaddress located at the properties tag in the pom.xml of MSA to the private Ipaddress of the sonarqube server by

vim pom.xml    and edit the ipadress  and then quit/save. Then run


mvn clean     to remove the old build  and then

mvn sonar:sonar 

 
 With some comapny using Enterprise version of Sonarqube. This is what happens

 When developers commit/push their code to staging environemnt in git branch, a pull request (PR) will be activated and that pull request will trigger sonarqube CodeQuality analyses. The Engineering that will be reviewing the PR will also be reviewing the sonarqube codeQuality analyses. If there is a pass, before maven can now pull from sanarqube and do a build.

 NOTE: SOME COMPANY DOES CODE QUALITY ANALYSES BEFORE A BUILD WHILE SOME DOES A BUILD BEFORE CODE QUALITY ANALYSES AND IF THE CODE FAILED THE REQUIRED CODE QUALITY AFTER THE ANALYSES, THAT PROJECT WILL NOT BE DEPLOYED.

 iT IS BETTER TO DO CODEQUALITY ANALYSES BEFORE A BUILD.

 QUESTION ASKED.

 What is the difference between SONARQUBE AND SONARCLOUD

 Sonarqube is software that is being installed in aws cloud distributors like redhat ec2 server. The process is that you craete a redhat ec2 server and connect the server to an ssh clients

Examples of ssh clients are

MobaXterm  software in windows OS
Terminal in Mac OS
Putty
etc

Once the redhat server is connected to the ssh clients and then sonarqube software is intalled on the server, it now becomes a Sonarqube server.

SonarCloud. With sonarcloud, you dont need a server. You can your register or create an account in sonarcloud site and then you can be running all you codequality analyses in that sonarcloud account. Android does their codeQuality analyses with sonarCloud, So many companies does it with sonarCloud.

NOTE:

Anytime you shutdown you sonarqube server in aws, you need to start it when yoyy get back.

If you want to start sonar, you just start it from you sonar user or another user. 

Toubleshhot ins Sonarqube

If you try to start sonarqube from a root user, it will not start. By the time you realsied and decide to start sonarqube with the sona user you created, it will not start also because a temp directory has been created.

What you need to do is that, you would have to check the logs directory by cd into sonarqube and then cd into logs, then cat the file sonar.log. That is 

cat /opt/sonarqube/logs/sonar.log   

you will see that why tring to start sonarqube from the root user, it failed to start but when start it from sonar user we created, it failed too because of the fact that we attempt to it once with root user and temp directory was created.

We need to delete the temp directory and then start sonar again from the sonar user we created. that

sudo rm -rf /opt/sonarqube/temp

When you start sonarqube, it will be started.

To confirm if it is running,

ps -ef | sonarqube

IQ

What problem did you faced recently in your project

Problem. Sonarqube refused to start
Analyses: We discovered that unauthorized user was trying to start sonarqube as the root user, when we cat sonar.log file which is located at the logs directory in sonarqube. That is

cat /opt/sonarqube/logs/sonar.log

Solution: What we did was to delete the temp directory that was created as a result of the unaruthorized individual. We deleted the temp directory that is located in sonarqube by running the command

sudo rm -rf /opt/sobarqube/temp


COMPONENTS INSIDE SONAQUGE WEB PAGE


Projects
Issues
Rules
Quality Profiles
Quality Gates
Administration

1. QUALITY profile. Quality Profiles are collections of rules to apply during an analysis.
To create you quality profile, click on quality profile, click on create, type in your project name and the programming language of your project, click on none.  then activate more to activate more rules you needed. whne you click on activate, you need to sElect if it is a major, minor etc.

In your quality profile, where ever the default is on the different project present, will be the project that would receive codeQuality analysis by default whenever a build is carried out. Please take note.

If you want to change the directory to the prject you want, click on quality profile, select the programming language to java and the go to the project you want and click on the settings symbol opposit the project and then click, set as default. so whenever you do a build, the profile that you create with rules you like will be the profile that will do the quality analysis of the code that has been or about to be build by a build technology like maven.

2. RULES: These are rules that have been set using quality profile. You can view or see the rules that have been set like bugs in the code, vulnerability in the code, security, dunplicated lines, code smell etc. A collection of these rules that makes up the quality profile

3. quality gate This is used to set the percentages or number of what you want in every rules that you have set. It is use to set the benchmark or standards of the rules that was set. For example, how many percentage or number code smell that you pardon, number of code coverage that is accepted etc.

You can create your quality gate by clicking on quality gate and click on create at left top side, and name it, to add conditio, click on condition, select coverage on new code, then enter the number of percentage you want on the error,if it is less than 90% you selected,it means that if your code coverage is less than 90%, then the code coverage is error. not passed, add more condition like, duplicated lines. If you sent it for, duplicates lines great than 5% error, it means that if the dup. lines is more than 5%, it means that there is error, that is not passed. You can add bugs, and set it for 5, add new vulnerability, set it for 3, add security, set if for 5. You can add as many condition as you like.
Then click on quality gate, click on the name you used for the quality gate, go to the up right side and click on set as default, so that whenenver you do a build, the quality gate will be the one to screen the code if it meet the condition that was given.

The quality gate is used to set the threshold or set the benchmark or standards using conditions to be met by the code before the code can be accepted for further deployment.

let us do a build on our mwa project by running

mvn clean package sonar:sonar     

to clean and to run codequality analysis by sonarqube abd then to do a new build. It successfully build with the quality profile and the quality gate that we have created by setting both on default

SECURITY OF SANARQUGE SERVER WEB PAGE

How are we able to secure our sonarqube environment? To secure our sonarqube server, we do the following.

1. To secure your sonarqube server, you need to create a new account because remember that your login credentials of snarqube that you inserted on the pom.xml was admin as username and admin as password. This can easily be guessed by hackers.

Under security, so can create users, group, global permissions, permission template, on your sonarqube web page

a. You can create users by clicking on the administartion, 

b. click on project, click on management. In the project, managment, you can delete the projects you have before because those projects right there are not in a secured sonarqube environment, we need to delete the project and run another package build on a builder technonlogy like maven integrated with sonarqube that will be on the new secured sonarqube account.

c. Once you have deleteed. Go back to administration

d. click on security. Uder security, we have users, group, global permissions, permission template.

e. Let us first of all create user by clicking on users. In users, you will see a user called administrator. That was the user we used in login in. Let us create another user

f. click on create, to create a new user, and put in login name, user name, password, email address is not very important, you can quip if you like  and the click on create 

g. By default, you will notice that what you just created is on the sonar-user group. Any new user you create would automatically join sonar-user group and in this group, there no admin access.

h. to grant you the new user an admin access, by clicking on administration, click on security, click on sonar-user, click on select and then tick sonar-administrators and click done. By doing this, you have grant your new user admin privileges or access. 

In work scenero, you can choose the engineers you want to grant admin access to. You can decided not to grant admin access to any other worker. All you need to do is just to create user account for them without granting admin access.

2. Group: We are now in sonar-user and sonar-adminstrators group

You can create groups by clicking on security, click on group, click on create group e.g like   tesla.admin, name the group and click on create.

Note

i. create tokens. For best practises, you need to create tokens which is considered to be more secured than the use of password and username. Tokens are more secured because it is encrypted.

6590875220df07ae99b7fca3f5885f6075617c53

above is the token i generated.

You can created tokens by clicking on the token line that relates to your user, name your token and click generate.

j. Go to the property tag in your project pom.xml file and edit or delete the sonar login name and replace it with your tokens you generated. 

Also delete the whole line where your sonar password is. or better still, put a comment where the password is. That is <!--         -->   i will put the password line inbetween. so that sonarqube scanner will not read that password line. and then save and quit

DIFFERENCE BETWEEN SONARQUBE AND SONAR-CLOUD

1. With SONARQUBE you are required to create a redhat ec2 server in aws and install sonarqube software to it via an ssh-clients to become sonarqube server in linux environment.

This is called IAAS.   IAAS = Infrastructure As A Service. 

This means that you need to create the infrastructure (by creating a server) and then install the software (on the server created)

2. With SONAR-CLOUD, you dont need to create any server. It is already an existen codeQuality platform where You can only create an account or subscribe in order to carry out codeQuality analysis. This is called SAAS

SAAS = Software As A Service

NEXUS

Nexus is a backup technology called artifactory that is used to backup or store build artifacts and can be retrieved when need.

For example, when something goes wrong with the build server, Nexus would be a place where those artifacts that was created by the build server can be pulled from because a backup of all the artifacts where also uploaded and stored in Nexus.

Example of artifacts that was created by the build server like maven and uploaded to Nexus are

war/jar/ear and some others are nodePackages like nodeJS

Nexus is one of the Artifactories that are used to store artifacts. Some company uses Jfrog as their artifactory.

Because of the fact that we can be able to pull the artifacts that was built by maven, it is safe to say that

- In our environment, we maintain a fault tolerant Architecture or Infrastructure
-  We deploy a highly available Architecture or Infrastructure

- We uploads artiifacts from the maven into artifactory like Nexus/Jfrog/AwsCodeArtifacts

Eaxamples of Artifacts uploaded from maven to Nexus are

jar/war/ear 

Coding:  

In our coding environment, we use git for versioning

We use GITHUB/GITLAB/BITBUCKET as our source code manager tool (SCM tool)

IN A COMPLETE LIFECYCLE OF MAVEN

Lifecycle: There are 3 life cycle of maven. These are 

    1.  clean Lifecycle: This will delete or remove old build by deleting the target directory that is containing the build artifacts.

       mvn clean  --- delete old artifacts
    2.  site/swagger Lifecycle
       site --- Create java classes 

    3.  Default Lifecycle: These is the most important and completed life cycle that has 7 goals. These goals are 

       a. validation goal: This will validate the java based code to make sure that there are no syntax error

        mvn validate 

       b. Compilation goal: This will validate and compile the code by converting it to java-classes. plugins for this is, OpenJDK

       mvn compile

       c. Unit Test Goal: This will validate, compile and run a unit testing of the code. The dependencies for this is, Junit

       mvn test 

       d. Package goal:  This will vailidate, compile, run a unit testing and build/create artifacts like  .war or .jar or .ear packages in the target directory of the application repository depending on the type of application that was built by developers.
       (target/*war, app.war. app.jar, app.ear) by running

       mvn package

       e. Install Goal: This will valaidate, compile, runa a unit testing, create artifacts in the target directory, and store the artifacts in MLR (maven local repository) by running

       mvn install 

            (target/*war, app.war. app.jar, app.ear)
            (MHD/*war, app.war. app.jar, app.ear)

        Sonarqube goal: This will validate te code, compile the code to java-classes or bytesCodes, run a unit testing, create artifacts in the target directory, store the artifacts in MLR (maven local repository) and then do a codeQuality analyses by running

        mvn sonar:sonar: 

            sonar = plugin
            sonar = goal

        deploy goal: This will validate te code, compile the code to java-classes or bytesCodes, run a unit testing, create artifacts in the target directory, store the artifacts in MLR (maven local repository), do a codeQuality analyses and uploads the artifacts to an artifactory like Nexus or Jfrog

        mvn deploy = Uploads artifacts to Nexus 

   NOTE:

   As a developer, your computer or laptop is your local environment

Artifactory/Repository:  Artifactories can also be called repositories

artifacts depends on the type of project/application and language used to develop the application:

  -java based application: java is a programming language. The application developed in java will create war.jar.ear artifacts. Example   --> app.jar, maven-web-app.war, boa.ear 

  -docker -->   Docker Image 
               docker build
               docker run 

  NodeJs  -- > will create NPM Packages (non-package manager packages)

  yum --- > yum creates yum Packages.  For example

            yum install maven git httpd tree

            yum is a package manager for Redhat and CentOS 

          yum is an artifactory/repository in the internet were artifacts like tree are stored.
  
       yum install maven

       If yum run yum install, it will go to yum respository in the internet and search for the version of maven software that has been stored there and then pull and install in your server

       yum is a package manager for redhat server and cent-Os
      

  apt  -->  will create apt packages . For example

  apt install tree 

  If you run apt install tree, it will go to apt respository in the internet and search for the version of tree software that has been stored there and then pull and install in your server

            apt is a package manager for Ubuntu/Debian


If you want to install software in your windows computer or computer with window OS, you can use chocolatey

For example:

choco install terraform 

chocolatey is a package manager for windows OS

 brew:   --> brew is a package manager for mac OS 

  brew install terraform    

  brew is an artifactory/repository in the internet that stored terraform
          
  helm-->  is a package manager for kubernetes

  pip --> is a is a package manager for python 

IQ
What is an artifactory in Software management?

Ansers: Artifactory is repostory that contains build artifacts.

-----
IQ: How do you use Nexus in your environment?

Answer: Nexus is an open source artifactory, In our environment, we use Nexus to backup or store built artifacts from the build server and this stored artifacts can be retrieved when needed

For example, when when maven server is corruped or down, we can recover the build nartifacts from Nexus if we need them

Nexus is an Open source Artifactory Repository
       It is used to store and retrieve build artifacts
       We can retrieve artifacts when needed
       Nexus acts as a Disaster recovery server for maven  

Other types of artifactory are

JFrog: Is an  Artifactory Repository - Licence. it is not free
       It is used to store and retrieve build artifacts 

Amazon CodeArtifacts:
     It is used to store and retrieve build artifacts 

IQ

what is the difference between target and /target dir?? :  target dir and /target dir

Answer: 

target directory is a normal directory=  

mkdir target       this will create directrory in the present working directory (PWD)

if you want to loocate such directory, you run

ls         it will show you all the directory/files that is in YOUR pwd   
 
/target directory is a target directory that is inside root directory that requires admin access or sudo access. If you want to create such directory, you run   

sudo mkdir /target   this will create target directrory inside the root directory


        if you want to locate such directory, you run

ls /         it will show you all the directoryfiles that is in root directory


IQ: How do your developers connect with your SCM repos in GitHub?
Answer: Developers connect to our github remoterepo 

 via authentication: by ssh or PAT (personal access token)

 git push alName branchName: This is used by the developers to push the wriiten code to our github RemoteRepo 

IQ: How DOES MAVEN connect with SonarQube?

We connect maven with Sonarqube by going to the properties tag <properties>  <properties/> in pom.xml of maven application and modify the sonarqube Ipaddress and login credentials.

    MODIFY THE   <properties> <properties/> in pom.xml  
    by adding sonarqube server IPaddress and login credentials 


NEXUS INSTALLATION.

You can get the direction of how to install maven from prof. github account. In his package management repo readMe file. This is below:

Pre-requisite
AWS Acccount.
Create Redhat EC2 t2.medium Instance with 4GB RAM.
Create Security Group and open Required ports.
8081 ..etc

Attach Security Group to EC2 Instance.

Note: It better to open all port 	when configuring or creating the security group in aws by selecting ALL PORTS. so that when you eventually want to change the nexus default port to another new port for security reasons, it will be successful.

Create nexus user to manage the Nexus server

#As a good security practice, Nexus is not advised to run nexus service as a root user, 
# so create a new user called nexus and grant sudo access to manage nexus services as follows. 

sudo hostnamectl set-hostname nex
sudo useradd nexus

# Grand sudo access to nexus user
sudo echo "nexus ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/nexus

sudo su - nexus

#Install Java as a pre-requisit for nexus and other softwares
cd /opt
sudo yum install wget git nano unzip -y
sudo yum install java-11-openjdk-devel java-1.8.0-openjdk-devel -y
#Download nexus software and extract it (unzip).

sudo wget https://download.sonatype.com/nexus/3/nexus-3.54.1-01-unix.tar.gz 
sudo tar -zxvf nexus-3.54.1-01-unix.tar.gz
sudo mv /opt/nexus-3.54.1-01 /opt/nexus
sudo rm -rf nexus-3.54.1-01-unix.tar.gz

#Grant permissions for nexus user from root user to nexus user in order to start and manage nexus service. That is 
# Change the owner and group permissions to /opt/nexus and /opt/sonatype-work directories.
sudo chown -R nexus:nexus /opt/nexus
sudo chown -R nexus:nexus /opt/sonatype-work
sudo chmod -R 775 /opt/nexus
sudo chmod -R 775 /opt/sonatype-work

#Open /opt/nexus/bin/nexus.rc file and uncomment run_as_user parameter and set as nexus user.
# change from #run_as_user="" to [ run_as_user="nexus" ]
echo  'run_as_user="nexus"' > /opt/nexus/bin/nexus.rc
# > Above is a sign of append. This means to add  'run_as_user="nexus" ' to the content in the #file nexus.rc
# or  echo  'run_as_user="nexus" ' > /opt/nexus/bin/nexus.rc
#>>Above is a sign of redirect. This means to replace totally the content in the file nexus.rc #with 'run_as_user="nexus" '

#CONFIGURE NEXUS TO RUN AS A SERVICE
#You need to run nexus as a service by creating a link. Make sure you are in /home/nexus

sudo ln -s /opt/nexus/bin/nexus /etc/init.d/nexus

#9 Enable and start the nexus services

sudo systemctl enable nexus
sudo systemctl start nexus
sudo systemctl status nexus
echo "end of nexus installation"

You can investigate if nexus is running by running

ps -ef | grep nexus   

curl -v localhost:8081   or curl -v publicIpadress:8081

If you want to access nexus in the internet browser, paste the nexus publicIpadress:8081 on the browser. That is 
http://3.133.95.235:8081/


Default login credentials for nexus is

  userName = admin 
  password=admin123

  You try to login in with the default credentials but will not log in. It will ask you to go to your server and copy an encrypted password. and then paste it on where the password is. That is, you can 

cat /opt/sonatype-work/nexus3/admin.password

You can copy and paste the encrypted password on the password box

Then it will ask you to change password. You can now change your password to anything you want. I changed mine to  admin123

(NHD) Nexus Home Directories

home Directories for Nexus are 2. These are

/opt/nexus
/opt/sonatype-work

for /opt/nexus

It has the folowwing components

    bin  = contains binary files like commands  

    lib  = contains libaries files

    etc  = contains configuration files. The most important files in etc directory is the nexus-default.properties file. This file is used to configure or change the default value of nexus in away that.

NEXUS CONFIGURATION

1. you can actually change the port number to another port number for securities resons. You can do this by vi into the nexus-default.properties file and change the port number.

2. You also modify the nexus-context-path=/  by adding username after the forward slash (/). That is 

    cat nexus/etc/nexus-default.properties   
     application-port=8081
     application-host=0.0.0.0     this means that it can be accessed from anywhere
     nexus-context-path=/

For the above illustration. We want to configure the nexus-default.properties file in etc directory to become as it is below 


     application-port=8291
     application-host=0.0.0.0
     nexus-context-path=/devops

After you have reconfigure, you need to restart nexus by running the command

sudo systemctl restart nexus

Henceforth, If you want to access nexus, you need to use the recent port number since it has been changed to port 8291. that is  use http://3.133.95.235:1980/ to access nexus on web browser

http://3.133.95.235:1980/ 

access with context-path: You can final access nexus web page by using

http://3.133.95.235:1980/devops

TO MANAGE A FINTECH CLIENT: Steps you apply are

 1. clone the code to your rehat ec2-user home directroy of your maven server
 2. mvn clean package = to remove old build and build new artifacts 
 3. run codeQuality with sonarqube
 4. upload artifacts into Nexus by first of all
 Created a nexus server and configure the server to a more safer one and then, Intergrating Maven with nexus 

fintech client  
Maven SonarQube Integration:  mvn sonar:sonar
   54.236.232.85
   172.31.0.41
   PORT = 9000
 1. vi pom.xml and check for "properties" tag
mvn package  
Maven Nexus Integration:

CLASS QUESTIONS

Where is the database found in our sonarqube.

Answer: The database is in our sonarqube server found when you click on administrator, click on system, you will see the folLowing sonarqube components;

SYSTEM: THIS GIVE THE DATABASE INFO when you click on it
WEB: this is the web server that enables sonarqube to be able to be accessed on internet brwoser
COMPUTE engine: This Compute engine that does the calculations in percentage or numbers for quality gate

When you click on system, you would see the type of database your sonarqube is using by default, it is called H2 database.

Note: The information that was used when installing our database are stored in the sonarqube database. Your information that you used in registering sonarqube like you password, username, token are all captured in the database. If you are entering the wrong credentials on your sonarqube server, it would not work because it is not in the sonarqube database.

In some sonarqube, tyou can also be required to create your own database in the sonarqube server database  PostgreSql   
so that all your report is going to be stored on the new database        
Databases 
  H2DB by default 
  PostgreSql  can be configured to your own database 

CLASS QUESTION
iN WHAT CIRCUMSTANCES WOULD YOU ADVISE YOUR COMPANY IF YOU WANT TO CHOOSE BETWEEN SONARQUBE ABD SONARCLOUD?

ANSWER;

Recently, there is a total migration from sonarqube to sonarcloud. Companies are now migrating to using a serverless infrastructure.  So if you have a situation whereby your sonarqube server is always failing with so many errors, you can advise them to use sonarcloud so that we will not be worried about using a server but instead we can now be managing codeQuality analysis on a serverless platform to avoid failure 

VIDEO 66

NEXUS 2

NEXUS WEB CONFIGURATION AND INEGRATION WITH MAVEN

Once you have accessed nexus on your internet browser via the public ipaddress with a new port number modified context path. That is 

http://NexuspublicIpaddress:8291/devops

3.16.43.120:8291/devops

Then click on sign in and then login with the default user and and password

default username: admin
default password:  admin123

LET US CREATE REPOSITORY WHERE BUILD ARTIFACTS WILL BE STORED

1. click on administrator and configurator. It is like a settings simbol on the upper side of your page

2. click on repositories
3. click on create repo. Under recipe. you will other package managers which you can create repo that can store their artifacts
4. click or select maven2 hosted, since it is maven build artifacts we want to host or store here
5. enter the name you want to know your repository. the name of your prject is tesla. so you can name it tesla-snapshot     snapshots is the version  and the click create

Note; You are going to create 2 repositories with same project name but different versioning. That is, repo. for snapshot  and repo. for releases

6. copy the url of the tesla-snapshot repo you just created by clicking on copy and then copy and save it. which is below

http://3.16.43.120:8291/devops/repository/tesla-snapshot/

7. create another repository and name it tesla-releases by clicking on create repo and select maven2 hosted. enter the name you want to know your repository. the name of your project is tesla and this time, the version will be releases
8. copy the url of the tesla-release repo you just created by clicking on copy and then copy and save it. which is below

http://3.16.43.120:8291/devops/repository/tesla-release/

IQ: 


what is the difference between snapshots-repository, releases-repository?
MIXED-repository

Answer: 

Snapshots Repositories/artifactories are repo that are basically created and shared with the developments team in the development branch to deploy build artifacts into it.

Build from the development branch are uploaded to the snapshots  repos in nexus.

Releases Repositories/artifactories are repo that are basically created and shared with the deployment team in the master branch for the purpose of further deployment to the application server or somewhere it is needed if the maven server failed
 
Builds from the master branch also known as release branch are uploaded to the releases repos in nexus  

Build from the development branch are uploaded to the snapshots  repos in nexus
 Builds from the relesae/master branch are uploaded to the releases repos in nexus

 Snapshot repository receives and stores build from the master branch

Release Repository receives and store builds from the development branch 

A TICKET with ticket number : Tk001452 has been raised

Ticket Task: uploads Artifacts into NEXUS artifactories/Repositories

Follow the below to execute such a task:

Executing tasks:
  1. Create and and configure a nexus server where applicable  
  2. Request for nexus server admin credentials = userName/password
  3. Create artifactories / Repositories both snapshots and releases
  4. Share releases Repositories with the deployment team
        http://52.53.227.31:8191/landmark/repository/tesla-releases/  
  5. share snapshots Repositories with the developments  team  
       http://52.53.227.31:8191/landmark/repository/tesla-snapshots/ 

Integration of nexus and maven

  6. Configure maven-nexus integration for seamless artifact uploads 

        a = modify the <distributionManagement> tag in pom.xml  by

        vi pom.xml

 This can be achieved by going tpo your maven server  by vi into the pom.xml file of your maven application and modify the distributionManagement tag for repositories. that is <distributionManagement> <repository> by changing  

 napshot repository path to

 http://3.17.160.10:8291/devops/repository/tesla-snapshot/

 and releases repository path to

 http://3.16.43.120:8291/devops/repository/tesla-release/


        b = modify settings.xml    which can be found in your conf directory 

        vi into settings.xml on your maven server and go to server tag and create you server tag, open and close

        this

sudo vi /opt/maven/conf/settings.xml and modify the "server" tag by creating another server tag, open and close by scrolling down to the last server paste the below server tag you created

<server>
      <id>nexus</id>
      <username>admin</username>
      <password>admin123</password>
 </server>




inbetween     -->     and      </servers>


 That is, you have put in the default login credentials of nexus.

              userName  --- admin
              Password   admin123

              That is 
 
    <server>
      <id>nexus</id>
      <username>admin</username>
      <password>admin123</password>
    </server>


Let us do build and deploy to nexus repositories that we have created and configured in maven by running

mvn deploy

To verify this,

If you go to your nexus web page, click on browse, click on your snapshot repo you created. that is tesla-snapshot, tesla snapshot repo., you will see that what was deploy in maven is right there. It is inside one-numbering folda that is inside snapshot folda.

mvn deploy is sused to upload artifacts into artifactories like nexus for storage which can be retrieved when needed.

If you re-run 

mvn deploy

You will notice that another artifacts has been uploaded to nexus having another tag number. Every uploads has their different tag number.

Note: If you are trying to deploy artifacts to the application server like tomcat, you can usually download the artifacts from the build server like maven or from the artifactory server like nexus and deploy the artifacts to the application server.

If you take a look, you will notice that all our deployment went to snapshot repo in nexus.


DEPLOYMENT TO THE RELEASE REPOSITORY IN NEXUS

If you are ready to go to the market or ready to deploy the artifacts to application server, you can deploy the artifacts to the release repo. in nexus, which means that the artifacts is ready to be released out.

For you to do the above, simply vi into your pom.xml file, you will see version tag, 

if it is snapshot that is there, delete only the snapshot. just the word "SNAPSHOT" 
and then save-quit

If you run another mvn deploy command. that is

mvn deploy

If you go and check your snapshot repo, you will notice that there was no deployment to snapshot but when you browse your release repo., you will see that deployemnt came to the release repo.

iF YOUY WANT TO RE-DEPLOY ARTIFACTS TO NEXUS. it means that you want to update the artifacts that you already have in your release repository. If you to do this, 

you need to go back to your administrator tag, go to repository and go to your project release hosted ( that is. tesla-release ) and then on deployment policy, enable redeploy by selecting "allow redeploy"   and click save.

Once you do the above, if you re-run mvn deploy, you will notice that the artifacts that was there has been updated or replaced to the new deployment that just happened.

Note: This redeployment and update of the artifacts in release was also made successfully because the release version number in the pom.xml is still the same, you did not change the version number.

If youy want to deploy the artifacts to release repo. and you want to the artifacts version number to be 0.0.2  , all you need to do is to go to version tag in your pom.xml in your maven application and replace ther version number there with 0.0.2 and then save-wuit.

If you run mvn deploy, if you to your nexus release repo., and refresh the page, you will see that this version number of artifacts will be there. it will not replace the old version that was there before. If you want it replace the old version that was there, use the same version in pom.xml and re-ren mbn deploy.

You can continue to change ther version number if you like. if you want to version number to 0.0.3, just change the version number in pom.xml to 0.0.3

FOR A SECURED ENVIRONMEMT

Maven need dependencies and plugins to enhance its funtionality. Maven need dependencies and plugins to function effectively

Maven gets its dependencies from 4 stages. During build processes or in all maven build and deployemnts. That is, Maven default lifecycle

mvn validate
mvn compile
mvn test
mvn package
mvn install
mvn somar:sonar
mvn deploy

maven will need dependencies and plugins to do the above task. Maven would search for these depemdencies and plugins from the following

1. MLR= Maven would search fordepemdencies and plugins in the maven localRepo which is found in the maven server. That is  .m2/repository
2. MRR = If maven did not find depemdencies and plugins in the MLR. it will search MRR like Nexus and other artifactories

NOTE: ARTIFACTORIES are used to do the following

a. can be used to store artifacts
b. can be used to store dependencies and plugins

3. MPR= wgen maven did not get depemdencies and plugins in MRR, it will search depemdencies and plugins in the maven proxy repo.

MPR= can be used to store dependencies and plugins

4. MCR+ when maven did not get depemdencies and plugins from MPR, it will search the internet to get the depemdencies and plugins but for enhance security, maven always get the depemdencies and plugins from MCR via MPR because MPR will stand as a proxy repo for maven to avoid hackers to be able to hack into your maven server. They can be able to see only the proxy ipaddress and not that of maven server. It is the MPR that will pull the dependecies/plugins from MCR and then maven server will now get dependecies/plugins from MPR

It is not advisable to set your environment in such a way that maven would get dependecies/plugins directly from the internet. That will make your build server vulnerable to hackers. For best practises and to ensure maximum protection, maven should get dependecies/plugins from either MLR or MRR or MPR which are in the same VPC (which would require them to communicate or integrate to each other using private IP or local IPaddress)

CREATION AND CONFIGURATION OF PROXY REPOSITORY AND REMOTE REPOSITORY IN NEXUS AND MAVEN

For maximum security, It is better for maven server to download dependencies/plugins from the MRR or MPR in Nexus to achieve its build goal if it can not found dependecies/plugins in MLR.

and also, to avoid maven of downloading from central repo. but get it via MPR.

MRR and MPR are created in NEXUS web page and then integrated or configured in maven server.

Let us created MRP AND MRR in Nexus web page

1. acces your nexus web page by your nexus server publicIpadress:portnumber/name
NOTE: Whenever you stop your nexus server, if you start the server, the ipaddress changes and you need to use that ipaddress to login to your nexus web page.

http://3.17.160.10:8291/devops

2. login with the default login credentials. that is, username: admin, password: admin123
3. click on administrator and configurator. It is like a settings simbol on the upper side of your page

4. click on repositories
5. click on create repo. Under recipe. select maven2 proxy
6. enter repo name. like tesla-proxy
7. under version policy, select mixed
8. under remote storage, this means the location of maven central repo can be found

https://repo.maven.apache.org/maven2/           and paste it there

9. click on create. by so doing, you have created your PR (proxy repo)

You can also create Remote Repo.

1. click on repositories
2. click on create repo. Under recipe. select maven2 hosted
3. enter repo name. like tesla-remote
4. under version policy, select mixed
6. click on create. by so doing, you have created your RR (remote repo)

NOTE:  Our intention here is that, we want a situation that maven would get dependencies from proxy repository if it can not get it from local repository or remote repo and not totally from central repository

Since we have created these 2 repostories. Let us integrated these nexus tesla repos into maven server. We will achieve this by configuring 2 files in maven server.

1. let us go to pom.xml in maven application and create the nexus repository tag  inbetween properties tag and dependencies tag, if there are no such repo in it before now by creating repository tag inbetween properties tag and dependencies tag and create the below 2 repository

<repository> 
<repository>
       <id>nexus</id>
       <name>nexus proxy Repo</name>
      <url>http://18.118.78.160:8291/devops/repository/tesla-proxy/</url>
    </repository>
    <repository>
       <id>nexus</id>
       <name>nexus remote Repo</name>
      <url>http://18.118.78.160:8291/devops/repository/tesla-proxy/</url>
    </repository>
    <repository>
       <id>nexus</id>
       <name>nexus group Repo</name>
      <url>http://18.118.78.160:8291/devops/repository/tesla-proxy/</url>
    </repository>

</repository>

The above is an example of how you can created repo in pom.xml. Since we only created 2 repositories, then let us configure the two that we have created. thats is 

<repository> 
<repository>
       <id>nexus</id>
       <name>nexus proxy Repo</name>
      <url>http://18.118.78.160:8291/devops/repository/tesla-proxy/</url>
    </repository>
    <repository>
       <id>nexus</id>
       <name>nexus remote Repo</name>
      <url>http://18.118.78.160:8291/devops/repository/tesla-proxy/</url>
    </repository>
    </repository>

    NOTE: If the repository tag has been created before, all you need to do is to modify the repository if need be.


Let us first of all create the Proxy Repo.

vim into maven application pom.xml, go inbetween properties tag and dependencies tag and paste
<repository>
<repository>
       <id>nexus</id>
       <name>nexus proxy Repo</name>
      <url>http://18.118.78.160:8291/devops/repository/tesla-proxy/</url>
</repository>
</repository>

Note: to get the actual url of your repo in nexus or anywhere. when you are inside the repo. copy the url or link from the browser. the browser will give the exact path to the repo.

<url>http://3.17.160.10:8291/devops/#admin/repository/repositories:tesla-proxy</url>

2. also go to the settings.xml in maven server under conf directory. 

a. copy the settings.xml into another file, maybe to be settings as backup. so that if there is any error during the configuration, you can always get back the default content of the settings.xml

sudo cp settings.xml  settings

b. go to last mirror tag in settings.xml of maven server and change the url to the url of the proxy repo we created and also

c. change the id to nexus

d. place a comment on the  <blocked>true</blocked>  that is 

--> <blocked>true</blocked> <!--
e. create another custom localrepo if you dont have a customise one before. that is 

pasting this  <localRepository>/home/ec2-user/mavenlocalrepo</localRepository>  inbetween   <!--   and --> then save-quit

If you search your proxy repo, you will notice that it is empty. then if you run

mvn clean package

it will download dependencies and plugins from proxy repo in which proxy repo pulled the dependencies and plugins from central repo. Maven did not get it dependencies and plugins directory from the central repo.

To very this, go to search your Proxy Repo in nexus web page and you see dependencies and plugins are now present

In some cases, you are working on a project. That project has dependencies and plugins in both in your MRR na d in MPR. When maven want to build, he would have to download dependencies and plugins in both repositories. 

What you will do in the above case is to create a group repository. like, tesla-group. and then select which the repositories or members that you want to add to the group repositories. just scroll down and you will see available and then click on the members you want. that is, you may decide to add MRR and MPR  and click on create


SECURITY OF YOUR NEXUS WEB PAGE

You have to secure your nexus web page so that even though someone or annanymous user have access to the url of your nexus login, he/she will not be able to have access to the resources of artifacts that you have. To ensure this. do the following

1. go to your administrator
2. click on anonymous
3. dis-allow anonymous user access by unticking or unchecking the allow anonymous user icon

CREATION OF USERS IN NEXUS WEB PAGE

You can also create users in nexus web page by clicking on the user icon

click on create local user, fill in your login credentials. make sure you select active status. it was ask you what type of user you want to create. There are 2 option there. nx-admin and nx-anynymous. nx means nexus. if it will be a user that would have admin access , then select nx-admin

IN SUMMARY

Nexus can be used to do the following

1. store artifacts
2. can be used to store dependencies and plugins

NOTE: You can download the artifacts in your nexus web page by going to your nexus tesla-release and click on any of ther sion of artifacts your created. make sure you are in the the place it says .war and  right click the link and the click on copy link  and then paste it on your browser and press enter

http://3.17.160.10:8291/devops/repository/tesla-release/com/mt/maven-web-application/0.0.5/maven-web-application-0.0.5.war

You will see that it will download

IN THE INTERVIEW
You can say that

1. We use Nexus as our remote repository for our maven build server. whereby artifacts build in maven can be stored in nexus as backup and can also be retrieved or downloaded  or pull from nexus when the need arise
2. Nexus can also be used as proxy repository for maven during maven build in a way that maven are configured to downloads dependencies/plugins from nexus proxy repository instead of maven downloading directly from the internet to avoid our environment of being exposed to hackers.

NOTE: if you want to view all files/directories both hidden and non-hidden, you run the command

ls -a      this command will list both hidden and non-hidden files

NOTE: When you run the command mvn sonar:sonar, it will validate, compile, run unit testing, build package and then run a codequality analysis.

VIDEO 68

JENKINS 1

When developers are done writing code, they will commit/push the code to a source code management tool like github.

Developers manage github with what is called branching strategy

A branch is a line of development. we have development branch, stage branch and master branch. These are the minimum branching strategiues that we maintain in landmark

Developers would push source code, unitestcases and buildscript (this is called pom.xml file. the file is written in xml) to github using git as a versioning tool

The code is now cloned by a build tool like maven (maven support java based project or application) to create packages. 

If maven is being used as the build tool, maven will run testing and create packages like jar/war/ear packages in the target directory of your project directory/repo that you cloned.

After the code as been tested and packages/artifacts created by maven, sonarqube would run a codeQuality analysis and testing to see if the application is a quality application. That is, if it meet the standard that was set for such application interms of the number of code smell, vunerability, duplicated lines, bugs in the code etc. using the a tool called sonarqube has a plugin called sonar and the a goal called sonar. that is, (sonar:sonar)

If the application passed the codeQuality analysis, we need to backup or save a copy of our artifacts that was created on  an artifactory like Nexus or Jfrog. Since we are working on a java based application, we can backup the artifacts in nexus by creating 2 repositories that will backup our artifactories (snapshot artifactory and release artifactory). and this artifacts can be retrieved anytime it is needed.

When the backup has be carried out by the artifactory technology called nexus, then we can deploy the application to the application server or deployment server like tomcat.

Introduction of Jenkins.

There is a tool that can integrate all the steps together starting from github --> maven-->sonarqube-->nexus-->tomcat to be run in just one command. This is called an automation tool. Example of the name of this tool is called JENKINS.

JENKINS  is an automation tool that is used for CI/CD. That is, it is used for continuous integration and continuous deployment.

HOW DOES JENKINS WORKS

1. Jenkins will clone the projectremoterepo containing the code written by developers from github using git clone command. This means that git would be installed in jenkins. CD= CONTINUOUS DEVELOPEMENT

2. Jenkins will connect with maven to do unit-testing and create packages/artifacts. This is called jenkins-maven integration. CI= CONTINUOUS INTEGRATION

3. Jenkins will connect with sonarqube to do codeQuality analysis. This can be achieved by integrating sonarqube with jenkins- Jenkins-sonarqube integration. CI= CONTINUOUS INTEGRATION

4. If the software or application that has been created is of quality or meet the standard that was set, Jenkins will connect with an artifactory like nexus to upload artifacts to nexus to backup the artifacts. This is called jenkins-nexus integration. CI= CONTINUOUS INTEGRATION

BUILD AND RELEASE

The process whereby Jenkins pull codes from github, run unit-tesing and create packages, run codeQuality analysis and then upload artifacts to nexus is called build and release process.

If you know how to run this process very well as an tech person, you can work in a company as a BUILD AND RELEASE ENGINEER. THIS IS ALSO CALLED CONTINUOUS INTEGRATION

BULD AND RELEASE ENGINEERING DOES NOT INCLUDE DEPLOYMENT

5. Jenkins would do a deployment to the application server. Before jenkins will deploy to app. server, further testing would be carried out. 

a. If the testing done requires manual approval. This process is called continuous delivery. This type of testing usually take place in the staging environment. This is where the software will be tested by the uat/qa team. customers will be testing to see if what was bargained for was what was developed or built. If the testing is passed and software is manually approved, then the software can be deployed to the app. server. This type of process is called CONTINUOUS DELIVERY. CD

b. If there are no manual approval of the software required in the testing environment, the approval process is automated. tHIS IS AUTO-APPROVAL. The process ius called CONTINUOUS DEPLOYMENT. Jenkins would connect with app. server/deployment server like tomcat for the software to be deployed to the app. server. via jenkins-tomcat integration. This is called CONTINUOUS INTEGRATION. CI

DISCUSSING ABOUT JENKINS. 7Cs are involved

1. Continuous development (while developers are continuously developing application, Jenkins is continuously connecting with github and cloned projectrepo from github for every code that is developed)

2. Continuous integration ( Jenkins will integrate all the tools needed for automation. That is, Jenkins-maven integration to run unitTesting and build artifacts, Jenkins-sonarqube integration to run codeQuality analysis, Jenkins-nexus integration to uploads build artifacts to nexus for back-up). This is called BUILD AND RELEASE ENGINEERING

3. Continuous testing (This happens in the testing environment where the uat team would have to test the software. Customers have to test run the application or software to see if it is what was bargained for)

4. Continuous delivery (This happens in the staging environemnt. The BA and QA team will look at the software that was developed and approve if it is what they expected. This is when the software that was developed requires a manual approval.

5. Continuous deployment (Jenkins auto-approve the software and then connect with the app. server like tomcat for deployment of the artifacts to tomcat.)

6. Continuous Monitory: This is where jenkins automate monitoring process. The whole infrastructure or architecture is continuous being monitored by automation process so that if anything is going wrong, the system will auto-raise a ticket that something is going wrong. Jenkins will connect with monitory tool called

7. Continuous Security: Jenkins also automate the security of the software architecture. Jenkins would connect with security tool called to ensure a secured architecture


WHAT IS JENKINS


Jenkins, is an open source Continuous Integration/continuous delivery (ci/cd)
tool written in Java. 

Junkins can be installed in Multiple -Platforms :  This means, it is a multi-platform tool. This means we can install jenkins in the following:

  Linux
    Hosted by AWS 
  Windows
    LOCAL system
  MacOS-
    LOCAL system
  solaris

 Explain you EXPERIENCE in open source Technologies?? 

 Answer: The open source experienece that I have so far are:

Jenkins, maven, tomcat, sonarqube, nexus, Linux, git, github, Nginx 

 Jenkins was formally called HUDSON and was created in 2004  but because of the issue the creator has with Oracle, it was renamed to be called JENKINS in 2011.

CONTINUOUS INTEGRATION
This is the process of automating the testing and build of a code anytime a team member commit a change in a code to a version control tool like git/github.

Jenkins:
  Continuous Integration
  Let say we have been contacted by a fintech/e-commance/Health institute client to develope a web application for them. We can decide to use maven to build such application which is going to be based on java.

JENKINS PROCESSES IN BUILD SUCH APPLICATION

With the help of Jenkins-git/github integration, whenever there is a git commit of code, JENKINS will trigger a build and testing of the code with the help of jenkins-maven integration (it will validates, compiles, run unittesting, and create package), run a CodeQuality analysis with the help of Jenkins-Sonarqube integration, upload build artifacts to nexus for backup with the help of jenkins-nexus integration. All this all processes and stages would be automated using Jenkins. This what is called continuous integration

  Maven-web-app --> java  (thefintech / e-commerce / NIH )
      Code-->Git-->Maven(validates, compiles, tests, package)-->
         
    CODE--COMMITS--Git/github---Jenkins---Maven---sonarqube--nexus  This process is called continuous integration and also called build and release engineering

JENKINS JOB

1. UPSTREAM JOB
2. DOWNSTREAM JOB

If our current version code where version:v44 which has about 30,000 lines of code

If developers has writing another version of code version 45 by adding more 1000 lines of code which now have about 31,000 lines and commit to git/github, 2 Jobs would be carried out by jenkins

1. UPSTREAM JOB: Jenkins will trigger A CI or BUILD AND RELEASE job which we have explained before. This CI automation processes is called an UPSTREAM JOB.

2. DOWNSTREAM JOB: Once the upstream job is completed by Jenkins, it will trigger the downstream job which is involves the deployment processes (CD) to the application server like tomcat. CD= MEANS CONTINUOUS DEPLOYMENT OR CONTINUOUS DELIVERY.

ExPLANATION: Once the developers has developed the version 45 and commit the code to the development branch and a pull request is done and then merge the new version to the master branch after review, jenkins will trigger a CI or build and release job or upstream job first and then trigger the downstream job or CD job by deploying the software to the uat/testing environment and then to our target environment or production environment like tomcat.

Jenkins  Upstrean jobs   --- CI Job [Build & Release]. What happened here is below:
   Clone  
   UnitTesting 
   Build  
   CodeQualityAnalysis
   UploadArtifacts 

 Jenkins Downstream jobs --- CD Job. Jenkins will do 2 steps here
   1. Deploy app. to UAT/Testing Environment where
     . addition Testing is carried out by the uat team (users acceptance testing team) [ the tesing that will take place here are: integration testing, regression testing, functional testing, penetration testing (security testing), performance testing, load testing, ]

   2. Deployment to production or application server: Two things are required or take place before deployment to production. These are manual approval or auto-approval. 

   Approval is required by the BA team from the client will come and check if that was expected from the company to develope and they do the manual approval most times. The software can also be auto-approved. This means the approval can be a manual approval or auto-approval before deploying to production.  

  If the approval requires manual approval before deploying to production, it is called CONTINUOUS DELIVERY but if it is auto-approval before deploying to production, it is called CONTINUOUS DEPLOYMENT.

   Deploy app. to the Production Environment or to the app. server or deployment server.

   CI Tool (Jenkins): ---> 
      GitHub --> Maven --> SonarQube --> Nexus  --> CI 
                   +
                Testing *(automated)
                   |=   Continuous Deployment
                  Development to Production

  CI Tool (Jenkins): ---> 
      GitHub --> Maven --> SonarQube --> Nexus  --> CI 
                   +
                Testing *(manually with manual approval)  (Testing/UAT)
                   |=   Continuous DeLIVERY
                  Development to Production


dev --> runs unit testing in the development env.

  testing / staging env. More additional testing are carried out. These testing are

        performance 
        integration 
        functional
        regression 
        load
        security - penetration 

IQ: Have you managed a Continuous deployment and continuous delivery in your project before?

Answer: Yes, i have used both of them. 

We have manage Continuous deployment in our in-house or internal project whereby additional testing and approval is usually automated. That is, auto-testing and auto-approval is carried out before deploying to production. No manually testing and approval.

We have also manage continuous delivery in an external project contracted to us by a client whereby additional testing amd approval is done manually and approved manually before deploying to production/application server

The Nature of application that we support:
   We support critical applications for a FinTech/e-commerce 

   Jenkins installation:

IQ: 
Which edition of jenkins have you used in Production??

Answer: We have used community edition of Jenkins and Enterprise edition (CloudBees Jenkins)

 Installation of Jenkins:  Community Edition (CE)
 Jenkins CE  
 Jenkins EE - CloudBees Jenkins  

 Note: You can always copy installation package procedures from Prof githug package management repo. The package maybe obsulate or older version. You can get the newer version from the internet if the package is an open source package.

in your server you created for jenkins, run vi Jenkins-installation.sh and copy/paste the below into it and then save/quit
==========
#!/bin/bash
# Author: Prof Legah
# date: 25/08/2020
# Installing Jenkins on RHEL 7/8, CentOS 7/8 or Amazon Linux OS
# You can execute this script as user-data when launching your EC2 VM.
sudo timedatectl set-timezone America/New_York
sudo hostnamectl set-hostname jenkins
# CREATE HOSTNAME
sudo yum install wget -y
sudo yum install vim -y
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo
  sudo rpm --import https://pkg.jenkins.io/redhat/jenkins.io-2023.key  
sudo yum upgrade -y
# Add required dependencies for the jenkins package
sudo yum install java-11-openjdk -y
sudo yum install jenkins -y
sudo systemctl daemon-reload
# start jenkins
# Start Jenkins
# You can enable the Jenkins service to start at boot with the command:
sudo systemctl enable jenkins
#You can start the Jenkins service with the command:=
sudo systemctl start jenkins
# You can check the status of the Jenkins service using the command:
sudo systemctl status jenkins
sudo su - ec2-user
echo "end of jenkins installation"

what have you just done?

1. Firstly, I create a t2.meduim EC2 Instance-redhatServer or VM(virtual machine) in aws cloud.
2. Secondlly i Installed pre-requisit for jenkins (java-11-openjdk)
3. I Installed Jenkins using bash-shell script. This now makes this server to become a Jenkins server.

next step

4. I Started jenkins with the command

     sudo systemctl enable jenkins && sudo systemctl start jenkins

5. I Verified if jenkins is running by running the command

      systemctl status jenkins or ps -ef | grep jankins  or curl localhost:8080 or curl publicIpadress:8080  

6. I will Access jenkins on the browser  
      publicIpaddress:8080

      ACCESSING JENKINS FROM THE BROWSER

      publicIpaddress   it will ask you for initialAdminPassword and tell you the file you can get it from

      Will go to my jenkins server and cat the path to initialAdminPassword   that is

    cat /var/lib/jenkins/secrets/initialAdminPassword 

    sudo cat /var/lib/jenkins/secrets/initialAdminPassword  

 7. Once you run the above command, it will give you a password that looks like a token. 

    5ba28b28e3d9475b91dff37df5bf4c62

    Copy the password and then paste it where it was asking for password in jenkins web page.

8. It will tell you to install default and custom Jenkins Plugins  

   Note: Plugins extend Jenkins with additional features to support many different needs
   and automate tasks.

9. click on suggested plugins

10. It will tell you to create first time admin user.

username: Osas-DevOps
password: admin123

   http://13.52.240.58:8080/


 Installation of CloudBees Jenkins: ENTERPRISE ENTERPRISE -EE  You will need a license to install this

 NOTE: You can check your internet speed with fast.com  it will tell you how many speed in Mbps (megabyte per seconds)

 For best practise, always use :wq + enter key, to save and quit.


 VIDEO 69

 JENKINS-GITHUB INTEGRATION, JENKINS-MAVEN INTEGRATION, JENKINS-SONARQUBE INTEGRATION, JENKINS-NEXUS INTEGRATION AND JENKINS-TOMCAT INTEGRATION

 This is a CI/CD job

 Let us perform a CI/CD job.
 
 This is a Tesla Client: 

  java web applications    

  We have created and shared the project GitHub repo  with developers and  
     Developers commit/push code to the project repo in GitHub using Git branching strategy

  1. Jenkins-GitHub Integration: This is when Github is integrated unto Jenkins for a pull or clone of the project repo in github to be carried out to Jenkins

  Jenkins will have to clone the projectrepo which developer has push the code to from github.

  https://github.com/LandmakTechnology/maven-web-app    prof. private repo in github

  https://github.com/Landmark-Technologies/maven-web-application   prof. public repo in github

  https://github.com/CLIFFOSA22/maven-web-application-1   the prof public repo that i forked

  Note: You will not be able to edit or modify prof repo because you dont have the permission. 
  You can only modify prof repo that you forked to your github account. Take note

  Process: Let us do the GUI configuration of maven integrated into Jenkins.

  a. Login into Jenkins account with you user credentials you have created

  username: Osas-DevOps
  password: admin123

  b. click on NEW ITEM to create a new item and name the item, tesla-app
  c. select FREESTYLE PROJECT and then click ok
  d. describe the project by writing , tesla web application on the DESCRIPTION icon under GENERAL
  e. click on SOURCE CODE MANAGEMENT and then tick git as our versioning tool

  f. Choose a repository by coping and the url of the project repository (private or public repository depending on which you chose) on the repository icon.

  - When it says, failed to connect, error in performing git command.  This means that it could be that git is not installed in your Jenkins server. This repository would need git plugins to function since the repo we have choosen is in github.
  Go to your Jenkins server in linux and then run  git  and it says, command not found, this means git is not installed. install git by running  
  sudo yum install git   and then git --version   to confirm if it is running.
  After you have install git in your Jenkins server and then go back to your jenkins web page and refresh the page, you will notice that the error message will disappear.

  This disappearance of the error is due to the fact that git has been installed and that you are cloning from a public repository in github.

  If you clone from a private repo in github, it will still show error even while you have installed git because it will require authentication since it is from a private repo.

  - most of the time, developers would be committing/pushing code to private repository for security purpose. Let clone that code from a private repository. This type of authication is username and password/token. But Jenkins only support token called PAS= PERSONAL ACCESS TOKEN

  If you clone from public repo. it will not ask for any authentication.

  -  go to your githbu account and generate your personal access token by clicking on settings at the right hand side, scroll down click on developer settings at the left side and click personal access token, click on generate a new token. it will ask for your github password. put in your password. then name your token (like, pat4apps), enter the expiration date that you will like, select the scope of the token ( just tick on 'repo') and click generate token and copy the token anytime you want to use it

  - go to your jenkins web page, under source code management, under credentials, click on add and select jenkins. Enter your github username and PAT.  where ther is ID, put, github credentila, description: github credential and then click add. Once you do this, you have created a credential. Go to where credential is and then select your username. once you do this, you will notice that the error message would disappear.  and then, scroll down and save. This will take you to your tesla project you just created

  -  click on build now. this will clone the projectrepo from github to the workspace directory in Jenkins. You can view the content of the project repo that has been cloned by checking the absolute path to the projectdiretory using CLI in jenkins server. That is 

  ls /var/lib/jenkins/workspace/projectRemoteRepo


 2. Jenkins-Maven Integration: 

 This is when maven is being integrated into Jenkins to run a JUNIT testing and build artifacts.

  -  Leave the previous page by clicking on the dashboard and then click on "manage Jenkins" tools, scroll that to MAVEN, we want to install maven in our Jenkins server using GUI. As for now, maven has not be installed in our Jenkins server

  click add maven and then enter maven name like, maven3.8.6 and the version o maven you want to install (probabbly, the latest version of maven, like 3.8.6. The system will show you that lastest version of maven which usually dsiplay as the first version number that you will see on the version icon) You can even add another maven if you need be. and finally save.

  - Since you now have maven on your jenkins server, click dashboard and then click on your repo you cloned (tesla webapp) and click configure.

  - click on build step, click ADD BUILD STEP, and then click on INVOKE TOP LEVEL MAVEN TARGET, under GOAL, write (package or install or clean package. let us use clean package), highlight maven3.8.6 and then click save

  If you want to see the content of the repo that you have you have cloned, click on workspace. You can also access your target directory that house your build artifacts in your workspace.

  - click on BUILD NOW. It will start to build. You can track the building process by clicking on the # sign down below where a sign of download is and it will show you the page where building is taken place. It is called CONSOLE OUTPUT. You will see that artifacts are being created.  

  After every Build, the artifacts or anyother that was built is always stored on your BUILD HISTORY.

  If you click on workspace, you will notice that the target directory is now present. If you click on the target directory, you will see the artifacts that was created (myapp.war, this means it was a maven web application.

  3. JENKINS-SONARQUBE INTEGRATION.

  The command for this is  mvn sonar:sonar

NOTE: For the above integration to be possible, your SonarQube server must be running. Try and investigate if sonarqube is running before you begin with the configuration by running the command. 

The required port for sonarqube must be open so that traffic can be allowed from jenkins server.

sh /opt/sonarqube/bin/linux-x86-64/sonar.sh status      if it is not running, start it by
sh /opt/sonarqube/bin/linux-x86-64/sonar.sh start

- Login to your sonarqube server web page, click on administrator, click on project, click on management. Let us delete the project will have there before by ticking  and then click delete and then let us run codeQuality analysis on new project.

- Since you have already integrate your github with Jenkins, Go to your github project repo and click on "edit this file" go to your 'Properties' tag with SonarQube server details in  pom.xml of your maven application in github and modify your server details url to your new sonarqube url 
because anytime you stop sonarquage and start sonarqube, the url changes because the ipaddress would change. 

and also the default sonarqube username and password to sonarqube default username and password which is

username: admin
password: admin

If you have changed the login credentials to a token, then input your token details.


- commit the change by clicking commit change

- Go to your jenkins web page and re-run or rebuild the project you cloned from github since we have configured sonarqube on the pom.xml in github by clicking on dashboard, click configure, click on build step, click on ADD BUILD STEP, we are going to add another INVOKE TOP LEVEL MAVEN TARGETS, put (sonar:sonar) on the GOAL icon. Because we want to run codequality analysis on the code or maven web app that we cloned.  and the save.

- click on build now. When you do this, it will start to build. That is, it will be running codeQuality analysis on the project

4.  JENKINS - NEXUS INTEGRATION

  This is a process of integratinbg Nexus into Jenkins in order to uploads artifacts to Nexus for backup after every build. (mvn deploy).

Processes

- Create repos or artifactories in nexus-UI (nexus user interface) to upload aritifacts by going to your nexus web page and enter your login credentials. 

18.118.78.160/

Though you already have a tesla repo. We can create another repo by creating both SNAPSHOTS AND RELEASE repositories like, tesla-fe-snapshots, tesla-fe-releases. and then copy and save the 2 url of both. that is, click on administrator, click on repository, click on create repository, click on maven hosted, type in your repo name and highlight snapshots as version policy (tesla-fe-snapshots) and also create anoother repo and name as (tesla-fe-releases) and click on maven hosted and highlight releases as version policy and click create. copy the 2 repos url'

- Go to your project repo in github, click on pom.xml file, "click on edit this file" and modify 'distributionManagement' tag  and under repository tag modify the release url to the new release url repo we created  and then go to the snapshot url and modify it to the one we created and finally click on commit changes.

make sure you always commit the changes.

 It will look like the following below

            <distributionManagement>

  <distributionManagement>
    
      <repository>
        <id>nexus</id>
        <name>Landmark Technologies Releases Nexus Repository</name>
        <url>http://52.53.227.31:8191/landmark/repository/tesla-fe-releases/</url> 
      </repository>
      
      <snapshotRepository>
        <id>nexus</id>
        <name>Landmark Technologies Snapshot Nexus Repository </name>
        <url>http://52.53.227.31:8191/landmark/repository/tesla-fe-snapshots/</url>
      </snapshotRepository>
  </distributionManagement>

  and then, commit the changes.

- Go to your Jenkins server and look for maven settings.xml inside jenkins server and  modify or modify the last server tag containing nexus server details or if there no nexus server tag created before. This is a new jenkins-maven server and so, it has not been modified. 

You can add one with nexus default login details by adding

<server>
         <id>nexus</id>
         <username>admin</username>
         <password>admin123</password>
 </server> 

 inbetween    --->   and  </server>

 in settings.xml of maven-jenkins server by via its absolute path and modify. That is

 maven absolute path is 

 /maven/conf/setting.xml 

 maven was installed in jenkins server 

 jenkins home directory  by default is /var/lib/jenkins 

 if you run

ls /var/lib/jenkins

it will show you contents on the jenkins hone directory

cd into tools because that was where we installed maven. We are looking setting.xml in maven-jenkins server. That is

cd /var/lib/jenkins

ls

cd tools

ls

cd hudson.tasks.Maven_MavenInstallation

ls

cd maven3.9.2           this is the maven will installed inside jenkins via jenkins ui 

ls

cd conf

ls

sudo vim settings.xml  or sudo vi settings .xml

or you can just do the long absolute path to access settings.xml. that is

sudo vi  /var/lib/jenkins/tools/hudson.tasks.Maven_MavenInstallation/maven3.8.6/conf/settings.xml

 and add another server you created that has nexus as id and the nexus login credentials as admin, admin123. That is

            <server>
                <id>nexus</id>
                <username>admin</username>
                <password>admin123</password>
            </server> 

inbetween    --->   and  </server>

and the save and quit.

- Open required ports like, 8081 in Nexus server ip to allow traffic from jenkins server. I remembered that i did changed my port to another port number to secure my nexus more. Please found out my new port number in my previous note.

- Go to your nexus web page, Since we have gotten to this stage. Let us reconfigure our new release repo in nexus by clicking on repository, click on the release repo name you created, go to deployment policy and the highlight ALLOW REDEPLOY.

- Go to your jenkins web page dashboard, click on build step, click ADD BUILD STEP, and then click on INVOKE TOP LEVEL MAVEN TARGET, under GOAL, write (deploy), highlight maven3.8.6 and then click save


and click on BUILD NOW. It will create artifacts and then upload the artifacts to the releases repository that you created.

- Go to your nexus web page and click on browse, click on the new release that you created and you will found the new artifacts there.

JENKINS-TOMCAT INTEGRATION

- INSTALL 'Deploy to container' Plugin on Jenkins UI (user interface)

 This plugin allows you to deploy a war to a container after a successful build. 

  Go to Jenkins dashboard, click on manage Jenkins, click on manage plugin, click on available plugin, type in deploy to container on the search and click or tick deploy to container, click install without restart

- ADD A TOMCAT USER into tomcat-user.xml file in your tomcat server

go to your tomcat server and check if your tomact is running. If it is not running, them start tomcat by running

sudo starttomcat

absolute path is  /tomcat9/conf/tomcat-users.xml   

I think it was not tomcat that was used as the tomcat server that i named it to. Please check. i think i used "tomcat" as the name of my server in linux environment

cd /opt   and ls
cd tomcat  and ls
cd conf   and ls
sudo vi tomcat-users.xml

or
   sudo vi /tomcat2/conf/tomcat-users.xml   

   and add the following configuration below by coping

   <user username="osas" password="admin123" roles="manager-gui,admin-gui,manager-script"/>



In the tomcat-users.xml file, scroll down to th end and then paste the above configuration you copied inbetween 
 ----> and  <tomcat-users>     and the, save and quit

If you have done the cobfiguration in tomcat before, thats fine. You can check if the configuration was properly done because if it is not, the integration will not be possible.

- vi into context.xml file in your tomcat server

cd /opt   and ls
cd tomcat   and ls
cd webapps   and ls
cd manager     and ls
cd META-INF    and ls
sudo vi context.xml

or

vi /opt/tomcat2/webapps/manager/META-INF/context.xml

place a comment on <valve which end with />     that is 


<!--- <valve which end with />  ---->     and then, save and quit


Note the above configuartion in tomcat is done when you know it is a new tomcat server and you have not dobe such configuration on it before.

access application:

curl ifconfig.ca      to know your public ip address while hostname -i   is to know your private ipaddress

- Try to access your tomcat server in the internet browser by pastiing 

tomcatserverIpaddress:8080    on the internet browser

something ike this  http://3.138.246.169:8080/

- To finally do the integration of tomcat into Nexus, to back to Jenkins dashboard, click on our project name or the name of the project that you cloned to jenkins, click on configure, click on POST BUILD ACTIONS, click on, add post build actions, click on DEPLoY WAR OR EAR TO A CONTAINER, type in, target/*war  , click on ADD A CONTAINER, highlight the tomcat version you have. (you can know the version of tomcat that you have when you access tomcat with your ipaddress:8080 online. the version will appear on the front of the page) like, Tomcat 8.x Remote,  

To add credentials, click on add, click on jenkins, put in the login credentials that you have added to your tomcat-user.xml file in your tomcat server, enter (tomcat credentials on both the ID and description) and then click add to save

click on credetial icon and highlight/click your username. osas  put in your tomcat url. like, http://35.170.249.131:8080   and then click save

- Let us deploy the application, click on build now. It will start to build and create artifacts and finally deploy the application to tomcat. You can always check you console output to track or check the build process as it is building by clicking on # below and the click console ouptput

After the deployment is successul, you will see that the application which is usually the word that end with .war   like, myapps.war  copy the word before .war  like,  myapps

artifacts created and deployed to tomcat is  maven-web-app.war

- To access the application that has just be deployed to tomcat, paste the below on your internet browser.

tomcatipaddress:8080/theWordBefore .war   like below

tomcatipaddress:8080/myapps

SUMMARY: After developers has committed/push the code to github, with one jenkins command, jenkins will clone the code from github with the help of jenkins-github integration, Jenkins will run a Junit testing and create packages/artifacts via jenkins-maven integration, jenkins will run codeQuality analysis via Jenkins-sanarqube integration, jenkins uploads build artifacts to nexus via jenkins-nexus integration and finally jenkins will deploy artifacts to tomcat via jenkins-tomcat integration

tHE ABOVE IS HOW jenkins is used to automate tasks with just command.0

NOTE: It is good to enable jenkins and then start Jenkins. The reason is that if you start jenkins without enabling, anytime you stop jenkins server in your aws and start agin, you will need to also start jenkins server in your linux env. So always enable before starting.

sudo systemctl enable jenkins && systemctl start jenkins

VIDEO 73   JENKINS 3

7Cs:
   CD---> Continuous Development [Git/GitHub/IDEs /GitBranch/gitTag]

   CI --> Continuous Integration [Git/GitHub/Jenkins/maven/sonarqube/nexus] 
         [Build and Release Engineering]

   CD --> Continuous Delivery [build=packages/artifacts-->testing/UAT--manualApproval--- prod]

   CD --> Continuous Deployment [build=packages/artifacts-->testing/UAT--auto--- prod]

   CM --> Continuous Monitoring (to avoid system breakdown or infrastructure failure, continuous monitor would be automated by Jenkins also.)

            https://www.tesla.com/en_ca/powerwall  
            https://www.tesla.com/en_ca/modelx

   CS --> Continuous security

   CT --> Continuous Testing 

COMPANY TASK

Onboard a new project for Tesla  tesla-app =  
Create a Jenkins job to test, build and deploy the application to a tomcat server
Requirements:

  1. Create Team in our GitHub company a/c add and and assign member's roles and tasks
  2. Create a github repository for the  project  
  3. share the repository details for Team members and developers  
  4. Install and configure:
      jenkins for build and ci/cd 
      SonarQube for CodeQualityAnalysis  
      nexus for artifactory   
      tomcat as appSever 
      nginx as web server
  6. Create a Jenkins job for the project  
  7. configure Jenkins:
        maven Integration  
        SonarQube Integration  
        nexus integration  
        tomcat integration  
        git Integration
        GitHub  Integration
        notification [slack, email]

PROCEDURES/ANSWERS

Since already have a well modified src of tesla project in my github account. I will just use that as an exmaple.

My aim is to make tesla project repo that i forked from landmark github which is public to become private in my github by opening a private repo and then have all the content of the public repo to be on my private repo.

Knowing that you can not changed a forked repo of public to private since you are not the owner of the repo and you do not have the permission. What you will do is that,

a. create a folda/directory manually called testing in the downloads folda of your local computer and then go to your windows via gitbash and run a command

cd downloads/nameOfTheFoldaCreated    that is 

cd downloads/testing     run the below command to clone the public repo that you forked

git clone cd https://github.com/CLIFFOSA22/maven-web-application-1

b. Go to your github account and open a private repository and call it tesla-private-repo and the click on UPLOAD AN EXISTING FILE 

c. USING YOUR LOCAL COMPUTER, go to your downloads folda and to your testing folda, double-click the application and drag the important files like your scr and the pom.xml to your new repo you jUst created and then click on commit the change

d. rename the branch name called "main" of your new repo to a master by click the "main" , click on VIEW ALL BRANCHES, click on NEW BRANCH, type in MASTER, and click on CREATE NEW BRANCH. Click your projectrepoName called tesla-private-repo and then click on "main" and then select "master"

With this, i now have a private repo of the project because in realtime, private repo is what is integrated with Jenkins.

e. Let me integrated my private project repo with Jenkins by login to your Jenkins (osas-devops  admin123) to do the configuration. create a new project name in jenkins, write (tesla new project) as description, click on git, and then enter the url of your privateremoterepo

since it shows error, you need credetials that will validate you. under credential, click on add, click on jenkins, put your github username as the username here cliffosa22

You need to create a token as password, go to your github account, click on settings, click on developer settings, click on personal access token and click on token classics, click on generate new token, click on token classice, enter your github account password, you can change the expiration date to 60 days, enter your token name, click on repo to have permissions on what you can do, copy the token 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 and put it as password in jenkins credentials for the new project, and then save.

under credentials, click on none, click or highlight on cliffosa22-tesla new project. Go to the build steps and add all the corresponding goals and highlight maven version you used in the first project, follow the same procedures and also integrate with tomcat with same procedure and the same credetials.


 Jenkins slack or email notification
 ===================================
 Step 1) 

   Install Email Extension Plugin in Jenkins as follows:

Install “Email Extension Plugin “ (we might not neccessarily install this because during our installatiuon, email extension plugin may have been installed along side. just go to  Manage Jenkins ---> system ---> scroll down and you will find Email notification)
 

Step 2) When you get to email notification by scrolling down, it will ask you for smtp server
    Add the smtp server host.

        smtp = simple mail transfer protocol 

Your company they will have their smtp server host but for now, let us use the free smtp server, which is

 smtp.gmail.com

 you will see DEFAULT USER EMAIL SUFFIX: You can put in your company email add or the email address of the team or anyone that is tasked or saddled with the responsibility to monitoring the jenkins job. For now, let me use, landmark@gmail.com, the email may not be an existing email. it is because we are practising.

click on ADVANCED,  Cliick or tick USE SMTP AUTHENTICATION

 It will tell you to email username and password.

 Put your company email address and password.

 For now, let us just put landmark@gmail.com  and password  admin123

 Click or tick USE TLS.   The smtp port for TLS is 465

 under REPLY TO EMAIL; you can put landmark@gmail.com, telateam@gmail.com, developers@gmail.com   It usually sent response this emails as follow-ups

 click or tick TEST CONFIGURATION and put email address. Since all the email we put are unreal, base on practise, we can skip and click APPLY and click save. 

 go to your dashboard, click your projectName, click on configure, click on POST BUILD ACTION, scroll downward and click on add post build action and then click on EMAIL NOTIFICATION, put RECIPIENT email address: you can put landmark@gmail.com  if something is wrong with the build, email is going to be sent

 click or tick on SEND SEPARATE EMAIL TO INDIVIDUALS WHO BROKE THE BUILD  and the click save

 then click on BUILD NOW

 NOTE: The purpose of email notification is to notify the team of the status of the build and deployment of the artifacts that was carried out and if there was a problem, it can be resolved.


If you notice, 

BUILD NOW- manual= This is what we have been using anytime we want to build. This is the manual way of building/deploying on a project in Jenkins. Companies do not use BUILD NOW

BUILD TRIGGER: This is the automated way of running and accomplishing a Jenkins job. You can automate your builds using a BUILD TRIGGER

WHAT CAN TRIGGER A BUILD
   
A build can be triggered by the following way

1. Build Periodically: This uses a Timer

For example, Let assume you want to do database back-up. you can configure jenkins with a cron job or a timer, like configuring it to run this build every midnight. This means that,  every midnight, this build is going to run.

For those of you who uses WHATSAPP, you will notice that it does back-ups of messages or contacts every day usually 2.am early morning. That is a periodic configuration. A builds like (back-ups) happens on a particular time daily. It is periodic. That is a TIMER

EXAMPLE OF USING A TIMER TO TRIGGER A BUILD

Go to your jenkins web page, click on the project you want to work on. click on configure, click on build trigger, click on build periodically, typE in 5 ycap simbol on the schedule box (* * * * *) which means every minute, a build would be triggered. click on apply and then click on save, If you wait for a minute, you will notice that a build will commence by itself because it is every minute.

You will notice that a build is being carried out every minutes. Check out what triggered the build by check the console output of the build, you will see that its says, STARTED BY THE TIMER. This means that the timer triggered the build.

You can also configure it to be every hour typin the simbol (H * * * *) in the BUILD PERIODICALLY chat box and a build will be cariied out every hour.

NOTE: there is a space between the Ycap e.g   * * * * *  for every minutes    H * * * *   for every hour

The Timer is what we called, build periodically.

IQ
wHAT TYPE OF JOB SHOULD YOU CONSIDER TO BE BUILD PERIODICALLY BY USING A TIMER?

Answer: The rype of job that should considered be build periodicall is a database backup job.
Timer configuration would be good for backing up database for future data recovery should incase of disaster like computer crash or lost of phone. Whatsapp uses TIMER to backup customers contacts and messages very 2.am in the morning.

TIMER can be configured periodically in Jenkins like

 * * * * * means Every minute Jenkins will trigger a build  
     H * * * * means Every hour Jenkins will trigger a build

2.poll SCM:  This uses a timer and Changes in SCM

This usually happened when there is a changes source code manager due to changes in the source code. Maybe a developers has done some modifications to upgrade the application to a new version. Maybe from v12  to v13, this changes in the version of the application would be evident when there is a new commit-ID of the application in SCM. 

A POLL-SCM would be configured in Jenkins that would always search for changes in SCM like github periodically so that whenever there is a new commit-ID or new version of the application in SCM, a build would be triggered by Jenkins. This is what is called a POLL SCM

SCM POLL is good for software builds like software upgrade or software update, building and deploying a new version of an application.

POLL-SCM can be configured periodically in Jenkins like

 * * * * *  means Every minute Jenkins will check for changes in the src to trigger a build  
     H * * * * means Every hour Jenkins will check for changes in the src to trigger a build

EXAMPLE OF USING A POLL-SCM TO TRIGGER A BUILD

Go to your project, click on configure, click on build trigger, unmark or untick the build periodically and then tick or click on POLL-SCM, type in   * * * * *   on the SCHEDULE bOX,

which means that in every minute, Jenkins should run a periodic checks in SCM to see if their is any changes scr of the application or there is any new commit of the application in order to triger a build. If there is, jenkins will trigger a build, if there is not, no build would be triggered.  

click on apply and then click save. and then wait for a minute.

You will notice that it get to a minute, no build was triggered. This is because there is no changes in SCM.

Let us modify the src by modifying prof src. click on src, click on webapp, click on jsps, click on home.jsps, click on edit place, go to the heading tag <h1> of the src and then change the date on the script to a more recent date and then commit the changes. As soon as you have done that, go to your jenkin web page, you will notice that a build is triggered. When you check the console output OF the build, you will see that it says, CHANGE BY AN SCM CHANGE.

What is cron job used for?

A Cron Job is a Linux command for scheduling a code or scripts to execute.

Cron Jobs or a TIMER allow you to automate specific codes or scripts on your server to complete repetitive tasks automatically.
     
 Difference between CRON and TIMER

Cron job uses encrypted algorithm to place jenkins on a periodic mode to do a build. This is mostly used in CLI Jenkins

Timer uses words that can be readable to place Jenkins on a periodic mode to do a build. This is mostly used in GUI Jenkins

They both BUILD PERIODICALLY

3. Changes in SCM via GitHub-webhook 

 In this case, Webhook is configured in GitHub so that if there is changes in the SCM or src, Github notify Jenkins to trigger a build .  There is no periodic check involved here. Once there in changes in the src by modification, github will notify jenkins and jenkins will do a build.

Difference between POLL-SCM AND WEBHOOK

 The github will be the one to notify Jenkins immediately TO DO A BUILD if there is any changes in the src but for Jenkins-POLLSCM, it is Jenkins that will go to github to search for changes by itself depending on the time-schedule that was configured in jenkins by a cron or a timer, to be carrying out this checks.

 EXAMPLE OF USING A GITHUB-WEBHOOK TO TRIGGER A BUILD

 Copy the url of your jenkins server, in the form of  http://13.52.240.58:8080/

and then write github-webhook just after jenkins url, in the form of

 http://13.52.240.58:8080/github-webhook/      that is 
     
Jenkins Server url =  http://18.221.246.205:8080/
GitHub-webhook  url   =  http://18.221.246.205:8080/github-webhook/  

Go to your project, click on configure, click on build trigger, untick the poll-scm and then tick GITHUB HOOK TRIGGER, click APPLY and then click save.

Also go to your Github account, go to where you have your project (tesla project) repo, inside your project repo, click SETTINGS, click on WEBHOOKS located at your left handside, click on ADD WEBHOOK, it will ask you for password, enter your github password, under PAYLOAD URL, put in the jenkinsURL/github-webhook, in the form of http://18.221.246.205:8080/github-webhook/ , under CONTEXT TYPE  CHOOSE application/json, tick JUST THE PUSH EVENT and then click on ADD WEBHOOK and then wait until there is a green good-mark ti imply that everything is fine.

Let us do some modification on the src in github by going to the scr, clickm on edit, go to the heading tag <h1> of the src and do some modification, maybe adding, "Landmark is a powerful E-learning Platform"  and then commit the change

As soon as you commit the change, github will notify Jenkins immediately and build will commence within 2-4 seconds. It is usually very fast. 

Github webhooks is faster than the other 2 above. Github-webhook would be useful also for 

  Software builds like software updates/upgrade
  build and Deployment of a newer version of application

 When you check the console output, it will tell you, STARTED BY GITHUB PUSH BY GITHUB-ACCOUNT

 This is like an API call, CLIFFOSA22 is doing what is called an API call, That is, Cliffosa22 is informing Jenkins to trigger a build due to changes that occured in the SCM.


 IQ: WHAT IS UPTIME

 Uptime means, how long a server has been up and running. If you have a server that 1000,000 minutes without shutting down, it is a good server.	

 Note, It is not good to open all port. It is a security bridge. If you go to work and open all ports, you will be fired. Please take note

NOTE: Maven go with the shh port number 22 because you do not access maven from the browser. You can access Sonarqube (ssh port number 22 and sonarqube port number 9000), Nexus (ssh port number 22 and nexus port number 808i), Jenkins (ssh port number 22 and jenkins port number 8080), tomcat (ssh port number 22 and tomcat port number 8080)						


4. Build other projects  

     Upstrean project: 
       - build & release engineering  or CI job

       - Downstream project 
             deployment. That is deploy2 to app. server like tomcat

Explanation of 4:

Jenkins would be configured in such a way that anytime build and release engineering or Continuous integration is completed, it will auto-trigger the downstream project to deploy the artifacts to app. server


NOTE: during the course of my study using a private repo from my github account integrated with my Jenkins, i notice that using A TIMER to build and deploy worked on the TIMER but using POLL-SCM and GIT-WEBHOOK did not work on my private project repo.

When i used the public project repo, everything worked. Please out if this is true.

VIDEO 76 CONTINUATION OF JENKINS

IQ. You make be asked in an interview, what are your TOOL STACK OR TECHNOLOGY STACK?
This means what are the technology that you have been using in you work or what technology or tools that you have been exposed to or what are the name of the tools you used in your technilogy?

Answer: My tools stack are as follows

Linus, Windows, git, github, aws cloud, maven, sonarqube, sonarcloud, nexus, java, apache, jenkins, tomcat, Nginx, vim, bash scripting etc

What do we do with Jenkins?

We used Jenkins to automate tasks or jobs or projects or workloads

W2hat do you use to automate task in Jenkins?

Build Trigger is used to automate task in Jenkins. This Build Triggers are 

1. Build Periodiucally using a timer
2. POLL-SCM using a timer and a change in SCM
3. Github webhook
4. Builds other projects

Types of Jenkins jobs:
======================

- Freestyle  project: Generally, all kinds of job can be run using a FREESTYLE PROJECT

We are been using freestyle projects in building and deploying.

Example

Lets us rum another Freestyle projects. Let us create andother project in jenkins and name it tesla-dev.   

Go to my github account, go to the public tesla project that i forked from prof. in github, and create other branches called development and also stage branch from master. You know that if you create other branch from master, they will automatically have all the contents inside master.

Firstly, Let us run the tesla repo on the dev branch. Follow corresponding processes that we used before to execute jenkins freestyle job or project by clicking on new item, name the project tesla-dev and follow the other steps to complete it just as previously and then build.

Note: The repo you are to copy or clone from github is always in master. so copy when you are in master, and the on the BRANCH SPECIFIER, you can put */development of any other branch that you want to build from. We want to build from development branch for now.

We created a new tesla repo called tesla-project-dev in Jenkins and clone code from development branch in github and then build and deploy the application via Jenkins Freestyle jobs.  

Note: Jenkins make use of plugins called plugins management. Plugins extends the functionality of Jenkins

- Maven project.

Let us build and deploy application using Jenkins Maven project. If you check the default types of Jenkins Projects/jobs on our Jnekins webpage, you will notice that there are no MAVEN PROJECTS installed or present. We need to install maven plugins to be able to run maven projects.
Click on manage jenkins, click on plugins, click on available plugins, type in maven on the chat box, and then tick maven integration and the click INSTALL WITHOUT RESTART.

After it has been installed, let us create another repo that will build from jenkins maven project that we just installed. name the repo tesla-uat      (user acceptamce testing = uat)
click maven project, click on build trigger, click on BUILD AFTER OTHER PROJECT ARE BUILT, type in the tesla repo that needs to build before this repo can be triggered, like tesla-dev, click on TRIGGER ONLY IF BUILD IS STABLE. do other configurations, clone code from stage branch, no need to INVOKE MAVEN TOP LEVEL, under build, type in deploy, under post build, click RUN ONLY IF BUILD SUCCEED, click on POST BUILD ACTION, 	click on deploy to war/ear container, type in the directory target/*war  , click on ADD CONTAINER, click on the version of tomcat you want to use, 9x version, click your app server container credentials, put in your tomcat url  and the click apply and click save

When you go back to your tesla dev repo in jenkins and then click on build now, you will notice that as soon as it finish building, tesla-uat in jenkins would be trigger and start to build and deploy to tomcat,

If you configure your tesla-dev repo BUILD TRIGGER in jenkins to BUILD PERIODICALL OR POLL-SCM OR GITHUB WEBHOOK, Once they start to build depends the one u used, once they are done with build, it will trigger tesla-uat to deploy to tomcat or app. server.

Note; THis explains the 4th different type of build triggers that is in Jenkins. This means deployment (to the app. server like tomcat) would trigger when other project build (like artifacts tested and created, codeQuality analysis done and uploads of artifcats to nexus) successfully.

We can use the tesla-dev repo to be THE OTHER PROJECT, THis means that if tesla-dev build successfully, it will trigger tesla-uat to deploy artifacts to tomcat.

When you are building using maven configuration in Jenkins, it will

    - reduces the configurations and also increase flexibilit when you build with Maven   
          mvn package  [ pom.xml, src]   
          apple [ apple.xml, src]  You just renamed your pom.xml to apple.xml in your github repo. For it to build, go and configure your repo in jenkins whose repo was changed to apple.xml in github, change it to apple.xml and then click build, it will build successfully.

In generally, when you are building a maven project, it is better to use maven configuration than using a freestyle configuration.

NOTE: For everyth build that you run in Jenkins, it consumes the companys resource. If you have 14 build on your build history, It means more consumption on the companys reseources has been made. Alot of money has been spent. so be very careful with the way you build and make sure you are doing the right thing in order not to consume too much reseourse 

- Pipeline projects  

- Multi-Branch Pipeline projects; This explains how you can build projects from multiple branches at once

NOTE:

when you build in jenkins, you can access or see the artifacts that was built in the workplace in jenkins.

The absolute path to the workspace is /var/lib/jenkins/workspace/tesla-app

Jenkins Home Directory = /var/lib/jenkins/

Jenkins User = jenkins. When you install jenkins, you have a user called jenkins automatically (Which is an automation server).

Let us check in actually, a user called jenkins was created along with jenkins installation by running

cat /etc/passwd  or tail -6 /etc/passwd  (to know the last six user that was added to the etc/passwd file)

You will see that there is a user called jenkins. Let us switch user to jenkins

su - jenkins  = run as a normal user  which will not respond

sudo su - jenkins  = run as the root user    

After you run above command, you will notice that it will not switch to jenkins. if you run

whoami   it will still say ec2-user

FOR YOU TO BE ABLE TO SWITCH TO JENKINS. DO THE FOLLOWING BELOW

Let us vim into /etc/passwd file to modify the line that has jenkins detaills by changing FALSE to bash. and then save and quit. run the below

sudo su - jenkins     and the run

whoami   or   pwd    you will notice that it will say

jenkins      when you run pwd, it will say  /var/lib/jenkins

You will notice that you are im jenlins user and present working directiob pwd is /var/lib/jenkims

Remember that mavenHomeDir = /opt/maven   

JHD = /var/lib/jenkins/:

if you     ls -a   you will see some important directories like below

workspace: This directory is that one that housed all the projects repo or directories that you have created.

If you cd into workspace and ls,

 you will see all the projects that will have done so far.
if you are in workspace, cd tesla_uat , 

you will see some important files and directory like, pom.xml which is the prject file, you will see src and target directory that house the built artifacts

jobs:  This directory records all the builds you run on a project. If you want to see the number of builds that was run on tesla-new, using Jenkins CLI, run

cd jobs, ls, cd tesla-new, ls, cd builds, ls
     or
ls jobs/tesla-new/builds     you will see the number of builds you did. This consumes our resource like server resourse or space that you have.

plugins:

 This directory contains all the plugins that was automatically installed when we install jenkins in our server and the other plugins that we installed by ourself. You can cd into it and you will seee all the installed plugins. These are the plugins that make jenkins to achieve automation

Note; All Jenkins plugin files end with .jpi  example; deploy.jpi, junit.jpi, maven-plugin.jpi

Credentials.xml: This is a file that stores all credentials that you used in jenkins. Cedentials like username and password.

Jenkins options:  Jenkins have options you can use to manage your projects or task

  1. Discard Old Build; it is important to always delete old builds to save more space or resources for new builds or to accomodate new builds.  You have to configure you project artifactory or repo in a way to that you set how many days you want the build to stay in your server.

  To discard old builds, go to your project repo/artifactory in jenkins UI and click on configure, tick discard old builds, for DAYS TO KEEP THE BUILD put maybe 15 days depends on what you want, for MAX BUILDS TO KEEP, put may 3  and then click apply and click save

 Since you said to keep 3, anytime a build is triggered or you build manually, only 3 builds would remaina. 

 Example, in my jenkins tesla-new, i have about 19 builds. I will configure my tesla-new project to keep just 3 builds and then click build now to see if it works. When i did it, it worked

     Building in workspace /var/lib/jenkins/workspace/tesla-app

2. Disable this project; You can disable project if you are not using it for the time being. 

  This can be achieved by clicking on the project repo name, and click on DISABLE PROJECT which is located at your top right hand side.
Once you click on it, it will say, This project is currently disabled 

At what time do you want to disable your projects?

Answer: You can disable your projects when:

a. Schedule Maintainance of servers has started
b. When you want to back-up your database

NOTE: If you have done a build and it has been deployed to tomcat, this means that the application is already running in the app.server and so, if your maven or sonarqube or nexus or jenkins server is down, it will not affect the application that is already running in tomcat

IQ; WHY SHOULD YOU INSTALL DIFFERENT SERVER WHEN YOU CAN JUST INSTALL ONE SERVER LIKE JENKINS AND THEN INSTALL EVERY OTHER SERVER IN IT.

Answer; If you install one server like Jenkins and then install other server including tomcat into it, you might be faced with a situation whereby the Jenkins server would be down, this will affect every other server that was installed in it and so if you already have an application running in tomcat, it will affect the application  and so the application will stop running because tomact was installed in Jenkins. This is for Jenkins server

It is of best practise to have  Jenkins server and different server to avoid such disasters. 

         Jenkins-Git/GitHub  Jenkins-Maven  Jenkins-sonarqube  Jenkins-nexus  Jenkins-tomcat

WORKING ENVIRONMENT SCENERO

If Sonarqube server or nexus is undergoing maintenance or database backup, and you have a project that has been configured to be building periodically in every 1 hour. That is 

            H * * * * 
That job or project will fail  because every build will follow all Jenkins integration process outlined above.

In the above scenero, the best thing to do to avoid the application to stop running in the application server is to disable the project that was configure for the build periodically. By doing so, even though sonarqube or nexus stop running due to maintenance, the application will still be running in tomcat.  If you are done with what you are doing? you can enable the project

3. Delete workspace before build startsDE; You can delete the workspace before any new builds starts by yicking DELETE WORKSPACE BEFORE BUILD STARTS . That will delete old builds and replace with new build anytime a build is triggered by autoation or by manaul build

4. Add timestamps to the Console Output;  This is when you configure jenkins ui by attaching time spent to every stages that the applications undergoes before a build is completed which can be viewable in the console output.

 Go to your project container or artifactory or repo in jenkins UI and click on configure, click on build environment, click on ADD TIMESTAMP TO THE CONSOLE OUTPUT and click apply and click save. click build now to verify what you did.

  sudo timedatectl set-timezone America/New_York          you can set your time zone to new york

  5. build with parameters:  You can confure by adding additional parameters for a build to follow and for a build to be completed
  go to your project, click on configure, tick on THIS PROJECT IS PARAMETIZED, click ADD PARAMETERS, click CHOICE PARAMETERS, under NAME, you can choose your choice name like "BranchName" write your CHOICE vertically inside the box like below

master
development
stage

On the DESCRIPTION, you can put, please select the branch to build from.  This means that you can given instruction

You can also Click on another ADD PARAMETERS and then click on STRING PARAMETER, under NAME, just enter "name"  and then enter DEFAULT VALUE as Clifford Egharevba, amd DESCRIPTION box, wrtie,  Please enter your name   and then click appley and click save

If you click on build now, it will dsiplay what you just configure ask you to select a branch name and then ask you to enter your name, use can use another name or yours and then click on BUILD. What you have done is that you have added parameters to your build.



  Plugins Mgt  
============
 ---Plugins extends the functionality of our severs like jenkins, maven, etc.   

PROJECTS: 

  In LandmakTechnology I work in a Team where we:
1. Develop, test, build, deploy, secure,  manage & monitor the web and 
   enterprise applications for FinTech our clients;

   Our fintech clinets inckudes:

           Banks  [ boa, wf, barclays, rbc, td, bicec, uba, ecobank ] 
           Insurance [BLI, WFG, TD, Sunlife, AIG  ] 
           Money Transfer [zelle, cashApp, interact, MTN-MOMO,
           payment gateways = VISA, MASTERCard, Paypal

    2. We automate the entire process stated above.


PROJECTS Requirements: Below are the software or tools we need to execute our projects or jobs or task, This is also called technoloy stack

    Git [git-bash]:
        IDEs = Integrated Development EnvironmentS, that simplifies the deployment process. Example of the IDEs are vscode, pycharm, eclipse, atom etc  
        vscode, pycharm, eclipes, atom 

    SCM=GitHub : 
        Create PROJECT REPOSITORIES and/or organisation  
        TEAMS with required/minimum permissions/access granted
        Branching 

    maven:   
    SonarQube or SONARCLOUD /
    Tomcat
    NewRelic   to monitor the application
    Docker     to containerise
    kubernetes
    ANsible      to run traffic from users via webserver to the different app. server
    terraform    to create our server

    It is not recommended to run multiples services/applications in the same server  
    because that can result to a single point of failure  

aws account:
 Create your infrastructures in aws: 
    ec2 instances - servers / virtual manchines[VM]
 -- NameTAGs are for easy identification   
    EC2=VM - SonarQube = because we have the SonarQube software installed 
    EC2=VM - Nexus     = because we have the nexus software installed 
    EC2=VM - Tomcat    = because we have the tomcat software installed    
    EC2=VM - Jenkins:  = because we have the jenkins software installed    

But with terraform,  we create all required infrastructures/servers/ec2 in  just 1 command

Creating servers separately like we have been doing when we will create maven - sonarqube - nexus - ansible - tomcat - jenkins server. These are a manual work, its time consuming. Terraform makes it fast and easy.

Terraform will automate how to create all these server (maven - sonarqube - nexus - ansible - tomcat - jenkins server, ec2 instances and others) in just one command


Ansible we configure the entire infrastructures/servers/ec2 just with one command.

As an engineer, if you can know how to run these 4 software below:

Jenkins  / Ansible / terraform  / kubernetes 

There is no way you will not be hired because these are the 4 peak of automation in the market. Take note.

Infrastructure Engineer:  With Jenkins we achieve the below

    Continuous integration     
    terraform apply  

PLUGINS MANAGEMENT IN JENKINS: 

These are as followS

Plugins Extends the functionality of JENKINS.

• JACOCO = Java Code Coverage  it is a java plugin found in Jenkins

• SSH: 

Helps Jenkins to run commands remotely in other Linux servers

• Publish Over SSH: 

a. Helps to transfer files from jenkins to other Linux servers. For expample you can use Jenkins to transfer artifacts from jenkins into another Linux servers with the help of this plugin.
b. Helps Jenkins to run commands remotely in other Linux servers  
     
•  SSH Agent: 

a. Helps to transfer artifacts from jenkins into another Linux servers
b. Helps to connect Jenkins master to slaves/agents  
        maven-web-app.war
        TomcatServer -- EC2-Linux-RedHat-Instance 
    Which protocol is used to copy files/dirs into a Linus server?? SSH 
            /webapps  

• Deploy to container plugin--- 
    deploys applications into Tomcat/GlassFish/JBoss servers

• Deploy WebLogic plugin:

 WEBLOGIC is an app. server like jboss, tomcat in which an application can be deployed to

deploys applications in WebLogic servers

• Maven Integration  plugin

      spring-boot-mongo-1.0.jar 

• Safe Restart plugin: 

This is a process of using safe restart plugin to restarting your Jenkins server without stopping the server suddenly but allowing the projects or jobs running in Jenkins server to finish run before restarting tghe server.

jobs are running 
      
sudo systemctl restart jenkins

• Next Build Number plugin: 

By default, if we are building in Jenkins, it reads 1, 2, 3, 4. With this plugin, you can change your build number to  dev1, dev2, dev3 or uat1, uat2, uat3 or prod1, prod2, prod3.

     1, 2, 3, 4 
     dev1, dev2, dev3 
     uat1, uat2, uat3 
     prod1, prod2, prod3

• Build Name Setter:  also called, BUILD NAME AND DESCRIPTION SETTER

This can also be used to change your build name to another name

• Email Extension

• slack notification  

• Jira Integration  

• SonarQube Scanner plugin

• Audit Trail:

This plugin can be used to audit any Jenkins user. It used to hold any user accountable for the task or job that they are doing.

• Audit log:

There can be multiple users in Jenkins
 
        username=simon | pwd=admin123
        username=percy | pwd=admin123
        username=mercy | pwd=admin123
        username=florence   
        username=olu  


• Schedule Build:  

You can schedule a build to place at a particu;ar time

• Artifactory Plugin:

• Cloud Foundry

• Blue Ocean  

• Publish Over SSH
    jenkins --> ansible --deployment

• ThinBackup

• Convert To Pipeline

• Job import plugin: This plugin is used for disaster recovering

For example. If the version of your server1 is redhat6 and the version of your server2 is redhat8 and you can simply install JOG IMPORT plugin in server2 in order to be able to import the content in server1 to server2.

  jenkins migration 
    Jenkins server1 redhat6 
    Jenkins server2 redhat8  
    Jenkins server3 docker-container
    decommissioning Jenkins Server

• Maven projects  
     25 mins 
  freestyle projects
     30 mins 

Let us install all this plugins in our Jenkins. Although, some of the plugin has been installed by default when installing Jenkins.

Note; If you want to install, click on dashboard, click on MANAGE JENKINS, click on PLUGINS, click on AVAILABLE and with the search box, type in whatever you want to intall


Jenkins master slave/agent integration



SH AGENT Plugin
================

master slave/agent integration

Why did we install SSH AGENT plugin?

We are use SSH AGENT plugin to achieve what is called  Jenkins master - slave/agent integration 

master slave/agent integration  

Example

Let assume that in our env., we have about 100 Jenkins jobs and these multiple jobs are going be running in Jenkins server, how is jenkins server going to be able to manage this?  How do we reduce the load on Jenkins?

Answer: We can reduce the load on Jenkins by creating Jenkins master slave/agent architecture.

This a Jenkins architecture created in such a way Jenkins builds from other server called agent or slave like ec2 redhat server, ubuntu etc.

Jenkins uses other server as slaves to build and deploy application. By so doing, Jenkins have reduce the load on its server.

Let say we have a server called AGENT1 and another server called AGENT2. 

With the help of JENKINS EXECUTOR, Jenkins master can run job on AGENT1 and AGENT2

All builds in Jenkins would be configured to take place in Agent1 and Agent2. It can be multiple agents. The idea is to reduce the workload of Jenkins.

JENKINS MASTER, for it to be a Jenkins Master, Java and Jenkins must be running or must be installed

AGENT1 AND AGENT2, 	For it to be a healthy slave or agent, java must be installed and must be running

Jenkins Master would be able mto connect with the slaves called agent1 and ganet2 via TCP internet protocol (IP) called ssh protocol 

JENKINS EXECUTOR: 

JENKINS EXECUTOR: Jenkins executor makes it possible for Jenkins master to be able to run or builds jobs/projects on Jenkins slaves or agent

Let me rename my linux server to Jenkins-Agents by ticking on the linux server and the click a sign to looks like < sign

I registered my ssh key by saving my ssk-key file on my Jenkins-Agents server and then run 

ssh -i "maven.key" ec2-user@ec2-3-101-53-49

and then to make my maven.key not to be publicly viewed, i ran a command on Jenkins-Agents server 

sudo chmod 400 maven.key

Let us copy the shh clients link from our aws ssh client by clicking on the Jenkins-Agents server on aws, click on connect, click on aws client, scroll down and copy the link just after EXAMPLE. Which looks like below:

ssh -i "maven.key.pem" ec2-user@ec2-3-12-103-47.us-east-2.compute.amazonaws.com

Let us connect server as a Jeninks slave or agent by going to Jenkins UI. For this to happen, the server must have the credetilas needed like

sshKey    =  maven.key.pem.pem
 username  =  ec2-user
 AGENT1 IP-ADDRESS   =  3-12-103-47 PublicIp / 172.31.16.136 privateIP

 We want to configure or create Jenkins master slaves/agents archotecture using ssh agent plugin

 lET US CREATED THE AGENT1 with publicIP AND AGENT2 with privateIP of the same server. wE dont want to create so many server, that is why am creating agent in public and private Ipaddress of the same server

 Go to your jenkins UI, click on MANAGE JENKINS, click on NODES, click on NEW NODE, let us call the name of the node AGENT1, click on PERMANENT AGENT, click CREATE, put 2 for NUMBERS OF EXECUTOR: This means jenkins will be build from master and from agent1, put /home/ec2-user  as the REMOTE ROOT DIRECTORY, choose USE THIS NODE AS MUCH AS POSSIBLE for under USAGE, choose, launch via ssh for LAUNCH METHOD, put your serverIpAddress as the HOST, click ADD to create credential for agent/slave, put ec2-user as the USERNAME, 	select USERNAME WITH PRIVATE KEY for KIND, write 'Agent-Credentials' on an ID, DESCRIPTION. put ec2-user as a USERNAME, tick ENTER KEY DIRECTLY, click on ADD KEY FILE, copy your key and paste it on the box, and click ADD, select MANUALLY TRUST KEY VERIFICATION STRATEGY on HOST KEY VERIFICATION STRATEGY, uNDER CREDENTIALS, select the agent credentials you just created and added. and then click SAVE.

Go to your project to run a build. click on your organization name, click on MY VIEW  (this will bring out the prjects you have been working on) 

First, click on the project repo, click on configure, tick RESTRICT WHERE THE PROJECT CAN BE RUN, write "Agent1"  on the LABEL EXPRESSION, click save

Note: LABEL EXPRESSION is case sensitive. You must enter the word just like the way you configured it on the NODE.

 If you run build now, you will notice that it is building the application renotely on jenkins slave or agent called agent1 

 whereas other jobs we are beem doing build normally from jenkins master


 CONFIGURE AGENT2 using private IP

 click on CREATE NEW NODE, tick PERMANENTLY AGENT, click COPY EXISTING NODE and type in agnet1(this will copying all the configuration you in agent1) but you will have to still edit number of executor to be 3 since we alredy have Jenkins-master and jenkins-agent1 executor, and also create another credentials for agent2 with private ip, change the IP of public to privateIP and then apply and click save.

Building remotely on Agent2 in workspace /home/ec2-user/workspace/tesla-dev  build remotely on jenkins slave or agent

Building remotely in workspace /var/lib/jenkins/workspace/tesla-dev  build normally from jenkins master

BASICALLY, With the help of JENKINS EXECUTOR, Jenkins has been able to create artifacts successfully in other server called Jenkins-slave/agent. This is waht is called Jenkins Master-Slave/Agent Architecture.

Note: The architecture also trigger the successful deployment of the downstream job (That is, deployment to the application server

Note: Using CLI, you can check the command that you have executed by running or just type "history"

VIDEO 78

=========================
Technology stack:
  Linux Servers/OS 
  Windows 
  maven 
  Nexus  
  SonarQube   
  Bash shell scripting    
  Git  
  GitHub  
  SonacLoud  
  AWS cloud     
  NGINX 
  java  
  Tomcat 
  Jenkins   
--------------------------------

KEY INTERVIEW QUESTIONS FOR JENKINS

IQ: Explain your experience using Jenkins in your Environment??? 
IQ: What problems have you faced using/applying Jenkins in your Environment??

1. We use Jenkins to automate tasks and run projects or workloads like,:
   ---  freestyle projects
   ---- maven projects  
   ---- Pipeline projects    

2. With Jenkins i have managed projects such as:
    --- software or application [builds and testing and deployment ]  
    --- systems or application monitoring  
    --- database backup
    --- Infrastructure creation / provisioning
    --- Infrastructure Configuration mgt 
3. I have installed, removed and updated plugins to extend Jenkins functionality:

4. Using Jenkins Plugins to achieve automation: Some exaples of the Plugins are
  ssh, sshAgent, Publish-over-SSH, deploy to container, Slack notification,   
  job import, jacoco,

5. Troubleshooting jenkins build related problems like:

 a- permissions issues, Unauthorized [4** error codes]

b-- Jenkins agent/slave failing to connect to Jenkins master which sometimes id due to

    [java not install] or  wrong java version or AUTHENTICATION problem

6. Installation and configuration of the Jenkins server in Linux/Windows:     

7. Securing the Jenkins server.

JENKINS PIPELINE JOBS

  Jenkins Pipeline scripts are generally written in groovy 

  Jenkins Pipelines scripts are called Jenkinsfile 
 
 THERE ARE 2 TYPES OF JENKINSFILE

1.  SCRIPTED JENKINS-FILE
2.  DECLARATIVE JENKINS-FILE

1. SCRIPTED JENKINS-FILE. this script start like below

node{
  stage('1cloneCode'){}
  stage('2Test&Build'){}
  stage('3codeQuality'){}
  stage('4uploadArtifacts'){}
  stage('5deploy2UAT'){}
  stage('6approvalGate'){}
  stage('7deploy2Prod'){}
  stage('8emailNotification'){}
} 

Above is how a Pipeline project looks like.  Everything that you want to do will be inside the empty curl bracket

REMEMBER, Builds in Jenkins take place in the NODE when it comes to Maven and Pipeline projects. 

The Tesla project that we are working on has 2 repositories, private and public repository for prof repo.

https://github.com/LandmakTechnology/maven-web-application  PUBLIC
https://github.com/LandmakTechnology/maven-web-app = private  Repo

Let us build from public repository.

Let us clone the public repo by doing something like this below (This is my pipeline project. This is how it starts)

node{
  stage('1cloneCode'){
    git "https://github.com/LandmakTechnology/maven-web-application"
  }
  
Please check the recent version on maven that you are using in your Jenkins server to error. Prof maven version is an old one.

LET US LAUNCH A NEW PROJECT USING JENKINS PIPELINE

Copy the above groovy script, go to Jenkins, click on my view, click on new item and name the item (like, tesla-prod ), select PIPELINE, and click ok. scroll down, under PIPELINE SCRIPT, paste the script exactly inside SCRIPT 1, click apply and click save in order to clone the code from github. click on build now to see what happens

The above script can also be wrritten as 

node{
  stage('1cloneCode'){
    // git "https://github.com/LandmakTechnology/maven-web-application"
    sh "git clone https://github.com/LandmakTechnology/maven-web-application"
    }

What happened above is that we are using a bourne shell clone in Jenkins and then we put a single line comment on the top url so that it will not run. You can paste it on script 1 also, click apply and click save. If you click BUILD NOW, you will see that it has clone the application from github. They are going to do the same job by cloning code or repo from github.

NOTe: There was an issue that came up when trying to reclone the tesla project from github using tesla github repo url that started with sh, that is 

sh "git clone https://github.com/LandmakTechnology/maven-web-application"

when we click on build now, it was not successful. It stated that the file already exist. We troubleshooted and we used the one with sh. That is,

git "https://github.com/LandmakTechnology/maven-web-application

It was successful. So it is better to use the one without sh to clone code or project from github

NOte: single line comment in groovy  = //  
      multi-line comment in groovy  =  /*  */

NOTE: JACOCO means Java Code Coverage. is a plugin that was was installed in Jenkins server

Note: If it is windows OS, you replace the sh with bat. That is 

node{
  stage('1cloneCode'){
    bat "git clone https://github.com/LandmakTechnology/maven-web-application"
    }

Above is an example of how each of the node works.

Please note that, we installed maven in a Jenkins server as a tool. Maven is a plugin in Jenkins Server and so should be included. For you to know that versuion of maven that you installed in Jenkins,

Go to your Jenkins server by CLI, switch user to jenkins by running sudo su - jenkins, run ls, cd into tools and run ls, cd in hudson.task.maven_maveninstallation and ls and you will see the version of maven you installed using CLI.  you can also found out using Jenkins UI.

This means that you have to define mavenHome as a tool name which is the version of maven installed. That is

def mavenHome = toolname =  maven3.8.6

Please confirm the version of maven you installed as a tool in your Jenkins server

Also, once the code has beeen cloned, Maven would have to test and create package, this means. you would also need to include the absolute path to mvn that indicate maven going forward. 

The absolute path to mvn is {mavenHome}/bin/mvn

{mavenHome} is called, a  function of maveHome

{mavenHome} is jenkins/tools/hudson.task.maven_maveninstallation/maven3.8.6

which has the content bin and inside bin, you will find mvn. Therefore, When you want to run anything mvn, you would have to include mavenHome/bin/ before mvn going forward in Pipeline project. That is 

{mavenHome}/bin/mvn clean package
{mavenHome}/bin/mvn sonar:sonar
{mavenHome}/bin/mvn deploy

To call out this function of MavenHome, you must include a $ sign to it

${mavenHome}/bin/mvn clean package
${mavenHome}/bin/mvn sonar:sonar
${mavenHome}/bin/mvn deploy

Note: To clone the code, you can do without using the bourne shell  sh  but going forward, you can use  sh.

Please you can disconnect all your Jenkins agent so that this Pipeline job can build from Jenkins master. Go to your node to access your jenkins agent or slave and diconnect the agents.

Let us combine all the node to run in one command. You configuration will now be 

node{
def mavenHome = tool name: 'maven3.9.2'
  stage('1cloneCode'){
    git "https://github.com/LandmakTechnology/maven-web-application"
  }
  stage('2Test&Build'){
    sh "${mavenHome}/bin/mvn clean package"
    //bat "${mavenHome}/bin/mvn clean package"
  }
  stage('3codeQuality'){
    sh "${mavenHome}/bin/mvn sonar:sonar"
  }
  stage('4uploadArtifacts'){
    sh "${mavenHome}/bin/mvn deploy"
    //http://52.53.227.31:8191/landmark/repository/tesla-fe-snapshots/
    //http://52.53.227.31:8191/landmark/repository/tesla-fe-releases/
    // vi pom.xml and added repository details   
    // vi setting.xml and add AUTHENTICATION details  

  /* If the artifacts failed to be uploaded to Nexus, It means that your pom.xml file may be broken, you can launch another nexus server or skip by placing a comment on the deploy command sh "${mavenHome}/bin/mvn deploy"    as

 sh "${mavenHome}/bin/mvn deploy"    // means a single line commect of a groovy script

 After the upload of artifacts, let us first of all deploy the artifacts to UAT in the staging
 testing environment for clients validation and acceptance. 

Let us go back to project. click configure, go to where you are writing your pipeline commands, under it, click on pipeline syntax, select deploy:Deploy war/ear to a container. Container here is referred to an application server like tomcat, under war/ear files write target/*war , click on add container and select the tomcat container you wish to use which is usually the newest version that appears last , under credentials, select your tomcat server you to deploy to, paste your tomcat url, click on genrate pipeline script and copy the script inside the box, go to your pipeline command script you are configuring and paste the generated script the next line after sh when you press enter key to take you to the next line  and then click on apply and save. That is below. Copy the below command and paste in your pipeline job and then click build now. My tomcta credentials is cliffosa  admin123 logimn */

  } 
  stage('5deploy2UAT'){
    sh "echo 'deploy to UAT'  "
    deploy adapters: [tomcat8(credentialsId: 'tomcat-credentials', path: '', url: 'http://35.170.249.131:8080/')], contextPath: null, war: 'target/*war'

 /* If the artifacts is now in UAT environment, it is left for the client to accept or reject the built artifacts. That will bring us to the approvalGate where the client will approval or disapprove the artifacts as explained below, copy and paste it in your pipeline job and click build now. Once it is now in the approval gate, the client"s QA/BA team will have to review and if they are not cool with the application or aritifacts, they can click ABORT but if they are cool, they can click PROCEED. If they clicked or ABORT, developers will be contacted and developers will look into the src to try to modify the code to suit what the client want and then commit the changes. The clients will review the application again. According to our configuration for the approval gate, the time set for client to review and make their decision is 5 days maximum. Note: In the deployment of the application to approvalGate env., may be a tomcat server or another application server but know that deployment to approvalGate is always carried out in application server where the client can review */

  }
  stage('6approvalGate'){
    sh "echo 'ready for review' "
    timeout(time:5, unit:'DAYS') {
    input message: 'Application ready for deployment, Please review and approve'
     }


/* If it was approved or passed the approvalGate, the application woulkd be deployed to production be the configuration below. Since we are using the same tomcat server for now, we can copy only the generated pipeline script you did before and paste it where it is meant to be as illustrated below */

  }

  stage('7deploy2Prod'){
    deploy adapters: [tomcat8(credentialsId: 'tomcat-credentials', path: '', url: 'http://35.170.249.131:8080/')], contextPath: null, war: 'target/*war'

/* The next is email notification, click on pipeline syntax, click on SAMPLE STEP, select EXTENDED EMAIL, enter email address like teslaapp@gmail.com where you want to send email to on TO, click on ADD to add build-users, click ADD again to add developers, click on ADD again to add upstream committers, click again, others team or persons so that they can always get update about the projects that they where part of and if their is failure, each architectire or arm would be alerted and whoever cause the application to fail would be aware or notofied, under subject, put Build Status, under BODY, you can write  check build status and then click on generate pipeline script, copy andpaste it inside the empty curl brackect where email notification is, click apply and save and then click on build now */

  }
  stage('8emailNotification'){
    emailext body: '''Hi All,

Check Build status.

Landmark Technologies''', recipientProviders: [buildUser(), developers(), upstreamDevelopers(), brokenBuildSuspects(), brokenTestsSuspects(), contributor()], subject: 'build status', to: 'tesla-app@gmail.com'
  }
}

By doing the above, you have achieve DevOps end to end automation by utilizing a JenkinsFile.

Just like the Freestyle project. After you are done with all the config required, just one click or just one single command, end to end automation would be achieved. 

You can launch another job and click on build now, it will clone code from the projectRepo in github, done testing and create artiufacts, run CodeQuality analysis, upload artifacts to nexus, deploy artifacts to uat and the deploy artifacts to approvalGate and finally deploy artifacts to production

The following below order was use to do the configuration

node{
  stage('1cloneCode'){}
  stage('2Test&Build'){}
  stage('3codeQuality'){}
  stage('4uploadArtifacts'){}
  stage('5deploy2UAT'){}
  stage('6approvalGate'){}
  stage('7deploy2Prod'){}
  stage('8emailNotification'){}
} 

Before you start, paste it inside your pipeline script box and then begin to edit as required as shown below.

node{
  stage('1cloneCode'){
    git "https://github.com/LandmakTechnology/maven-web-application"
  }
  stage('2Test&Build'){
    sh "${mavenHome}/bin/mvn clean package"
    //bat "${mavenHome}/bin/mvn clean package" for windows only
  }
  stage('3codeQuality'){
    sh "${mavenHome}/bin/mvn sonar:sonar"
  }
  stage('4uploadArtifacts'){
    sh "${mavenHome}/bin/mvn deploy"
    //http://52.53.227.31:8191/landmark/repository/tesla-fe-snapshots/
    //http://52.53.227.31:8191/landmark/repository/tesla-fe-releases/
    // vi pom.xml and added repository details   
    // vi setting.xml and add AUTHENTICATION details  
   } 
  stage('5deploy2UAT'){
    sh "echo 'deploy to UAT'  "
    deploy adapters: [tomcat8(credentialsId: 'tomcat-credentials', path: '', url: 'http://35.170.249.131:8080/')], contextPath: null, war: 'target/*war'

    }
  stage('6approvalGate'){
    sh "echo 'ready for review' "
    timeout(time:5, unit:'DAYS') {
    input message: 'Application ready for deployment, Please review and approve'
     }

    }

  stage('7deploy2Prod'){
    deploy adapters: [tomcat8(credentialsId: 'tomcat-credentials', path: '', url: 'http://35.170.249.131:8080/')], contextPath: null, war: 'target/*war'

    }
  stage('8emailNotification'){
    emailext body: '''Hi All,

Check Build status.

Landmark Technologies''', recipientProviders: [buildUser(), developers(), upstreamDevelopers(), brokenBuildSuspects(), brokenTestsSuspects(), contributor()], subject: 'build status', to: 'tesla-app@gmail.com'
  }
}


If you clone a projects at work, it comes with 3 things

1. src  which contains the code of the project and the JunitTestCases
2. pom.xml which is the project file
3. jenkinsFile which is what is would use to build and deploy your project by putting it inside JENKINS SCRIPT box

Now if you want to build and deploy application you cloned from your work github repository that comes with src, pom.xml and Jenkinsfile, all you need to is to go to your jenkins server UI and create project name and the click on pipeline, scroll down to PIPELINE, under DEFINISTION, select PIPELINE SCRIPT FROM SCM, under SCM click git, under REPOSITIRY URL put your work githu repo, pout in your work github credentials, under SCRIPT PATH, put jenkinsfile name that you have named it to be. Note, when naming a jenkinsfile, make sure that the word jenkinsfile must be in forefront or written first  before attaching any other word to it. E.G,  jenkinsfile-new, jenkinsfile_tesla etc

Sometimes, the job may be delaying to deploy to production maybe the server is busy, in this case, you can add what is called SLEEP COMMAND so that if you configure it to sleep for maybe 30 seconds and afterwhich it should continue deploy, it will make the server refresh and then it will proceed and complete the deployment process.

You can go to your jenkinsfile that is already in your work Remote repo and edit the file and then go to where DEPLOPY2PRO is and then add the below command

sh "sleep 30"

The above command means it should sleep for 3 seconds and then click BUILD NOW.

In real-time, you are only going to be editing the jenkinsfile that has already be wriiten by your work developer other engineer.

LOOK AT THIS SCENERO

Let say you were told to only test and build the application. What you will do is to go to the jenkinsfile in projectRepo amd the place a multi-line comment /*   */ on other command that you dont want to run. that is 

node{
  stage('1cloneCode'){
    git "https://github.com/LandmakTechnology/maven-web-application"
  }
  stage('2Test&Build'){
    sh "${mavenHome}/bin/mvn clean package"
    //bat "${mavenHome}/bin/mvn clean package" for windows only
  }

  /*

  stage('3codeQuality'){
    sh "${mavenHome}/bin/mvn sonar:sonar"
  }
  stage('4uploadArtifacts'){
    sh "${mavenHome}/bin/mvn deploy"
    //http://52.53.227.31:8191/landmark/repository/tesla-fe-snapshots/
    //http://52.53.227.31:8191/landmark/repository/tesla-fe-releases/
    // vi pom.xml and added repository details   
    // vi setting.xml and add AUTHENTICATION details  
   } 
  stage('5deploy2UAT'){
    sh "echo 'deploy to UAT'  "
    deploy adapters: [tomcat8(credentialsId: 'tomcat-credentials', path: '', url: 'http://35.170.249.131:8080/')], contextPath: null, war: 'target/*war'

    }
  stage('6approvalGate'){
    sh "echo 'ready for review' "
    timeout(time:5, unit:'DAYS') {
    input message: 'Application ready for deployment, Please review and approve'
     }

    }

  stage('7deploy2Prod'){
    deploy adapters: [tomcat8(credentialsId: 'tomcat-credentials', path: '', url: 'http://35.170.249.131:8080/')], contextPath: null, war: 'target/*war'

    }
  stage('8emailNotification'){
    emailext body: '''Hi All,

Check Build status.

Landmark Technologies''', recipientProviders: [buildUser(), developers(), upstreamDevelopers(), brokenBuildSuspects(), brokenTestsSuspects(), contributor()], subject: 'build status', to: 'tesla-app@gmail.com'
  }  

  */

}

and then click commit change. then when you click on BUILD NOW, it will clone and the test-create an artifacts only.

CREATION OF VIEW

This is like a group where you add projects that you have executed, into different group that you created for them for easy identification.

Example; you can do this by clicking on MY VIEW, click the PLUS sign, enter your VIEW NAME like tesla, tick LIST VIEW,  and click on CREATE. then scroll down to JOBS and add all your tesla job or project to that view called tesla that you created by ticking all your tesla jobs, click apply and save, you can create another one and called it fintech and then add all bank jobs or project it the view or group. That will now reduce the amount of jobs that displays. If you are looking for any projects, click on the view name you think your project would be or click ALL to view all your projects.

HOW DO YOU DELETE A VIEW?

Go to the view you want to delete. If you are inside the view, clich on DELETE VIEW

REMEMBER WE INSTALLED MAVEN VIA GLOCAL CONFIGURATON IN JENKINS OR WE INSTALED MAVEN AS A TOOL IN JENKINS

NOTE: When you click on build now in Jenkins and it refused to build and when you check console output and it says "waiting for the next available executor, this means that there is not extra executor that would run the build in Jenkins master, just go to node, click on ibuilt in Node" click on configure, increase the node to the next higher figure. That is, if the executor was 1 before, increase it to 2 because 1 executor can only work for 1 project. 

VIDEO 80

One of my experience in Jenkins is that I have been making use of Jenkins jobs like

Freestyle Jobs
Maveen Jobs
Pipeline Jobs

to automate and manage task like

1. Building, testing and deployment of software
2. database backup
3. infrastructural creation / provisioning
5. system monitoring
6. Infrastructure Configuration mgt
  
Category of projects managed using Jenkins:

    --- software/applications [builds and testing and deployment ]  
    --- systems monitorings  
    --- database backup
    --- Infrastructure creation / provisioning
    --- Infrastructure Configuration mgt

------
Freestyle projects  vs Pipeline projects
==================     ==================

I.Q  Between Frestyle and Pipeline, which one will you recommend for companies to use?

ANSWER:

1. Freestyle jobs lack Infrastructure as a code (IaC) while Pipeline introduces infrastructure as a code (IaC). The (IaC) is called Jenkinsfile.

2. Freestle has limited re-usability, Pipeline has extensive re-usability in another project


WHAT DOES RE-USABLE MEANS IN THIS CONTEXT?

Example, if you have worked on a project called tesla-dev project and you have already written or have a jenkinsfile for the paypal-dev project.
Now, the company is asking you to work on paypal-UAT project.  Since you still have the jenkinsfile for the paypal-dev, you can easily just do some modificatiun on the jenkinsfile and use it to execute for paypal-UAT. That is what is meant by extensive re-usability. easily re-usable. that is

Jenkinsfile for paypal-dev  
The same Jenkinsfile but modified  for paypal-UAT  

IaC= Infrastructure as a code. This means that the whole Jenkins integration (Jenkins-SCM, jenkins-maven, Jenkins-sonarqube, Jenkins-nexus, Jenkins-UAT, Jenkins-ApprovalGate, Jenkins-App.Server(tomcat)) is written as a code in a file. This file is called Jenkinsfile.

FOR A PIPELINE JOB, We have 

1. Scripted Jenkinsfile : This starts with the word node. This is easy to write a scripted Jenkinsfile and to modify it. That is

node{
  stage('1Clone'){}
  stage('2Test+build'){}
  stage('9deploy'){}
  stage('10email'){}
}


Declarative Jenkinsfile = Declarative Jenkinsfile starts with the word pipeline. 	 

pipeline {
	
	pipeline{

agent any 
tools  {}
stages {} 
post   {}
}

To write a declarative jenkinsfile, we need an AGENT, TOOLS, STAGES, POST.  This can be configured as

pipeline{

 agent any   it can be different jenkins slave/agent. at thi stage, let us use 1 aganet
tools {} install different tools like maven, sonarscanner, JDK as tools.found in GLOBAL CONFIGU. 
 stages   {} 	list of stages, just like it was in scripted jenkinsfile.
 post     {}  message notification about success or failure

}



Under stages, we have stage and steps

ILLUSTRATION


pipeline{

agent any

tool{maven "maven3.9.2" } 
 stages {
 stage('1GetCode'){
 steps {sh "echo 'cloning from the latest version of the application' "
 put the pipeline syntax you generated here
 }
 
 }

 stage('2Test+Build'){
steps{ sh "echo 'run Junit testing'"
 sh "echo 'Junit testing must be passed before creating artifacts'"
 sh "mvn clean package"
 }

}

 stage('3codeQuality'){

steps{
	
	sh "echo'performing CodeQualityAnalysis'"
	sh "mvn sonar:sonar"
}
 }
 stage('4upload2Nexus'g){

steps{
sh "echo 'uploading artifacts to Nexus'"
sh "mvn deploy"

}
 }

 stage('5deploy2Prod'){

steps {

sh "echo'deploy to production'"

put the pipeline script you generated here

 }

 }

}
 post {

 always{ put the ALWAYS message that you edited from SUCCESS. please check the explanation below }
 success{ put the success message that you generated from pipeline syntax. please check the explanation below }
 failure{ put the failure message that you edited from SUCCESS. please check the explanation below }

 } 

 }

HOW TO CONFIGURE A DECLARATIVE PIPELINE

Let us create a pipeline project by creating a name paypal-app, if you scroll down, you will see where it says COPY FROM, that actully mean that you can copy a configuration of another project to reflect in this new project by typing the name of the project that you want to copy on the COPY FROM icon and click ok. 

CLONING CODE
Click on pipeline syntax. Let us clone code from a private repo., type in git in SAMPLE STEPS, put in your private repo url in the REPOSITORY URL, put github details you have before as credentials, hightlight the branch that housed your project in SCM (let us put in feature as the BRANCH. If the project you want to clone in SCM is not in feature branch, create feature branch and all the codes in master will be there automatically), click on GENERATE PIPELINE SYNTAX, copy it and go where 1GetCODE is and paste it in next line. as shown above. 

TEST AND BUILD
You can decide to just clone, Test & bUILD only. You can do this by placing a comment on the command that are not dela with clone, test+build. Copy the configuration and paste in your Pipeline script box, if you click apply and save, click BUILD NOW it, you will see that it will clone, test and build.

CodeQuality
If you want to run CodeQuality, use the configuration direction above and it will run code quality analysis.

UPLOAD TO NEXUS
If you want to deploy to Nexus, also folow the configuratuion above and it will upload artifacts to nexus. It the uploads failed, please launch another nexus server, create release repo and snapshot, copy and paste the url under repo tags, under distribution tag in pom.xml. also go to jenkins server CLI, check if you have more than one version of maven and if you do, make sure that all the maven version is modified by going to settings.xml, vi into it and create a new server tag that has nexus credentials and then save and quit, run build now and it will upload artifacts successfully.

DEPLOY TO PRODCTION
If you want to deploy artifacts to production, scroll down and click on pipeline syntax, select deploy deploy war/ear to container, put target/*war in WAR/EARS FILE, click on ADD A CONTAINER, and highlight the recent version of Tomcat, select tomcat credential in CREDENTIALS, put in your TOMCAT URL, click on GENERATE PIPELINE SCRIPT, copy and paste it in
next line under sh "deploy to production"

CONFIGURATION OF POST
TO configure POST, email notification.click on pipeline syntax, select, extended email, put email address on TO like paypal@gmail.com, click ADD and tick BUILD USERS, DEVELOPERS, put SUCCESS in SUBJECT, BODY: you put the following   

Good job guys, build and deployment is successful, Thank you guys
Landmark Technologies.
+14163127624 

click on GENERATE PIPELINE SCRIPT, copy and paste in the on SUCCESS under POST. 

under FAILED, Just copy and paste the script of SUCCESS into FAILED and then edit it to something like, 

Hey guys, build and deployment failed, please resolve the issue. Thanks
Landmark Technologies.
+14163127624 

under ALWAYS, Just copy and paste the script of SUCCESS into ALWAYS and then edit it to something like, 

Hey guys, Please check build and deploy status. Thanks
Landmark Technologies.
+14163127624 

After you have written the declarative jenkinsfile and you have tested it that it is good to go, try and create a file that will contain this declarative jenkinsfile in your SCM like github by going to your Project repo, under feature branch since it was feature branch that you cloned from. and then create a jenkinsfile like jenkinsfile-declarative-osas and the paste your configuration inside and then commit the changes. Note, the name of then file must start with jenkinsfile. It is case sensitive

Since you clone the code from a feature branch and you deployed to production and saw that it was good. Your team member or a higher engineer will have to review since it is still in the feature or developement branch.

You will create a pull request that says COMPARE AND PULL REQUEST on your feature branch, 
Write on the message like,

new features developed have been tested and working. Please review and act accordingly

and then, add REVIEWERS, you cabn add yourself as one of the reviewers and click on CREATE PULL REQUEST

Since you add yourself as one of the reviewers, you can merger it to master for now but under normal working environment, you are not the one to review. It is usually someone else. 

If you notice that is says CONFLICT and it says it is in pom.xml, click on it, check where the conflict are and resolve it manually by deleting what is not expected to be there asnd leave what is expected to be there. and then click MARKED AS RESOLVED and the commit the change.


Note:If you have cloned the code from development or master branch, make sure you edit the jenkinsfile to be master or development. Take note

You will just create a pull request 

MULTI-BRANCH PIPELINE

This a process of doing a build on more than one branch at once as a result of jenkins scanning through the jenkinsfile to see if there is a new commit as a result of changes made in each of the branches and then trigger a build.

EXAMPLE

Let us create 2 branches from master (dev and stage) having a jenkinsfile, pom.xml and src. Lets the jenkinsfile have the same name and the same content. 

Let us create artifacts only as an example. You can do that by commenting on the other part of the script with a multi-line comment  /*  */

Then, go to your jenkins UI, create a new job and call it paypal-mbp, click on multi-branch pipeline and click OK, put. paypal-mbp on the display name, select git as branch sources, put your github project repo url in git, select your github credential on credential, select jenkinsfile by default, put in your exact jenkinsfileName on script path, click apply and save.  click on SCAN MULTI-BRANCH PIPELINE NOW. You will see that it will bild successfully on each of the branches. If the build fail, it means that there maybe a syntax error in the branches, check the console output to found out where the problem is. Go to the jenikinsfile in each of the branches and then make the correction. Since the branches have the same jenkinsfile contents, you just do the corrction for all the branches.

Note. If there is amy commit in any of the branches in SCM separately, jenkins would trigger a build on that branch.

Multi branch pipeline enables you to deploy new commit in SCM to different branches at once. If the jenkinsfile are named the same. it will

code from dev branch will be deploy to dev environment
code from stage branch will be deploy to UAT environment
code from master branch will be deploy to production environment

MY PERSONAL CONFIGURATION OF A DECLARATIVE JENKINSFILE

	pipeline{

agent any 
tools  {}
stages {} 
post   {}
}

create a paypal project in jenkins UI

 declarative Jenkinsfile

    pipeline{

  agent any
  tools{ maven "maven3.9.2"}
  stages{

  stage("1GetCode"){
  steps{ 
 
 sh "echo 'clone the latest version of the application in Scm'"
 git branch: 'dev', credentialsId: 'cliffosa22', url: 'https://github.com/CLIFFOSA22/tesla-private-repo'

}

  }

 stage("2Test+Buil"){

steps{
	
sh "echo 'running a unit test cases'"
sh "echo 'code must be passed before creating artifact'"
sh "mvn clean package"
}
 }

stage("3CodeQuality"){
	
steps{

sh "echo 'running code quality analysis'"

sh "mvn sonar:sonar"

}

}

stage("4UploadArtifact2Nexus"){
	
steps{

sh "echo 'uploading artifact to nexus'"	

sh "mvn deploy"

}
}

stage("5DeployArtifact2Prod"){
	
steps{

sh "echo 'deploying artifact to production'"	

deploy adapters: [tomcat9(credentialsId: 'osas', path: '', url: 'http://3.15.159.165:8080/')], contextPath: null, war: 'target/*war'
	

}
}
}
  post{

always{
	
emailext body: '''Hey, check build status. Thank you
Landmark Technology
+4163127624''', subject: 'always', to: 'cliffeg1@gmail.com'

}

  success{

  emailext body: '''Hey, build and deploy was carried out successfully. Thank you guys
Landmark Technology
+4163127624''', subject: 'successful', to: 'cliffeg1@gmail.com'

  }
    
failure{
	
emailext body: '''Hey, build and deploy was not successfully. Please resolve issue
Landmark Technology
+4163127624''', subject: 'failure', to: 'cliffeg1@gmail.com'
}
}
}


ARTIFCATS WAS BUILD AND DEPLOYED SUCCESSFULLY. 

Let me save the declarative jenkinsfile in my project repo, under dev branch.



NOTE: My tomcat user credential is (username=cliffosa, pwd= admin123) 


VIDEO 83

DevOps = This is a combination of Developing or writing a software and also its Operation that goes with it

1. Developers are expected to Write the codes or build the softwares

2. Operations Team Involves the below:

      System Admins ; This team are responsible in creating servers, creating SCM etc 

      QA -- Testing:  This are responsible in doing the software testing. 

      Middleware admins; They are task for deploying the application  

      Security; Securing our build and deploy architecture

Jenkins is a CI/CD automation tool. Automation can be achieve in Jenkins with the application of CI/CD configuration
=============================  

We can achieve a CI/CD job via Pipelines or Freestyle jenkins configuration  

CI or Upstream job commence from when there is a code-commit in the SCM by developers, Jenkins will clone the code with the help of Jenkins-Scm integration, run a unit-tesing and creating artifacts when integrated with a build tool or server like maven, jenkins will run a codequality analysis with sonarqube or sonarcloud, jenkins with upload artifacts to artifactory tool like nexus via jenkins-nexus integration. This is where the CI OR UPSTREAM jobs end.

And the CD or dowstream job begins deploying the artifacts to UAT if, it is CONTINUOUS DELIVERY where the code would be tested by the customers and others involved and then deploy to the ApprovalGate where the application would require a manual approval. This is called CONTINUOUS DELIVERY. and if approved, would be deployed to production

In the case where the approval is an auto-approval, It will deploy code right from the SCM directly to production. This is called CONTINUOS DEPLOYMENT via jenkins-production or release or application server.

Since Jenkins is an automation tool, therefore

Managing and Securing the Jenkins servers is very important  
=========================================================

JENKINS SECURITY

Lets assume you work in a Team of 15 Engineers like

Simon, Chidi,  dominion,  mercy, joy, Pius, peter  ect, 


   DevOps Engin. team        = simon   chidi  
   QA   team                 = dominion  mercy  
   Developers team           =  joy Pius peter  


you have to be able to manage the Engineers in such a way that their roles and persmissions using Jenkins server defer. Everybody can not just have full permission using the jenkins server even though you are in the same team working for the same company.  

Jenkins server still neeed to be secured from those that are part of the team and those that are not part of our team.

FOR US TO SECURE JENKINS SERVER, WE NEED TO DO THE BELOW

1. create users in Jenkins with strong password policy. if possible, use token instead of password 

2. RBAC  (Role Based Access Control): This means Assign roles to the users. 

RBAC can be installed as plugin in jenkin server. To install RBAC, go to manage jenkins, click plugin, click available, type in ROLE BASED on the search and tick on ROLE BASED AUTHORIZARION STRATEGY and click install without restart. 

A team member can be granted permission to only view what is happening or going inside the jenkins server. This are called OBSERVERS. They dont have write or execution access but they only have read.

3. disable anonymous access.   don not allow annanymous access because if you do, people in the internet can access ur jenkins server


4. multi factor authentication.
    
Basically, the main thing for Jenkins security are

 a. Authentication:   

 We use usernamne and password or a token for authentication.

  Credential management:

  You can create credential that can permit jenkins to connect with SCM to execute a clone and a poll-SCM, credentials for jenkins to connect with ansible, tomcat, kubernetes, duncaHub, agent etc

  Let us create Credentials that will permit images to be able to upload to duncaHub
You can create the ceredentials by clicking on manage jenkins, UNDER SECURITY, click on Credentials, scroll down and click GLOBAL under Domain and then click ADD CREDENTIALS, you can select username with password under KIND, select GLOBAL under SCOPE, put in your username, put in your password, put duncaHub-credentials as your ID, and the click CREATE

All that we have done above is that we are using user's info stored in the Jenkins database to be able to access Jenkins server. 

We can also use what is called LDAP integration to be able to access jenkins server

LDAP CREDENTIAL

When you install an LDAP software into a server like rehat server. It becomes a LDAP server.

EXAMPLE

Lest say you have 50 users in your server, it means that you need 50 authentication like username and password or ssh-key or token.

If you have different servers like JENKINS, KUBERNETES, TOMCAT, NEXUS SONARQUBE, DUNCA and you need to create user account for authentication for each of these servers for members of your team. Since we have 50 users, it means you will create 50 user account for each server. That is, 50 user account for tomcat, 50 user account for dunca, 50 user account for Jenkins, 50 user account for kubernetes, 50 user account for nexus, 50 user account for sonarqube etc. This task would be too tideous and cumbersome. 

If we have up to about 200 servers in our environment, it means that you have to create 50 user account for each of the  200 servers. too much task.

Instead of you creating user account for each of the server, you can just create authentication account in the LDAP server. All we need to do is to integrate or connect the LDAP server with all the servers that we have in our environment. 

You can also activate RBAC (Role based access control) on the users that are in LDAP to grant the authorization on the limit of what they can do with the servers they are accessing. T

The role/permission of users can be achieved by placing all the users into groups like Group of developers, group of DevOps engin., QA team etc and now you can easily assign role on what each can do while accessing servers or synchronising with servers via LDAP

When you join a company, there is what it called ONBOARDING. During onboarding, New Enginerrs are being introduced into different evironments 


b. Authorizaation. 

This is a process where permission maybe and may bot be given to team members to perform a certain taks.

Example;

Let us create users

Let us go to our Jenkins Server. click on dashboard, click on manage jenkins, you will see security, click on manage users, click on USERS, click on  CREATE USERS, put in the user credentials u want to create. Users in various teams like Simon, dominion, Chidi,  dominion,  mercy, joy, Pius, peter.

Let us find out what each of the users can do

Go to Configure Global Security by clicking manage jenkinsYou can grant users access to be able to signup by themselves by you ticking ALLOW USER TO SIGN UP and then click APPLY and save. Copy the url and then send it to the team member for signup. Once they have created the account, you can go back to GLOBAL SECURITY and uncheck or unmark the ALLOW USER TO SUGNUP. If a new user try to use the link to create another account, it is not possible any longer because the access has been disabled

By default, any new users in Jenkins server have full permissions to do anything. 

For example, If you sign any of the user you created like dominion, you will notice that under AUTORIZATION, What is selected by default is LOGGED IN USER CAN DO ANYTHING.

You would have to remove dominion from that permission and select MATRIX BASED SECURITY OR PROJECT BASED MATRIX AUTHORIZATION STRATEGY

MATRIX BASED SECURITY

If you select matrix based security, if you log in with any new user you created, you will see that you no longer have any other permisssion on the server rather than to just log in.

let us grant permission to users according to what is expoect of them to do with the server by 

clicking on add USER, put username of the user that you want to restrict some access from, 

Let us first of all grant The first-user/osas-devops) by ticking the second to the last box to grant full permissions and click save

let add a user like simon and grant him some access like, under OVERALL, tick READ, Under CREDENTIAL AND AGENT you can refuse to grant him access because you would be expsoing your credentials and agent, under JOB, let us grant him READ access by ticking read, under VIEW, Let us also grant him by ticking the READ box. This will make simon to be able to read only. click save. After you have done that, you will notice that simon no longer have access denied. he can be able to read the jenkins server now

PROJECT BASED MATRIX AUTHORIZATION STRATEGY


This a process whereby some particular members of a team are permitted to be managing some Projects. This permssion is project based.

Let assume we have the following projects

Tesla  boa  wellsfargo   []  
dev, QA, Production,   access

Using the team as an example, we can use RBAC to assign role to various team on what they can do.
Example

 DevOps Engin. team        = simon, chidi  will be responsible in deploying the application 

 QA   team               = dominion, mercy will test and market the application to customers 

Developers team         = Joy, Pius, peter will write the application and commit it to SCM


Let assign project based security to some members our team. Click on manage jenkins, under SECURITY, click on SECURITY, select project based matrix authorization strategy under AUTHORIZATION and then once again grant full access to THE ORIGINAL USER called osas-devops by clicking the second to the last box (thats say grant full permission) and then click save

Go to any of your project, click configuration, if your scroll that, you will notice that project based matrix authorization strategy is now seen and so you can tick it. When you tick, it will show you where you can grant access to users by adding users or group. 

Let us add simon on this category since he is a DevOps engin. let us grant him full permission on this particular project. and also grant the first user full permission 

Let us also grant Mercy read permission since she is part of the QA team. under OVERALL tick read, tick VIEW under CREDENTIALS, under VIEW , tick read AND UPDATE access in this particular project. Go to manage Jenkins and also grant simon, first user and Mercy the same permission so that everything would be fully authenticated.

Go to the project while logged in as simon and you will see that you have full permision here, you can do a build, configure, do everhting but if simon go to another project, he would not see where to build nor configure. If you log in with mercy credetials, you will see that mercy can only read and view all the projects but can not write or execute any task.

Let us go back to our SECURITY, click security and select MATRIX BASED SECURITY and add simon and mercy clicking ADD and then grant simon. mercy and osas-devops full access so that they can be able to do anything whenever i logged in with their credentials.

5. Changing the default configurations 

  Jenkins default configuration are:

a. Default Port = 8080  you can change it to another port

HOW CAN YOU CHANGE THE JENKINS PORT

 google, changing the jenkins port in redhat servers.

 The below is the path to the file where you can change jenkins port to another port

    /etc/sysconfig/jenkins  --- in redhat server
    /etc/default/jenkins.   --- in ubuntu server 

 If you vi into /etc/sysconfig/jenkins by running

 sudo vi /etc/sysconfig/jenkins

You will notice that on the file, it say jenkins dedualt port number is 8080, JHD is 

/var/lib/jenkins    and  Jenkins Default user is jenkins

This means that you can change the default port number, default jenkins username and default JHD to another thing

CHANGE JENKINS PORT

Scroll down and go to where it says JENKIN_PORTS and change it to maby 1980. After you have change the port and you want to access jenkins with the new port number, it will not work

Note; Anytime you change any default configuration in Jenkins, you need to stop jenkins and then start jenkins again. By running

sudo systemctl stop jenkins
sudo systemctl start jenkins

netstat -utlpn is used to check what port number your server is listed with using CLI by running

sudo netstat -utlpn

If you run the above command you says CANNOT BE FOUND, you need to install net-tools in your server by running

sodu yum install net-tools -y



b. = JHD (Jenkins Home Directory)  = /var/lib/jenkins 
        JENKINS_HOME="/var/lib/jenkins"  you can change it to another home directory

        JENKINS_USER="jenkins"  you can change jenkins user to another name if you so desired


6. Accessing Jenkins via a proxy  

You can access jenkins via a webserver  or Load Balancer like Ngynx webserver or apache webserver just as users can access tomcat app. server via a webserver like Ngynx

       users ---> LB/WebServer [] ----> tomcat/applications  
       users ---> LB/WebServer [] ----> Jenkins 



JENKINS SHARED LIBARIES

What do you understand by Jenkins shared Libaries?

 30 java projects  
 Jenkinsfile is required for each projects

Scripted Jenkinsfile

In a situation whereby your company have been task to work on 30 java projects. In this 30 projects, it would require 30 jenkinsfile to be written for the pipeline job of the 30 projects. that is

node{
    def mavenHome = tool name: 'maven3.8.6'
    stage('1cloning'){}
    stage('2testing'){}
    stage('3Build'){}
    stage('4CodeQuality'){}
    stage('5uploadNexus'){}
    stage('6deploy2uat'){} 
    stage('7approval'){}
    stage('8deploy2prod'){}
    stage('9notification'){}
}

libraries:
   ci = test, build, CodeQualityAnalysis  
   cd = deploy to UAT, deploy to approvalGate, deploy to app. server


This means that each projects would undergo, test, build, codeQuality, upload to artifactory, deploy to UAT, deploy to approvalGate, deploy to app. server. This would be tideous somehow. This is where jenkins shared libary would come in.

Instead of written a script of containing CI and CD on the jenkinsfile for each projects, a libary can be created that would have the CI and CD script saved on it so that anytime you want to run a jenkins pipeline job for a java project, you can just invoke the libary where the CI and CD script is, to save you from witten the script over again for each project.

Jenkins Shared library is the concept of having a common pipeline code in the version control system that can be used by any number of pipelines just by referencing it. In fact, multiple teams can use the same library for their pipelines.

HOW TO CONFIGURE JENKINSFILE SHARED LIBARIES is in my DEVOPS INTELECTUAL MATERIAL. You can go through it if you need it. Thanks

Jenkins questions and answers  = 

JENKIN INTERVIEW QUESTIONS IS ALSO IN MY DEVOPS INTELECTUAL MATERIAL FILE IN MY DESKTOP

I now have 3 accounts in Jenkins. osas-devops, mercy amd simon. They same password admin123
 
=========================================
1. Following videos=
=====================
    1. Jenkins shared libraries -40 
    2. Jenkins questions and answers  
    3. New Relic {40mins} 
    4. Resume Lines [50minutes ]  

JOBS

Build and Release Engineer [ 130k + 20 = 150k]
+ aws [21 hours + AWS Solution Architect] 
Cloud Engineer  
+ Terraform + Ansible  
Infrastructure Engineer
Platform Engineer
+ Docker + Kubernetes    
Sr. DevOps Engineer / Kubernetes Engineers     
aws: 
    Thursday next week  
Opportunity cost = 300k plus 
why:
    sleep 
    visiting   
    parties 
    doing wrong stuff  ===== 

Landmark Metropolitan University  :
==================================
Certifications in: aws, kubernetes, Jenkins  
BSc.    
100 verification :
  AD, HND/HPD, Bachelors and Master   

Jobs:
=======
DevOps Engineer,
DevSecOps Engineer
Cloud Engineer and/or
AWS Cloud Engineer,
Platform Engineer
Infrastructure Engineer,
Kubernetes Engineer or
Sr. IT Recruiter,
Build and Release Engineer
SRE
Application Engineer   
System Integrator  
Automation engineer
DevOps Manager 
Scrum Master    
+ Java, Python 
software engineer [


*We shall revise this Jenkins PROJECT, Interview Questions and more during our Saturday  18/09/2021 class*

        *NB: Saturday class shall be projects and testimonies*

1. What is Jenkins executables?

  A Jenkins executor is one of the basic building blocks which allow a build to run on a node/agent (e.g. build server).

A Jenkins executables are one of the building blocks that enables a build to take place in Jenkins slave or agent in a node with the exception of jenkins master. 

The reason why Jenkins should also build from its agent is to avoid one signle failure and to also save Jenkins space-resource.

FOR EXAMPLE

There is a place where it is said,

Restrict where this project can be built

This means that you can decide to restrict jenkins from building from master by default but to build from slave or agent menaisng building from another server. Jenkins executable means it possible for the build to be possible in another server called agent or slave.

NOTE: 

If you forget your Jenkins password using Jenkins UI. You can actually log in as an ananymous user so that you can log in without user name.
 You can go to your jenkins CL, cd into JHD which is  /var/lib/jenkins and then sudo vi config,xml file. This is

sudo vi /var/lib/jenkins/config.xml

scroll down to Where it says, UseSecurity. change TRUE to FALSE.

Then restart jenkins by running

sudo systemctl restart jenkins

Then copy the jenkins server ipaddress:PORTNUMBER  and paste into the internet browser. It will log in as an ananymous user

2. Explain the difference betweem freestyle and pipeline projects?

Answer: The freestyle project was actually one of the project that was first used to do a build when jenkins was first developed.

In landmark, we use both freestyle project and pipeline project. We use freestyle project to execute projects that does not require a the creation of artifacts, project like database backup, monitory of server, provisioning of server (we will see this in terraform) etc.

Whereas, we use pipeline project to execute our CI/CD projects that has to do with a build and deploy of artifacts, by integrating jenkins with maven to do a unit-testing and create artifacts, integrating jenkins with sonarqube to run a codeQuality analysis, integrating jenkins with nexus to uploads artifacts to nexus and also integrate jenkins with the app. server to deploy jenkins to production. All this integrations can be written in a jenkins script called jenkinsfile.

The major difference is that freestyle project is running a project infracture as a service (IaaS). This means that all the integration is carried out as a service. There is no script or code that that has all the different aspect of the integrations put together but Pipleline project, the projects infracture is run as a code. IaaC

With a pipeline projects, jenkins can decide to build from a node or slave server or agent which can be possible with the help of jenkins executables but with freestyle projects, jenkins cannot build from a jenkins slave server or agent.

Using a pipeline projects, the jenkinsfile can be re-usable for another similar project

OTHER ANSWERS.
The freestyle project can enables you to connect or integrate jenkins to scm and a build tool to do a build.

You can also use a freestyle project to build on anything that not related to software development life cycle (SDLC) builds. That is 

non-SDLC   

An example is when you want to do a database backup. That does not require a build.
monitoring your server, provisioning etc.

PIPELINE projects are used for complex projects. It has to do with infrastructure as a code (IaC). Pipeline project was formally called WORKFLOWS.

This IaC means that you have the whole jenkins integration from application cloning to application deploy in a single script or file called jenkinsfile there making the build server to be installed in jenkins server as a plugin or a tool

The Jenkins file can be re-usable because if you have another similar project that you want to commence like when you want to update or upgrade your application, it is very easy to do that by just modifying your jenkinsfile and a build and deployment would be triggered

3. Is it adviseable to deploy code from the feature branch in production?

Answer: No, it is not recommended to deploy code from feature branch to production because in landmark, we maintain a minimum of 3 branches.

dev branch  code from dev is deploy to dev env
stage      code from stage is deploy to stage/UAT/testing env
master   code from master is deploy to production

You can only deploy code from master branch to production.

feature branch are mainly created from master and when a developer has done the neccessary modification on the code in the feature branch, the code need to be tested and reviewed. That is why the developer must create a pull request of the code in feature branch and select reviewers to add team members/member to review the code and test the code and it is the team member that can now merge the code to master and it in master branch that the code would be deployed to production. that is what is recommeneded. end of anser.

NOTE; The same thing applies to bug-fixed branch

4. Explain Jenkins intergration with GitHub?

Answer: Thank you very much for that question, For jenkins to be able to communicate and work together with github for a clone or a poll SCM to be possible. This can be achieved in tw ways

Authorization and Authentication or using github webhook to connect with jenkins.

1. Authorization and authentication. by going to the project repo in jenkins, click on configure, under source code repo, select git and paste the url of the private project repositiroy in github and then it will ask you for authentication since it is a private repo that you want to clone. If it is public, it will not ask you for authentication but our environment, we clone from a private repo.

authentication has to do with credentials. This credentials is either username and password or username and token. In this case, token is what jenkins recommened. you would have to create a user name and token that would connect your jenkins project repo with your github repository.
click on credentials in jenkins, put any user name that you want and then go to your github repo to create PAT and the copy and paste it on the password section of the username that you have chosen and click save.

Note;

 USING PASSWORD, access is difficult to manage. This means that you can not control or manage what the user can actually do on the server

USING TOKEN, access can be defined. This means that using token, permission to what the user can do will be defined or know during the course of creating or generating the token and you can also modify the permisions on the token after creation if need be.

B. Using github-webhook to connect with jenkin. Go to the private repo of your project and click on settings, click on webhook and the click on create webhook, put in your jenkins url/github-webhook  and then click on create webhook. This is means that you have integrated your jenkins server with the project repo in github. If you want to build the project in jenkins, click on confiogure and then click on build trigger, select github hook and save. So anytime there is a commit of that project repo src in github, a build will be triggered.

5. Walk-me through the steps you use to install and/or updates plugins in Jenkins?

answer: Thank you very much for that question. In landmark, when a new plugin is to be installed or old plugin to be updated,  what you will do is to 

a. Create a change request and state the purpose of the request. The purpose here is to install or update plugins. and if the requested is approved. then

b. you can log in to to your jenkins server, click on the dashboard, click on manage jenkins, click on manage plugin, if you want to install, click on available. you can type in what you want to install on the search button, tick the plugin you want and then click install without a restart or install after a restart

Note; you can install safe restart pluins in your jenkins server and then anytim you want to restart your jenkins server, you can click on it and your jenkins server would be restart.

6. Why do we need an artifactory in the Jenkins build process?

Answer: We need Artifactory in our Jenkins Build process should to backup our built arifacts should incase any goes wrong with our build tool, we can easily retrieve our artifacts from the Artifactory. In our environment, we use Nexus as our artifactory. 

7. Explain the difference between remote repositories and local repositories?

Answer: Thank you very much for that question

Local repository is usually created in the developers local environment while Remote repository is created in an external environment. 

For example in landmark, We use git/github as our versioning tool. So once developers has finished writing the code, they will commit the code to their local repository using the command

git commit -m (commit message)

It will commit the code containing the project file called pom.xml and the src to his local repository. If your run git log, the details of the committer and the commit ID will be seen.

The developer will go ahead to push the code to the project remote repository in github by running the command

git push aliasname stage

So Remote repository is an external environment called source code management in which versioning code are housed for further deployment. We use github as our SCM remote repository.

Also In landmark, we also use maven as our build tool. So when an artifacts is created, is it usually created in the project target directory by default when you run

mvn package

but when you run 

mvn install

It will create artifacts in the project target directory and also store the artifacts in maven local repository by derfault.    maven local repository is 

.m2/repository

if your run

mvn deploy

It will create artifacts in the project target directory, store the artifacts maven local repository also upload artifacts to remote repository called artifactory. The artifdactory that we use landmark is Nexus

8. what is sudo in Linux administration?

This is an elevated privileges that a user uses to be able to run a command as a root user or to run an admin task

With sudo privileges, you can be able to run bin and sbin files.

9. Tell me about the project you are currently managing?

Answer: thank you for this question. In landmark, we are currently managing a java based project. We are working with a team of developers. Those devel;opers are continuosly doing development to acheive what is called continuous development. We have integrated our development environment in such a way that we use jenkins to automate the entire environment. We have integrated our github with our jenkins server via authorization and authentication and the by github-webhook so that when ever developers commit any new version code to github repository, github will talk to jenkins and jenkins will pull the code and a build would be triggered in jenkins and an artifacts would be created and deployed

10. What are the syntax in ansible playbook? 



JENKINS Questions (JENKINS project)

1. what is jenkins?

Jenkins is an open source CI/CD automation tool witten in java.

CI= continuous integration which involves jenkin conects with, github, maven, sobarqube, nexus to be able to pull code from git, test and build on the code to create artifacts, runb a code quality analysis and the upload artifacts to nexus. This is also called the upstream job in jenkins.

CD= Thiw when the artifacts craeted is being deploy to the application server without a manual approval. If there is a manual approval. it is cllaed CONTINUOUS DELIVERY which usually has an approvalGate. 

2. what are the plugins you have used in Jenkins?

Answer: In our environment, we have use different plugin that will extend the functionality of our jenkins server. We have used plugins like

frestyle project= to execute simply project like database base backup, system monitoring, server provisioning

pipeline= to executing complex projects by the use of a jenkinsfile to run the entire infracture as a code

safe restart=  to restart your jenkins servesafely but if jobs are running on the server, it would not restart

maven project

node=   This is a jenkins agent or slave in which jenkins can build from. master is the default agent that is in jenkins itself, you can create other agents in another server and integrate i with jenkins.

job import= to copy jobs from one jenkins server to another jenkins server. You can use this to copy jobs from dev to stage environment

sonarqube scanner

maven integration

Schedule build; This is used to schedule a build to take place in a particular day.

artifactory plugin; This helps us to manage our build artifacts and move them into our artifactorus

publish over ssh

blue ocean:  This gives us a better view of jenkins server in order to see how work is being done

deploy to container: WHICH IS USED TO INTEGRATE jenkins to tomcat or jboss
jacoco
ssh agent     etc


3. How can you migrate jenkins to a new and upgraded server?

Answer: tHANK YOU VERY KMUCH FOR THIS QUESTION. This question just fall inline with what I was tasked to do recently. I was tasked to move jenkins server from a virtual machine(VM) to a container. What I used in achieving this is the use of JOB IMPORT plugin. This will be able to move your jobs in the jenkins server to container

4. Which plugins have you used for sonarqube and Code Coverage?

Answer; unfortunately, we dont use plugins for code quality analysis but how ever, we have a full sonarqube server that we use for CodeQuality analysis when we integrate sonarqube with a build server and so when you run

mvn sona:sonar

The first sonar represent the plugin that does scanning of the codes while the second sonar represent the goal to execute the codeQuality analysis.

Occassionally, we have also used a plugin called JACOCO which means java code coverage

5. what is the pre-requisites for Jenkins and Jenkins slaves?

Answer: 

     A. The pre-requisite for jenkins master is java. Java must be installed before jenkins master to function or start

     B.  JAVA MUST BE INSTALLED in the jenkins agent before jenkins excutables can run

  For example, in our environment, a junior engineer was trying to build from jenkins master and an error message showed saying, java not installed on the jenkins agent. It is very imperative that both jenkins master and jenkins slave must have java running because jenkins is written in java and without java, jenkins executables will not work for jenkins aganet to function.

6. Explain how to add slaves nodes to a jenkins master?

Answer: To add slave node to a jenkins server is by using what is called
ssh agent plugin

With the help of this ssh agent, we created credentials using using username and password or username and sshprivatekey  and then do other configuration like to decide if you want that slave server to be running by select ALLOW ONLINE AS MUCH AS POSSIBLE and so on 

7. Can I use different OS as nodes and master?

Answer: yOU CAN INSTALL JENKINS MASTER and jenkins node on any Operating system (OS) like

linux, mac OD. windows, 

8. What are different ways to create jobs in jenkins?

Answer: The different ways to create jobs in jenkins are

frestylee jon
pipeline jobs
maven jobs
multi-branch jobs
multi-pipeline jobs

9. What are different types of pipeline scripts?

10. what is the difference between scripted and Declarative pipeline?

Answer: With scripted pipeline script, the script or jenkinsfile is written with node being sighted at the beginning and contain stage 

but with declarative pipeline, pipeline is sighted at the beginning of the script and stages are broken down into multiple steps. 

11. what are 3 important stages in a pipeline job?

ANSWER; The 3 important stages in the pipleline jobs are 


BUILD , TEST, DEPLOY

12. what are the difference stages of a Jenkins pipeline job?

Answer. The different stages of a jenkins pipeline jobs are

CodeClone, Test+Build, CodeQuality, Upload2Nexus, Deploy2UAT, Deploy2ApprovalGate, Deploy2Prod , emailNotification

or

CodeClone, Test+Build, CodeQuality, Upload2Nexus, Deploy2Tomcat or deploy2Jboss or deploy2kunernetes or deploy2DuncaContainer or deploy2Ansible , emailNotification

13. what is the last stage in a pipeline?

Answer; THE LAST STAGE iN PIPELINE IS EMAIL NOTIFICATION

14. How can you select a custom Jenkinsfile which is in the project repository?

15. Explain how to configure nexus username and password?

Answer; yOPU CAN CONFIGURE your nexus credentaisl in jenkins by going to your jenkins server using command line, sudo switch directory to jenkins, cd into tools, cd into hudsonMavenInstallation, cd into the version of maven you extracted. cd into conf directory, vi into settings.xml , go to the server tag and if you did not see any nexus credentials, you create a new server tag by putting Nexus as the ID, admin as the username by default and admin123 as the password by default


NOTE: You will not found tools directory in jenkins if an artifacts has not been created successfully with that jenkins server.  Please take note


16. Explain how you configure kubernetes and Docker in pipeline script?

17. Where can I configure tools in Jenkins GUI?

Answer: yOU CAN CONFIGURE TOOLS using Jenkins GUI by logging into your jenkins GUI with your jenkins username and password/token and go to dashboard, click manage jenkins, click on system configuration, click on plugim, click on available, and type in or click on the tools that you want to install and then click install with a restart or install after a restart. If you install after a restart. then go to your dashboard and the click on safe restart so that your jenkins server can restart safely and if you have any priject running, the restart would not be possible because the plugin would not want to obstruct and stop the project that is running.

18. How you will check whether build is success or not?

Answer: You can check whether a build is success or not by going to going to the workspace of that project and check if an artifacts was created or not. if the build was successful, an artifacts would be created. Also check the console output of that build to check if the whole architecture of that build was successful or not. With console, you can be able to detect the particular place where the pipeline or workflows had an issue and find a way to fix the problem 

19. Explain how you have configure a job successfully to trigger another one? 

Answer:

20. How can you stop particular stage in a pipeline job from being executed?

Answer; Thank you very much for that question. You can stop a particular stage from running by placing a multiline comment on that stage. That is the stage will start will from forward slash-ycap and end with ycap-forward slash  /* */

21. what are the different ways to automate jobs?

Answer: You can automate jobs via jenkins CI/CD by using 

1. jenkins-SCM integration like jenkins-github-webhook in a way that whenever there is a commit in the project repository in github, a build would be triggered in jenkins server

2. Using jenkins poll-SCM. This is a process where jenkins is configured in a way that it scan through the project repository in github in a spectific time to see if there is any commit in so that a build can be triggered. Poll SCM is associated with time or period

3. Build periodically. This is another build that is associated with time. It is CI/CD is cobfigured in a way that a build will be triggered at a specifc time. For example. Whwn you want a job to continue to build every minute or hour or months. You can use the method. This build method is used for system or database backup. This is the type that whatsapp uses to run a data backup every 2.am in the morning.


22. Any plugin to migrate jobs from one server to another?

AnsWER: The plugin that can be used to migrate jobs from one server to another is called JOB INPUT plugin

23. How many master and slave machine are in your CI-CD projects?

Answer; We have one master and 3 jenkins slave server in our jenkins environment

24. Explain Jenkins master and slave architecture?

Answer:  Jenkins master is the main default jenkins server node that controls jenkins slave while the jenkins slave is an external server run in java in which jenkins master can build from, with the help of jenkins executables. The jenkins slave may not have any build server installed with it but a build will take place because it is being controlled by jenkins and jenkins have a build server like maven installed in it. The pre-requisit of a jenkins slave is java. so java must be installed in jenkins slave server for a builod so happen.

The reason why jenkins can build from it slave is a situation where a code is expected to run a multiple builds at once. For this to be possible, jenkins would have to build from slave so that can build from both jenkins master and jenkins slaves depending on the number of builds that is required.


VIDEO 89  AWS-1   (AMAZON WEB SERVICES). LITEARTURE REVIEW

What is the task of DevOps engineers?

We do the follwoing

2. Test
3. Build
4. Monitor
5. Secure
6. Backup
7. Deploy 

This is what a DevOps engineer is involved in.


1.Develop: This is coding. Using a platform like Linux or windows to write their codes. If you can write codes very well using linux. You can work as a platfdorm engineer specialize in linux

We need a platform to be able to manage the above tasks. The different platform we have are:

Platforms:   Platform Engineer ( Linux. Specializes in linux )

             Multi- Platform Engineer (specializes on different platform like Linux/WINDOWS/Solaris/mcOS etc)

Platform Engineer - Linux understands how to run tasks on Linux OS/Platform.

Platform Engineer - WINDOWS understands how to run tasks on Windows OS/Platform.

Multi- Platform Engineer understands how to run tasks on Multi- OS/Platforms  

In our environment (landmark), Jira is used for Ticketing and bug tracking:

We work in an agile environment.
Ticket: Install and configure Jenkins Server??? 10wd 


The paltform that we mostly use in our environment is in the percentage below
        --- Linux OS [70%]  70% of our task runs on linux platform
        --- WINDOWS  [20%]  20% of our task runs on windows platform
            others    [10%]  10% of our task runs on other platforms like MaC, CentOS

You May have joined a new company called Tesla and you are told or tickeft issued to your to install jenkins in windows and solaries.

Tesla: Install Jenkins on Windows & Solaris Platforms to run 
develop, build, test, monitor, secure, 
    backup and deploy applications
  development:

DEVELOPMENT: What is required for developers to commencing developing?

Answer; 

1. they required  an integrated development environment IDEs like VScode, Eclipse, Pycharm for coding or for deloping code  ( CODING === IDEs [vscode, Eclipse], [Pycharm, ). 

IDE is an environment that developers used in writing their code which makes it more simplified and easy. A developer must have IDE installed for coding.  

An example of IDEs are

Vscode, Eclipse, [Pycharm, etc

Developers uses IDEs like Vscode, pycharm, eclipse to writing their code.

Eclipse is very good for java based project

Note: IDEs can run on any platforms like windows, linux, mac, centOS, solaris etc

2. They need versioning  tool like git, svn,   
3. They need a source code management SCM  like github, gitlab, bitBucket
 
The versioning tool and the SCM are integrated with IDE for developers to writing and committing their code successfully

TESTING === You can use Selenium for testing code. 

SELENIUM is a tool that is used for test automation. SELENIUM can be installed.

BUILD: You can use MAVEN/Gradle/ANt to build packages or create artifacts. Maven, Gradle, ANt supports java projects.

UNIT-TESTING: You can use JUNit for unit testing

SONARQUBE: Use sonarqube for CodeQuality Analysis. What sonarqube does for codeQuality is that Sonarqueb would be checking for standards or benchmark against actual results. Sonarqube would check for the result and compare it with what was set as a standard in sonarqube and if the tesr result is inline to the standards that was set, then codes would be passed.

Back-up artifacts: In our environment, we are using Sonatype Nexus as our back-up artifactory

Some other comapnies uses JFrog for backup but we use  Sonatype Nexus as our back-up artifactory

JENKINS: We use Jenkins as our CI/CD automation tool in our environment

TOMCAT: We uses tomcat as our application server in our environment. Application server is where artifacts is being deploy to as soon as the artifacts has been backup by an artifactory(Nexus)

Users can not directly access tomcat where application has been deployed to. 

Users can only access tomcat via a web server like Nginx webserver, apache webserver


Users ----> webserver (nginx: users ) ---> nginx1/nginx2 ---> tomcat ----> database 

 Nginx will receive users and then route the traffic to the different application server available. 

Nginx webserver acts as a proxy to wage cyber attacks by hackers where they can not have direct access to the app. server. 

Nginx webserver also acts as a load balancer by routing users traffic to app servers in a very balance way whereby some of the app. server would not be overloaded.  Once the traffic has been well routed to the app. servers, app. servers would now communicate with the database to produce the result of what the user has requested for.

How do we know if our tool or server is working?

Answer: We need to deploy a tool called server monitory or application performance monitory tool(APM). This is where newRelic comes in place.

Server monitoring / Applications performance monitoring: NewRelic 

NewRelic is a tool that is deploy to check the performance of our server. It monitors our server.

We can use NewRelic for our APM so that once any is wrong with our server, we are going to be notified.

What was your experience with an open source?

My experience with an open source are

1.  The use of Linux

Linux is a secured Open source OS:

Linux has various distributors like

      Redhat -- Redhat (8,6,7)
      CentOS -- 
      UBUNTU
      Alpine 
      amazon LINUX 2/1

Managing Linux OS or running tasks on Linux Platforms: 

If you want to run any task on linux OS, there is what we call RUN LEVELS. We have 

  Run levels ---- 1, 2, 3, 4, 5, 6

 There are 6 run levels but we are going to dwell on the most popular level which is run level 5. run level 5 has multiple users


 Run level 5 is what we call multi-users

 Run level 5 Multi user:  

 This means that multiple users can log into a linux server and perform task via. 10 users can log into linux OS and run task at the same time via GUI or CLI without encountering any issue

Graphical User Interface (GUI) -   and
Commands- CLI 

Note: There are some computers that has linux OS installed in it and you can perform task via GUI just like the way you can perform task using GUI in windows. 

In Run level 5, you can use both GUI and CLI to run your task using run level 5


Run level 3 - Multi user (CLI) : 

This means that you can only use command line interface CLI to run your task using RUN LEVCEL 3. It is multi-user CLI only. This means that multiple users can run task but only with CLI when you are using RUN LEVEL 3. This means that we can only run commands here. That is, linux command

Commands- CLI :

How do we run commands in linux?

We run commands in linus as a single commands or combining the commands in a file.

The combination of commands in a file is called SHELL SCRIPTING

       Linux Commands 
       Shell SCRIPTING 
       ec2-user / simon / landmark

In those days,  Run level 1 - Single user (CLI) was what was available. If you are about 9 years  IT experience, you would knmow what this is

Linux CLI was mainly used troubleshooting servers using single-user mode. Only one user that usually log in. The only user is the root user.

Whe you create linux server those day, you would have to change the root user password, usingwhat is called a single user mode. Because you can not create another user. 

The admin user was the root. That was the only user before the advent of cloud computing like AWS.     

What have we use to access and run tasks on our Linux servers??

In our environment, we have been using COMMANDS ( CLI ) to run tasks in linux server

What are you using to access and run tasks on your workstation?? GUI

Workstation: This is the computer system that you use to work in your company. Maybe a Laptop or Desktop  

Servers: Srvers are super computers. Our workstation helps us to connect to servers. We want to manage task with servers.

For you to be able to connect to your server, you would need the following

1. SSH clients like Mobaxterm, terminal, etc

  mobaXterm === Helps you to connet to your virtual server like linux as an SSH-CLIENT  

Windows OS uses mobaxterm as their SSH client  
Mac OS uses terminal as theur ssh clint  

2. Laptop (PC): All your computer runs on Run level 5 Multi user

 Running commands in CentOS, Linux, etc, an interpreter is required to interpret what you are writing. These interpreters are

       bash / sh / ksh / bsh /csh /tsh /zch    that is

       bourne again shell / shell / korn shell / bourne shell   etc


Example of commands are like 

touch class29   this will create an empty file


macOS  uses Zch as an interpreter

WINDOWS 

2.  WINDOWS: Windows equally have commands line that you can run task with. like 
      Powershell
      GitBash 

Platform Engineer (Linux ): As a platform engineers, you would be doing the following below as part of your task.

1. creating server:
The vertual server you have been creating is called Virtual Machines (VM) which you can create using microsoft azure or aws or GCP cloud as the cloud computing service vendor/provider.

2. Patching servers

This means that you would be installing, removing, upgrading, updating a server that is, yum remove or yum upgrade or yum update.

That is, patching involves COMMISSIONING SERVERS, DECOMMISSIONING SERVERS, RECOMMISSIONING A SERVER

COMMISSIONING SERVERS means installing a server. like installing jenkins 2.70

DECOMMISSIONING SERVERS means removing a server. like removing jenkins 2.70

RECOMMISSIONING SERVERS means updating or upgrading a server.  if there is now a newer version of jenkins like 3.10, you can noq upgrade to a newer version of jenkins. This is called RECOMMISIONING THE SERVER

3. Security Management
4.  Users     Management
5.   File      Management
6.  Package   Management 
7. volume mgt -   (OS +  Storage + applications). THERE IS WHAT WE CALLED Logical volumme management LVM. iT IS A TOOL THAT IS USUALLY USE IN LINUX BUT SINCE WE HAVE CLOUD COMPUTING, 

Cloud Computing or Cloud Engineering:

--- Requirements to run a Business

       1. capital expenditures  
       2. Factors of production [Land ,Labour,Capital,Enterpreneur]

SERVER is a super computer

Server computer has the following below

HARDWARE: hardware manufacturers are HP, LENOVO, DELL, APPLE, ASUS

OPERATING SYSTEM  which are linux or windows or mac or solaris etc, 

Database which is storage (to store resources) and 

Application

INFRASTRUCTURAL ENGINEERS: 

in this type of engin., we are looking for what is called 

on-prem Infrastructure or On-prem DATA-CENTER = 

This a situation where a company own and manage the storage, operating system, application and networks infrastructure in their premises and not outside their premises.

on-prem Infrastructure  simply means that a company own and manage all required IT infrastructures which include storage (database), servers, operating system, networks, application, inside their premises.

On-premise infrastructure, is also called private clouds, are cloud computing environments that are reserved for use by one organization. 

On-premise infrastructure provides the same type of elastic, virtualized services as public clouds, but also offers greater control and visibility into infrastructure.


Datacenter:
  1. Applications: This is software or a build that you are taking to the market. this is what is most important
  
  2. Database: This is the storage of your server
  3. OS    This is the oprating system of your server
  4. HardWare: This is the physical properties your application or software contains

One of the disadvantage of the On-prem infrastructure is that if there is a redundant in the company storage due to lower users subscription, the company will suffer the lost alone. This is why companies this days to not really subscribe to on-prem infra. anymore. They are looking for engineers that can create and operate cloud computing for them.

DATACENTER MANAGEMANET

How do you manage a datacenter or Infrastructure?

For you to manage a datacenter, you must have the following

1. dedicated space
2, You need a high bandwidth super fast internet
3. Redundant power supply: This means that you need to have unfailing power supply. There should not be any power supply faliure. This means that you have to have alternative of other source of power just incase the one you already have failed.
4. support readily available. The linux admin should be there to render support at any time
5. Proper capacity planning
6. It is time consuming. You would speend 80% of your time to ensure that your infrastructure are functioning, when you own an on-prem datacenter
7. Leadership experience is needed to manage an on-prem datacenter

If you look all this. You will realise that owing and managing a datacenter would be tideous or capital intensive. This is where cloud computing like AWS comes in

Oon-prem infra., are not as elastic as cloud infrastructure

CLOUD COMPUTING INFRASTRUCTURE

When you have a cloud service provider like 

AWS
AZURE
GCP 
CLOUD

What is a cloud service?

Answer: A cloud service is a service made available to users on demand via the internet the internet from a cloud service provider. Some of this services are EC2 instance (server) by AWS, database storage (EBS by aws), application, networks etc.

What is cloud computing?

cloud computing is the delivering of computing services like server, application, database, storage, networks to users via the internet

ADVNATAGES OF CLOUD COMPUTING SERVICE

1. it makes your life easy. You do not want to the bothered about so many thing you bother in an On-prem infrastructure. that is
when you subscribe with AWS, AWS will manage your, database, network, server, OS but you will manage your application yourself

2. cost: By utilizing a cloud service, it would be less expensive for you because you only pay for what you have used. It is pay as you grow. 

3. highly Scalable. It works with scaling because you can be able to scale or change in size or storage as your business grow or shrink.

SCALABILITY: this is the process of changing the size of your datacenter or infrastructure as your traffic or users grow (increase) or shrink (decrease).

For Example

If we have 1m users subscribe to app. server1 via Nginx. If the app server has be configures in such a way that it can only receive a traffic of 1m, Nginx will route the 1m users to app server1.  if the number of users grow and exceed 1m, the infrastructure is configured in a way that another app server would automatically be created and Nginx will route traffic (the number of users) not more than 1m to the second app server called app server2. If the numbers user now exceed 2m, another app server3 would automatically be created and Nginx would route traffic to the app. server3. 

If the number of users now begin to shrink or reduce. Let say that number of users now shrink to about 1.5m, the infrastructure is configured in such a way that app server3 would be taken out automatically and Nginx would route traffic to just app server 1 and app server2. The Nginx is standing as both a proxy and a load balancer to the app. server. 

This above example, app server can be removed or added automatically. This is called SCALING

4. Elasticity: Elasticity and scalability are explaining the same point.
 If we can achieve scaling with relatiove ease, it means our infrastructure is elastic.

5. Speed: With cloud computing, your organization can start using or connect to an application within a minute like enterprise application. You can create or provision a server, network, database within a minutes.

On prem infrastructure is not elastic compared to cloud computing or cloud infrastructure because the degree to which they attain a scalable infrastructure is not easy ro not easily done

For example

Netflix = Neflix is an example of enterprise application

What does NETFLIX DO?

Netflix bring cinema to your homes. 

HOW DO THEY DO THAT?

The will buy movies from movie producers, then they will buy or rent a server from AWS. 

They might rent A server with storage size or CPU of 100-terrabyte  by  8TB of RAM, 256VCP in AWS..

Netflix will now upload those movies in the server or s3 bucket they just rented or bought.

When you watch a movie, you watch the movie from man s3-bucket

5. You organization can deploy application globally within a minutes using cloud computing service like AWS. AWS has a regional data-centers all over the world.

6. High availability and Reliability: You organization has to be readily avaialable anytime of the day so tha customers can have access. It has to be reliable also

TYPES OF CLOUD COMPUTING

1. PUBLIC CLOUD: This is the cloud computing that is in public. Like when you want to access your aws account, you access it via the internet 


2. PRIVATE CLOUD: A private cloud computing is a cloud in which an organization use all the resources that is in that cloud alone. Companies like MTN owes their private cloud or data center.

MTN can also rent out some of their datacenter to other companies like barclays bank to create their own private datacenter

3. Hybrid cloud: This is a process whereby an organization uses both private and public cloud all together and task would be shared amongst them

4. multi-cloud

      CLOUD SERVICE MODELS

In cloud service model. we have the following

Cloud service provider and the subscriber (you)

There is what is managed by cloud providers and there is what is managed by you.

In the above context, 

CLOUD SERVICE PROVIDER here could be  AWS OR GCP OR AZURE

SUBSCRIBERS OR USER here could be TESLA, BOC, BOA etc

Looking at the below server, you will notice that we did not create any server for some and why we did for some

Jenkins ------>  we created server
Sonarcloud  ----->  we did not create server. You create an account and subscribe
Sonarqube  ----->  we created server
Newrelic----->  we did not create server. You create an account and subscribe

IQ  

Explain the different infrastructures that you have managed?

Answer: Thank you for that question, when it comes to cloud service model, I have managed what is called

1. IaaS = infrastructure as a service. This is when the infrastructure is used by admin users to host an application like tomcat or nexus or maven or sonarqube, choose the operating system like redhat8 linux or ubuntu linux or window OS or centOs , data like 250GB of ram, runtime like Java runtime evironment 11 (JRE11) or Java openJDK

But the cloud service provider will manage the visualization, the storage, the servers like EC2, networking.

A good example of a cloud service provider that operates IaaS is aws ec2

2. PaaS = platforms as a service: This infrastructure is mostly used by developers to only build application like web-application and the developer determine and manage the data

But the cloud service would determine and manage the operating system like redhat8 linux or ubuntu or windows, runtime, visualization, storage, server, networking

A good example of a cloud service provider that operates IaaS is aws aws ecs

eks = elastic kubernetes service
ecs = elastic container service


The cloud service provider  while the service is eks or ecs

AWS provide that kind of infrastrure above. Example is eks and ecs all in aws.


3. SaaS = software as a service: this infrastructure is used by end-users to consume application. The application is hosted and built by the cloud service provider, the data, the runtime, the OS, the visualization, the server, the networking, the storage are all determined and managed by the vendor or cloud service provider.

A good example of a cloud service provider that operates PaaS is Sonarcloud, github, gitlab, NewRelic etc.

In SaaS, you just use the service directly, you dont have to create any server or determine the data. You just create an account and then use the services.

In AWS, CloudWatch is in the category of SaaS. This is also used for manitory your infrastructure or server performance as well. Just like NewRelic

IN TYPICAL EXPLANATION OF ON-PREM INFRA, IAAS, PAAS, SAAS are as follows

1. On-prem infrastructure: Buying of a car CASH is an example of an On-prem infrastruture. The own the car so therefore, the car stays in your premises that you pay for, oil change, change of tires, breaks change, insurance, fuel, driver, tolls, car deppreciation everything about the car is managed by you.  This is expensive to maintain.

2. IaaS: To lease a car or Car finance is an example of IaaS. This is because of the fact that you will pay for car packing, buy the gas, drive the car, oil change, change of breaks, tire change, but the lease company will be resposible for car deppreciation in terms of value, car finance, owe the car.

3. PaaS: Car Hire is an example of PaaS. This is because when you hire a car, you only responsible for the gas, driver and tolls but the vendor or hire company own the car, car service, oil change, tire change, car insurance, car deppreciation

4. SaaS: Taxi or Uber is an example of SaaS. Once you book uber or Taxi by paying, you would not be responsible for anything.  the gas, driver and tolls, owner of the car, car service, oil change, tire change, car insurance, car renewables, car deppreciation are all managed by the taxi or uber and not you.

Generally, amongst the above infrastrutures, the most fastest type is the SaaS because it will deploy you to production in a minute. The user can directly access the service. Just like facebook. Ohers is time consuming compared to SaaS. 


POPULAR CLOUD PROVIDERS.

Below are the popular cloud providers to take note of

 aws = amazon web services   
 microsoft azure  
 GCP = google cloud platform  
 ibm  
 VMWare Cloud  
 alibaba
 Oracle cloud
 Red Hat
 SalesForce

IQ

Explain your experience in SaaS delivery

Answer: Thank you very much for you question

With SaaS experience which involves an already existing software that can be used as a service whereby the vendor or cloud service provider determines and manage the application itself, the data. the storage, the server, the opearing system, the networking, the visuals etc. 

Example of SaaS are

Github
Sonarcloud
NewRelic
AWS CloudWatch

All you need to do is to create an account and use the services. Like Github, 

I have been able to create an account in github, create a project repository in which i have integarted the project repo with jenkins for jenkins to clone an application or code from my github successfully.

I have also created an account with sonarcloud and then integrated it with jenkins to run a codeQuality analysis.


AWS GLOBAL INFRASTRUCTURE

The url is https://aws.amazon.com/about-aws/global-infrastructure

Regions in AWS 

There are various regions in aws that you can actually host your aws server from. There are

us-east-1   =   North virginia
us-east-2   =   Ohio
us-west-1   =   California
us-west-2   =   Oregon

Availability Zones in AWS

Availability zones; are physical isolated multiple datacenter locations in a region. This means that a regions has more than one availability zones or more than one datacenter locations.

This also means that availability zones is like a datacenters which is in a region.

In us-east-1(north virgina ), we have various availability zones or datatcemters. which are

us-east-1a   =   Arlington
us-east-1b   =   fairfax
us-west-1c   =   loudoun
us-west-1f   =   prince williams

Available zones are designed in such a way that a failure in one zone is almost impossible to affect the functionlity of the availability zones or datacenter of other locations

Example. If there is an earthquake in us-east-1a (Arlington), it will not affect the availability zone of us-east-1b (fairfax)

If you decide to host your application servers in different availability zones or data-center in north virgina like

If i host app.server1 in  us-east-1a, host app.server2 in  us-east-1b, app.server3 in  us-west-1c and host app.server4 in  us-west-1f

the availability zones are designed in a way that, If there is an issue with availability zone in  us-east-1a due to an earthquake, it is almost impossible fo it to affect other availability zones in another location.

If you host the app. servers in different availability zones, you a very sure of a very high and reliable availability and functionality of your app. server

WHAT TO CONSIDER BEFORE CREATING/HOSTING RESOURCES AND CHOOSING A REGION AND IN AWS

In choosing the regions, you have to consider

1. The price: Price is also a factor. Some availability zones is cheaper than the other. Availability zone in africa might be cheaper than the one in the USA.

2. Compliance regulations: You may be working in Barclays bank and you may be told that you should not host the database of the bank application server in the cloud. It maybe for security reason. All you need to do as a staff is to comply or obey the regulations. Every establishment has their own policy or regulations

3. Service availability: Create must create a resource that is available in the region you selected. Even if you do not leave or reside in that region but so long the region has the resource you are looking for please select the region. 

Like example. if you want to use ecs while you leave in africa and the closest datacenter top you is sount-africa. If you see that SA datacenter or avaialabilitry zone do not have ecs, but if it is available in north virginia, you have to choose north virginia.

High Availability.

How can we achieve high availability?

a. If you host/launch your resources in different availablity zones 

b. if you launch/host your resources in different regions

c. If you lauch your resouces in different cloud service providers like AWS, GCP, microsoft azure etc

d. If you host your resources in different instances (server) in AWS like having 4 app. servers in different ec2 instances

4. user/customer location: The location of customers or user is a factor you consider when hosting a server. If the end-users or constumers or company would mostly be in USA, you should not host your application server in UK or in India. It should be hosted in USA.

5. Latency: This talks about how easy or fast it is for users to connect or access your applications.

For example, if you type 

if you type  google.com  in USA and it takes 50ms (ms =mini-seconds) to display

if you type  google.com  in UK and it takes 10ms (ms =mini-seconds) to display

if you type  google.com  in SA and it takes 400ms (ms =mini-seconds) to display.

The one that has the best latency is UK connection because it has the lowest time for users to connect to google.com

google.com in this case is the application server.

6. Security: If you have a company in China, it is almost part of the chinese regualtion that all companies that operate in china to host their application server in china. This is for their own security as they see it.

VIDEO 92  AWS-2   (AMAZON WEB SERVICES)

just as you have the following services providers for customers consumptions like

ISP = internet service provider
PSP = Phone service provider

We also have 

CSP = Cloud service provider

CLOUD SERVICE PROVIDERS

Cloud service providers privides cloud computing services on demand

These are companies that provide cloud services. Examples of such companies are

AWS = Amazon web sewrvices
Microsoft Azure
GCP = Google cloud platform
IBM
VMware
Alibaba

What are the reasons behind Cloud computing?


cloud Computing services includes: They provide resources for users bases on demand. Some of the resources they provide are as follows
      servers  
      databases 
      storage  
      networking
      web hosting
      applications hosting
      virtualization
      monitoring
      backup
      Data Analytics
      security 
      auditing  
      ticketing, scheduling, systems management and auditing
      Paas, Iaas, SaaS  

If you know how to operate services in AWS, you can also know how to do that in microsoft Azure and GCP. The differences that you may see is the name of how they call their resources. Like
Server.

            aws              azure                       gcp  
SERVER      ec2 Instance     VirtualMachines             vm   

AWS call server as EC2-instances. Azure call it Virtual-machine, gcp also call it virtual-machine.

      EC2 BASICS

EC2 stands for ELASTIC COMPUTE CLOUD

EC2 simply means your computer in the cloud

What can EC2 be used for?

1. EC2 can be used to scale up and down to handles changes in your requiremenst
2. EC2 can be used to lauch virtual servers as many as you need to configure TO networking server, security server, database backup server, monitory server, auditing server, etc

COMPONENTS OF EC2

What is EC2 made up of?

                                        EC2---:
  Basic or regular Computer   vs        aws-EC2 :

  OS     ---->                         AMI [amazon Machine image] (OS + Additional Softwares)
  windows, linux                       Example of this is Redhat8/Ubuntu/centOS, windows

  Hard Drive       ---->               EBS (Elastic Block Store)\
  RAM             ---->                RAM

  CPUs[2, 4, 16 core]  ---->           Instance type [t2.mirco, t2.medium]

  Network CARD        ---->            IP Addressing 

  Firewall         ---->              Security Groups


  EC2 INSTANCE PURCHASING OPTION

  Choosing the purchasing option of EC2 instances, we have the follwing options

  1. On-demand option: 

  - This is the most expensive purchasing option.
  - You are charged on when your instance is running and billed per hour
  - It is flexible for you to stop, start and terminate your instance anytime
  - it is flexible to scale up and down when there are changes in your workload or changes in your user traffic.

  2. Reserved option: 

  - This allows you to purchase instance for a set period. This means that you be decide to purchase instance for just for 3 years. This option will allow you
  - For this option, even if you stop and terminate the instance, you would still pay for it because it was reserved for you.
  - This option is cheaper than the first. There is a significant discount using this option

  3. Spot  option: 

  - This option price flunctuate due to supply and demand in the market .
  - Amazon sells the unused instance for cheaper price for a short period of time and different company that are interested would bid for the unused ec2 instance and whoever wins the bid would get the service but the winner does not know how long it will last for them to rent the instance because once a better market come for the instance, AWS will withdrawn the service and give it to the one that want to pay the actual amount for it maybe reserve the instance. The spot OPTION IS THE CHEAPEST OF THEM ALL.


  HOW IT WORKS

  In AWS, you can create multiple EC2 instance on each EC2 hardware. Aws achieve the creation of multiple ultiple EC2 instance in EC2 hardware by a process called virtualization. 


EC2-HARDWARE

  EC2 hardware can accomondate multiple instance launch.

  For example:


  a company like Tesla can decide to Reserve an ec2-instance for 3 years. it is only one of the ec2-instance that was reserved and there are still plenty instances left. Another company like Iphone can still come and maybe subscribe spot option or subscribe via on-demand option of ec2-instance on the same ec2-hardware where tesla subscriibed. Other companies might also subscribe depending on the number of ec2 instance that can be launched on one ec2 hardware or data center

DEDICATED EC2-HARDWARE

another company like BOA may decided that they do not want to share the ec2-hardare with any other company maybe due to security reason and decide to Reserve the whole ec2 hardware to themself for 7 years. This means that no company other than BOA would be able to launch any ec2 instance in that ec2 hardware that was reserved by BOA.
If you want to launch any instance, AWS would not route you to the BOA reserved ec2 hardware.

The kind of instance that BOA has subscribed to here is called DEDICATED INSTANCE. The entire ec2 hardware is dedicated to BOA.

When you launch an instance in AWS, it is AWS that we determine which of the EC2 hardware that the instance will run on.

The most expensive amonge the differrent option of ec2 iNstAnces is the On-demand instances BUT HIGHLY SCALABLE (that means it encourage pay as you grow. It also makes your infrastructure to automatically expand as user traffic increases and shrink as user traiffic decreases) and when you stop or terminate the service, the payment would stop as were.

TAKE HOME HERE ABOUT PURCHASING OPTION

  - When you want to do testing  of your application that would require a high computing capacity, you can go for spot option because you are just using the service to test the application which is just for a short period.

  - If you want to host an server that you want to run for about 5 years, you can go for the reserved option for the launcing of AWS ec2 instance

  - If you want to host a server for practise purpose of for schooling, you can go for ON DEMAND option because you can easily stop and terminate when you are not practising and that also stop the accumulation of charges.


 HOW ARE WE CHARGED FOR USING EC2-INSTANCE?

1. Purchasing option
    on-demand, reserved and spot

2. instance type: this talks about computing power of the instance that you wnat to launch. Example of the instance type are: t2 nano, t2 small, t2 micro, t2 medium. this speaks of the speed of the instance.

Instance type to be chosen should be a function of the kind and size of the application that we are going to be deploying.

     For example

if you want to buy a computer Laptop in the shop and the one that is available in that shop are:

  i3, 16GB and 4 processors, 1000GB SSD  
  i3, 8GB and 2 processors, 256GB SSD 


Above, the one with the highest computing capacity is 

 i3, 16GB and 4 processors, 1000GB SSD 

 it is a function of the number of the processor that it has. The processor is the computing power of that computer

So your price would be a function of the computing capacity of the laptop. And this computing capacity is the number of processor it has.

Anything that has to do with computing capacity in a computer happens in the CPU. and so a laptop with  i3, 16GB and 4 processors, 1000GB SSD will be more expensive here because it has 4 processors while the other has 2 processor

INSTANCE TYPE FAMILIES

Below are the instance type family you can choose from when launching an instance and the description of the computing power and the type of thing or deployment it can be use for:


Family                   Description                 What Type Of Deployment It can Be For

t2, m4, m4             For general purpose           websites, web application, dev., code-
                       balance or moderate            repos, business apps, micro services
                       performance

C3, C4, CC2            Compute Optimized.            web-servers, front-end fleets
                       High CPU                      batch-processing, science and  
                       performance                   engineering apps 

g2, p2                 GPU Optimized.                high performance databases, science
                       High end GPU                  machine learning 
                                                     video encoding

r3, r4, x1             Memory Optimized.              databases, data-mining
q1                      Large Ram memory     

d2, i2, i3             Storage Optimized.              data warehousing, NoSQL
hi1, hs1               high densisty                   NAS

How to check your computing capacity of your computer.

click on start button, type in   THIS PC  on your search below, right click and click on properties. If you read the device specification, you will see the computing capacity of your computer.

3. AMI type: This talks about the OS of the instance you want to lauch. AMI = amazon machine image. like  REDHAT, UNBUNTU, CENTOS, windows

4. RAM: The size of your instance memory will also influence how much you would be charged

5. regions: The region you want to host your instance also influence prize

6. EBS = elastic block store:  the size of your instance hardrive that would stores all your work

LET US LAUNCH AN EC2 INSTANCE

In lauching an instance, consider where you company and its customers is located and site a region where aws datacenter or availability zone closest to the company si. If the company is in Nigeria. Africa, the closest region that has aws datacenter or vaialabilty zone is in capetown, southafrica. So select south africa. if the company is in california, then the closest datacenter to california is in california. so, select california.

After you have selected the region, then click launch instance

NOTE: I created all my instamces in OHIO which is the us-east2 region . Let me created these instances in north virginia which is us-east1 region

You can do launch the server using the new and old experience.

If you want to use the old experience, click on OPT OUT TO THE OLD EXPERIENCE located at the top right side of the page with blue coloration.

The first thing that it is asking you to do is for you to choose the AMI also known as the operating system.

Amazon Machine Image (AMI):

  1. MY AMI = Golden AMIs  

  To launch instance using the Golden AMIs is by creating an instance out of another instance that has both an AMI (OS) and softwares that was deliberately installed on it for a specific purpose.

  For exmaple

  If i have an instance running in redhat8 AMI and I installed java (openJDK11), Jenkins, git, vim  and some dependencies on the instance. I call the instance or server JENKINS1. I can decide to make it a golden IMAs by clicking on action, select IMAGE AND TEMPLATE, if you click on CREATE IMAGE, it means that you have customize jenkins1 as a golden AMIs.

  Whenever I want to launch a similar instance, if you select the customize image, the new instance would have everything that jenkins1 has. That is same OS and all the installations that was done. That is

   Instance summary for 
   i-0859bda0d6e112357 
   TAG = Jenkins1
   AMI = Redhat8 + openJDK11+ + jenkins + GIT + PLUGINS  

  2. community AMI = The once in community is free tier eligible but only have AMI (OS) only without software. Some of the AMI (OS) here are ubuntu, rehat, centos, SOLARIS, windows, amazon linux, etc 
     
  3. marketplace AMI = This are not free. This contains both AMI (OS) and Softwares  

     Example of some software here is called splunk. This is used to monitoring server in aws

Let us choose the community AMI by ticking amazon linux as our AMI, in configure, you can decide to launch more than 1 instances at the same time. You can launch 100,000 instances, 10000, 100 ,10 3, at the same time. Let us launch 2 instances at the same time by typing 2 in NUMBER OF INSTANCE. under DNS hostname, CLICK ON enable resouces based Ipv4 scroll down, 

You can decide to paste or write a script that run a task under USER DATA

WHAT IS A USER DATA?

A user data is a script that can run certian task while launching an instance or server

For example. Let apache web server be install along with the launching of the isntance by writing a script below:

 #!/bin/bash
 sudo useradd cliff  
 sudo yum install httpd -y 
 sudo systemctl start httpd
 sudo systemctl enable httpd 
 sudo echo "DevOps is good" >> /var/www/html/index.html 
 sudo echo "<h1>Welcome To LandmarkTech</h1>" >> /var/www/html/index.html 

 copy and paste the script on user data or metadata, and then click next

It takes us to storage. 

In storage, since we are installing apache web server on this instance, This webserver is going to have data writing on it. How would data be captured in the webserver. We need to create a storage called an EBS volume (elestic block store) that would capture and store the data. 

The EBS Volume IS GOiNG TO ACT AS A ROOT FILE SYSTEM. 

The root file system will be mounted as an EBS volume.

Whenever you want to create a file or directory, it need to be stored somewhere. in a storage device and that storage device in aws is EBS volume

Let us create an EBS volume of about 10Gb

So if you go to the storage, under volume type, you will see that it is root by default. under SIZE, type in 10 GB and then select GENERAL PURPOSE SSD for volume type (if you select the one with i01 and i02, the computing capacity performs better than the one with gp2 and gp3 but it is more expensive than gp2 and gp3)

EMCRYPTION
You can decide to encrypt the EBS volume in a way that the data that is going to be stored in the EBS would require a key or token for you to read the data. This can be achieved by click on where it says encryption. For now, we do not want to encrypt, so let us select NOT EXCRYPTED

DELETE OR TERMINATION

By default, DELETE OR TERMINATION box is always ticked. This means that whenever you terminate your instance, the data in your EBS volume will be deleted as well.

If you untick the DELETE OR TERMINATION box, anytime you terminate or delete your instance, the data in the EBS will still remain.   

click next

ADD TAG

In add tag, type in the word NAME under the KEY, and then type in the name you want under VALUE. Let us call it webserver29

You can add another tag by clicking on ADD ANOTHER TAG, under the KEY, then type what you want to name under key ( we can type in Environment) and then under VALUE. Let us call the environment to be uat. 

We can add up to 50 tags to an instance

click on next

SECURITY GROUP

NOTE: Before the administrator and users can access the server or instance you are about to create, you must go through the firewall called the security group in aws to open the neccessary ports that needs to be opened to get to the server or instance.

Let us create a new security group by ticking CREATE A SECURITY GROUP and then type in webserverSG as the name written on the security groupName and description

NOTE: Firewall is also known as the security group

For administrators to have access to the instance you are about to launch, the required ssh port has to be open which is 22 on the security group. it is via ssh you can connect to your ssh clients like mobaexterm or terminal.

Users will also be accessing or route traffic to our app. server via appache webserver and so it will need another port to be open. Since we are installing apache webserver along with the launch, then port 80 will have to be open. Apache webserver port is port 80 for http, also open https port 443

NOTE: ADMINISTRATOR ARE USING SSH PROTOCOL (PORT: 22) TO ACCESS THE INSTANCE WHILE USERS ARE USING HTTP (PORT: 80) AND HTTPS (PORT: 443) TO ACCESS THE WEBSERVER.

The administrator here is the guy that is launching this server which is cliff. so we have to open ssh port 22 by selecting ssh under RULEand 22 as port, then under source, select MY IP which is cliff IP address, under description, type in CLIFF ACCESS

You may have more than one administrator. Let us add another administrator. Another adminstrator to be added has a seperate IP ADDRESS.

Let us add joy as another adminsitratoR who can ssh into the server we are about to launch.

Let us assume joy IP is 100.23.245.22/32. You can choose or forge joys IP and then save the ip just incase

So, click on ADD RULE, select ssh, type in 22 on PORT RANGE, select CUSTOM for source, and type in Joy IP address (100.23.245.22/32) under source info, under description, type in JOY ACCESS.

You may or you may not choose IP for Joy. that is why i selected CUSTOM under SOURCE so you can formulate and write any number there as IP and make sure you remember that number

Note: SOURCE means, who can access my server

FOR OTHER USERS WHO ARE NOT CHOSEN AS ADMINISTRATORS OR MANAGERS

For those that are not chosen as admin., or every other users to access the server appache.  http port which is the port for the appache server has to be opened. The other users maybe anywhere in the world.

So, click on ADD RULE, select http, type in the port number 80 which is default port for http on PORT range, select anywhere on the SOURCE and under DESCRIPTION, type in USERS OR CLIENTS ACCESS. because clients would be accessing the app. server via apache webserver that is why clients access must be created.

Also, for users or clients to be able to securely access the server from anywhere in the world, i will create another rule by clicking on ADD RULE, select https on TYPE, type in port 443 as the default port number for https, under SOURCE, select custom, under DESCRIPTION, type in USERS OR CLIENTS access. finally, click on REVIEW AND LAUNCH

Once you click LAUNCH, it will ask you for ssh-key. Let us create a new key and call it key29
select create a new key, type in the key29 and then click on download key pair and safely save the key. then click on launch instance. 2 INSTANCE WILL BE CREATED BECAUSE YOU SELECT 2 for number of instance from the onset.

Once you have launched your instance. You can be able to access the apache webserver that you have created along with the instance you launched by copying the public IP address/filename on an internet browser. Once you do that, it will displAy the content that is in your webserver.

that is

3.93.216.142/index.html

index.html is the file name for apache webserver

http://3.93.216.142/index.html

If you try to access your webserver with your private IP on the browser, you will not because you are on a different network or VPC if you do so. Internet is a public PLACE AND SO, It can only work with a public IP

NOTE:

Anytime you launch an instance. there are 2 Ipaddress that comes with it. They are

1. public IP
2. Private IP

But you can also have an elastic IP but you have to pay for it.

3. Elastic IP

IP Address is the server or instance address on a network.

1. Private IP: By default, every EC2- instance comes with private IP. Private IP allows one server to connect with other server so long both server are in the same VPC network like aws. 

That is, Servers that was created on runs with aws, such servers can connect with each other via a private IP.

VPC= virtual private cloud

2. public IP: Ec2-instance in aws can be launched with or without public IP. pUBLIC NETWORK ALLOWS YOU TO COMMUNICATE WITH OTHER SERVER WITHOUT ASKING FOR AUTHENTICATION AND AUTHORIZATION

NOTE: Your laptop or desktop computer has its own public and private IP address.

For you to know the public ipadreess of your laptop, run the following command on your windows gitbash 

curl ifconfig.co   or curl ifconfig.ca to know the public Ip address

hostname -i    to know your private IP address

3. Elastic IP address: is used to retain the public IP so that anytime you stop and restart the server, the public will be unchanged.

HOW TO CREATE AN ELASTIC IP

Under RESOURCES, click on elastic Ip, click on ALLOCATE ELASTIC IP, click on ADD NEW TAG, just to assign a name for the elastic IP and type the word Name on the key info, under Value option, type in my IP which is now the name of the elastic IP and then click on ALLOCATE.

You will see that an elastic IP has been created. You can associate or integrate this IP on servers in your environment or VPC. That is, you can integrate it in webserver1 and webserver2 that you created. You move the IP around. THAT IS WHY IT IS CALLED ELASTIC. The Ip number would never change even if the server goes down. 

Elastic Ip adreess are not free. You would have to pay for it before you can use elastic Ip address because it retain your public Ip without changing anytime you stop your instance.

Since we created 2 webserver. We can be able to ssh into the severs. 

Note: swebserv1 and webserver2 are in the same VPC network and so, they can connect with each other by ssh with  private IP

Your laptop ipaddress are not on the same network with your webserver1 and webserver2. BECAUSE OF THIS, WEBSERVER CAN ONLY CONNECT WITH YOUR WINDOWS LAPTOP VIA THE PUBLIC IP ADDRESS


NOTE: tHE PRIVATE ip is static. The private IP remains even if you stopped the server. But the public IP is renewed by creating another public ip address whenever you stop and start your instance

Note: the instances or servers  that we have just created where amazon Linux server

LET US CREATE A WINDOWS SERVER OR INSTANCE

What ever you are creating, you must mount bthe root file system (EBS = elasti lock store) under storage and volume

let us create a windowsServer2019

click on LAUNCH an instance, it will put you on the new look or experience of aws. Click OPT OUT TO THE OLD EXPERIENCE,  since it was written that the old experience will soon start to work, let us opt out to the new experience.

enter the instance name you to give (windows-db-server as the name you want to give), click on ADD TAG if you want to create more server, let us add another server by clicking ADD TAG, under KEY INFO, put ENVIRONMENT, under VALUE INFO, put uat, if you scroll down, you will AMIs (amazon machine images also known as OPERATING SYSTEM in general terms)

NOTE: examples of AMIs are redhat8. amazon linux, windows, Macos, centos, ubuntu which all these are in the COMMUNITY AMI, if you click on BROWSE MORE AMIs, it will take you to more options where you have the COMMUNITY AMI, MARKETPLACE AMI, MY AMI, QUICK START AMI, If you click on MARKETPLACE AMI, you will see AMI that is in the market that you can buy. It is not free. The have a high availability, highly scalable etc. Since we dont want to buy now, let us go to COOMUNITY AMI which are free tier eligible

Click on COOMUNITY AMI, type in windows in the search button since we want to create a window server, make sure that it is free tier elgible so you dont have to pay so select the windows version with free tier eligible, under instance type, select t2-micro which is free tier eligible (t2-medium is not free here in windows), you can select the key that you have created before, let us select the key we created recently. 

For security group (firewall) in windows, admin-user can connect to windows instance or server via a protocol called REMOTE DESKTOP PROTOCOL (rdp) which is listed on port 3389. 

the ideas is there is always a firewall or security group when you are trying to connect to an instance. every instance has a firewall which has a port that needs to be selected before admins can have access to the instance.

So under CREATE SECURITY GROUP, ALLOW RDP TRAFFIC would be selected but if it is not selected please make sure you select it, and then you can just launch the instance.

click on you server url and then click on connect, click on RDP client, click on download remote desktop file, go to the download and then double click on the download, click on CONNECT from what appeared, it will ask you for password to be able to connect to the windows server. go to your windows instance in aws and click on GET PASSWORD located in the place where you clicked on DOWNLOAD REMOTE DESKTOP FILE

You will see that as you click on GET PASSWORD, it shows you the key token that you selected before. You need to go to where you saved the token and copy the token and then paste it in the box below where it says, private key content and then click on DESCRIPT PASSWAORD, then copy the new password  under PASSWORD (it means that it has converted the private key earlier into a password. This now becomes you RDP password).

Go back to the RDP download you did and then paste your rdp password and click connect

It takes time to conect when you are connecting to your windows server for the first time.

Note: most companies do not utilize windows server because it can easily be attacked by virus and the use of virus figter is wlays needed. Mots company uses linux server because it is virus free and secured. most companies want to use a virus free environment.

RESTFUL API IN NEWRELICS

Please see newrelics video for explanations.

IQ; why is it possible as to working remotely as a DevOps engineer?

Answer: It is basically because most of the IT resources that are needed are being hosted in the cloud. This is why It engineers can work remotely (work from home).

Some of those resources are

sever
database
networking
visualization
security
monitory
storage
etc


Note: If a company has an on-prem datacenter, it means that all the IT engibeers would have to work from the office because where they can have access to the It resources needed to work for the company.

Let us say 2 people called Paul and Silas trying to access an AWS reseouces like server (amazon-linux, amazon-windows). What would be requyired for both of then to have to access such resource in AWS.

SILAS: would required rdp (remote desktop protocol) on his laptop and also have the right amazon-windows credentials or authentication like username and password, and also have the firewall or security group details like the rdp port number (3389) and also the ip address of the amazon-windows server in order to be able to access the server.

PAUL: would required SSH client like mobaxterm, terminal on his laptop and also have the right amazon-linux credentials or authentication like username(ec2-user) and password (called ssh-privateKey), and also have the firewall or security group details like the ssh port number (22) and also the ip address of the amazon-linux server in order to be able to access the server.


  video 94.

    STORAGE 

(DIFFERENT STORAGE FACILITIES THAT AWS HAS)

 Why is AWS storage facility a multi-billion dollars facility?  It is because most multi-billion dollars company like NETFLIX make use of AWS STORAGE FACILITY.

 HOW TO STORE DATA IN AWS.

 DIFFERENT STORAGE OPTIONS IN AWS

 1. EBS = Elastic Block Store

 2. S3  (s3 bucket) = s3 stands for simple storage service. This is an object storage like storing of videos, audios, pdf files, artifacts, logs, snapshots etc.

 3. EFS  = Elastic File System

 4. SNOW BALL

 5. STORAGE GATEWAY


     EBS

DB-SERVER

We can launch an instance in aws using OS like aws-linux or redhat or windows or solaris to launch the ec2-instance or server and then install a database software or application like mySQL or NoSQL or Mongo on the server to become a DB-server (database-server).

After you have created and install a DBserver, an EBS volume is mounted on a root file system that usually store all of your data. You can also create more ebs volume and mount it to a new file system created by you and attach it to your server.

WHAT YOU CAN DO USING EBS

1. With EBS, you can choose the type of file system that you want
2. aws EBS is really fast
3. aws EBS is really cheap
4. YOU can store up 16 terrabite of data per storage using aws EBS
5. You can only access aws EBS in the region you created it and when you move from that region to another, you need to migrate the EBS to the new region that you are. You cannot access it in another region when you have not migrated the resource
6. You need a server or EC2-instance to attach the aws EBS to. 
7. You can increase the volume or decrease the volume of your EBS even when it is still in use or when it is still running.

DATABASE SERVER CREATION AND SECURITY.

1. For security purpose, when creating the database server, Make sure that you disable where it say allows auto public IP. you need to disable it because we do not want any external person to be able to access OR ssh into our database server directly using a public IP.

2. For security purpose, you need to encrypt the database by ticking ENCRYPTION, that would required a token or password or secret key to be able to read or modify data. For practise, we will not do that now

3. For now, you can click DELETE UPON TERMINATE so that whenever you want to terminate the server, the EBS would also be deleted to avoid charges since it is for practise purpose for the time being.

Let us create a t2-micro instance as the instance type and bear it in mind that the instance need to be secured because it is going to be our database server after a database software has been installed in it. Let us call the instance DBserver and also click on add new tag, under key put Environment and uat. let us create a new security group and call it dbSG, let us make the EBS under storage to be 11 GiB

select ssh for port 22 and then let the source to be anywhere and then click launch server. Note, we didnt use MY IP as source because we do not want to give out our IP for security reasons. We only want to make do of our private ip

if you launch, you will discover that there is no public IP comes with it because we disabled it. This is how you can actually launch a server without a public IP but only with private ip.

NOTE: You cannot delete EBS volume that is in use or running. you can only increase when in use.

EXAMPLE

There is an external consultant and an Engineer who want to access our DB server in our environment. For security reason, they cannot access our DBserver directly, they can only be able to access our DB server via a WEBSERVER (apache or nginx) or JUMP-SERVER (also called BASTION SERVER).

The EXTERNAL CONSULTANT  can access by ssh into the webserver or jump server with a port number and public Ip address because the both have a public IP 

They both also have a private IP. 

External environment can only access our server via public IP because they are not on the same VPC network. 

Meanwhile, any server in our enevironemnt like webserver, jumpServer, database-server, app. server can connect to each other via private IP because they are in the sane VPC network (like using aws network for all of our servers listed above).


INTEGRATING DB-SERVER INTO WEBSERVER

Since external environement can only access the database server via webserver, let us connect our DBserver into our webserver because it is only our webserver we left public enable.

1. Using ssh client like mobaxterm and shh into webserver that we created where apache were installed, with a private key .

2, change the hostname to web by running

sudo hostnamectl set-hostname web
sudo su - ec2-user

3. Let us access our DB server via webserver

To access our DB server, authentication is needed. 

You can even get the desciption on how to access DB server or any sever by going to where the server is in AWS, click on the url, click on CONNECT, click on SSH CLIENT and you will see the procedures.

The procedus are, you will need 

DB-server-ssh-key, username(ec2-user) and DB-server-private-IP

Let us first of all open a file in our webserver and save the DB-server-ssh-key by running

sudo vim key29     and then copy and paste the encryoted token inside the file

and then run the command

sudo chmod 400 key29

in order to hide and prevent the key from public view

You will run the following command on your webserver

ssh -i ssh-key ec2-user@private

and the type in yes, to process with the integration

once you do that, you will see that you have access DB server via webserver. You are now in DB server.

NOTE: It usullay takes sometime for the dbserver to load in order to integrate into webserver. So you have to be patient.

Since we are in DB-server, Let us change the hostname to DB by running

sudo hostnamectl set-hostname db

sudo su - ec2-user

So we have now enterred or switch from webserver to DBserver



ELASTIC BLOCK STORAGE
 
How to manage storage (EBS)

Our EBS volume is mounted on the root file system of our webserver and db-server or any other server(instance) newly created.

if you run 

lsblk   or   df -h

You will see that size or volume of our disk or storage is 11G mounted on a root file system /

Note, anytime you create a server, an EBS volume is always there by default, you can increase or decrease it during the creation of your server but you can only increase it after the creration when the server is running or not running and decrease it only when the server is not running or when it is stopped, not in use.

Let us go to db-server and try to delete the ebs storage volume that we have there by going to the db-server, click on volume, tick the volume, click on action and you will see that delete is not active. you will see that you are unable to delete it because the ebs is still in use or still running.

You can only be able to delete aws ebs volume only when you stop the server. 

You can also increase the volume when it is fully consumed, even when the ebs volume is still in use or still running but you cannot decrease the volume when in use. 

To verify this, go to the db-server, click on volume, tick the db-server volume, click on action and you will see that you can increase the volume by increasing the size from 11 to upward and then click on modify and then refresh the page, you will see that the volume has been modified.

if you run the command 

lsblk

It will list the ebs volume mounted in the root file system / and it will also show the volume or size of the disk or ebs

If you run

df -h

It will tell you the volume of your EBS used and what is left.  If you see that you have almost used your EBS, there is an option for you to increase your EBS volume or create another disk or ebs and mount it on a file system while is still in use.


CREATING ANOTHER EBS VOLUME WHILE THE OLD ONE STILL RUNNING

Let say our intention now is create another EBS volume of your DBsever while the server and old EBS are still running or still in use

go to your DB server, click on volume, under Elastic Block store, you see volume, click on create a volume, select GENERAL PURPOSE SSD gp3, let the volume size be 7 under size, then make sure the availability zone matches with the one that was chosen during your instance launch

NOTE: If you aws instance (DBservr) you created is in the availability zone (datacenter)us-east-1c, then your new and old EBS volume must also be on the same zone or datacenter us-east-1c, select us-east-1c under availability zone

You can found out the availabilty zone that was chosen by going to your instance (dbServer), 

click on volume and you will see the availability zone that was chosen located towrds the right end. If you can see it, reduce your font to minimal so that you can see the avaialability zone. and then select


The first volume was attached to the root file system / by default. 

Let us create another ebs volume called appvol and attach it to a new file system called /appvol. This can be achieved by clicking on add tag, write the word Name under key, type appvol under value and then click CREATE VOLUME

If you go back to your volume, you will see that another volume called appvol was created by you.

Let us attach the ebs volume newly created to db-server

scroll down the page and click on storage, there is only 1 ebs volume or disk that was attached to dbserver, the newly created ebs would say AVAILABLE.

if your run    

lsblk       

you will see that it is only one disk OR VOLUME of 12G which we increased mounted on the root file system

go to volume and you will see all the volumes, tick appvol, click on action, click on attach volume, under instance, select the instance or server you want to attach it to, and then click attach. if you go back to volume, you will see that it says, it is IN USE.

If you scroll down and click storage, you will see that two volume or disk are now attached to the instance or server dbserver.

You can also verifiy ny runining 

lsblk

You will see that two blocks or volume or disk are now attached or is seen on the dbserver. The first block or ebs was moount on the root file / by default but the second volume that was created has not be mounted on any file.

By implcation, we cannot save data on the new volume created because we have not mounted it to any directory file. We need to mount itto a file system so that data can be recorded.

if you run

df -h

it shows only mounted files and the gigabyte.

Let us mount this ebs volume or disk on a file

Below is the url aws that shows you the step to take to moumt you new ebs volume or disk into a file system

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html

After you have paste the above url on your browser, you will see that to mount your ebs volume or disk or device that you created on a file system , you need to create a directory  like /app by running

1. sudo mkdir /app

/app  is different from app

/app is a directory (app) inside a root directory /

while app is just a normal directory (app)

Note the directory you must create nust be under a root or the name of the directory must start with /

The next is to run

2. lsblk

to know how your disk is written. That is xvdg  or xvdf.   that is, f maybe be replaced by g  in your file system. if so, you can adopt v and if not, xvdf

Then run

3. sudo file -s /dev/xvdf  or sudo file -s /dev/xvdg

to know the type of thing can be store on your disk or volume. In this case, it will show (data = this means that would be stored here ).


4. To format the disk xvdf or xvdg, run

sudo mkfs -t xfs /dev/xvdg  or sudo mkfs -t xfs /dev/xvdf

5. let us mount the ebs disk on the file system /app by running

sudo mount /dev/xvdf /app  

your file system xvdf maybe xvdg that is, f maybe be g  in your file system. if so, it will be 

sudo mount /dev/xvdg /app

to know if another disk or file system has be created, run

lsblk  or df -h

you will see that (disk /app) is seen. This means that the disk or volume has now be mounted on /app   

the first default disk mount on root file system was xvda

the second created disk mount on /app file system was xvdf


Example 2   MOUNTING EBS OR DISK TO A FILE SYSTEM

Let us create another ebs volume and attach it to your DB server and then mount volume on Jenkins home directory (/var/lib/jenkins). 

Let the ebs volume be called jenkinsVol and let the size be 6G

The reason why we name the ebs volume to jenkinsVol is because you want all jenkins job to be recorded on that ebs volume specifically and not to be shared with other volume.

Follow the same way we used in created ebs volume

Please remember that Jenkins is not installed in the db server. You can verify on ur dbserver CLI by running

ls /var/lib/jenkins  

You will see that Jenkins was not installed but you can actually mount an ebs volume of dbserver created to the jenkins home directory. let us call the ebs volume created as jenkinsVol.

aFTER THE EBS mount point or vol has been created, let us attach this volume to db server

let us attached the ebs volume that was created  to db server by clicking on volume, tick the jenkinsvol and then click action, click oattach volume, select the instance and then click attach. You have attach the ebs vol to our db server.

 to know if you have successfully attach the vol to the db server, go to your db server, click volume, click on storage and you will see that jenkinsVol has be attached to db server.

 You can also run a command

 lsblk, you will seee that the ebs vol created is now attached to the dbserver

Let us created a directory  by running

sudo mkdir -p /var/lib.jenkins

Follow the same procedures to mount the disk or volume jenkinsVol on the jenkinsHomeDirectory /var/lib.jenkins.

MANAGING YOUR DATA IN EBS VOLUME.

1. Your ensure you data persistency or safety, you can created a snapshot or an exact replica of your ebs volume that is being mounted on a file system as a back-up. So that if your file system get corrupted, you can always retrieve your files back with the complete data inside the file.

NOTE: Every volume that is created must be mounted on a file system. If it is not mounted on a file system, the volume will not be able to record any data.

Note: The snapshot or the replica of your ebs volume that you have created as a back-up will be stored in the place called  s3 BUCKET

The s3 bucket can also in same region with ebs volume but it could be in different availability zone

NOTE: The snapshot or back-up can be utilized or use to create another ebs volume in another region or server

SNAPSHOT LIFE CYCLE POLICY

This is a process whereby a snapshot or a backup is being created from our ebs volume to our s3 buckect periodically and also deleted from our s3 bucket peroidically. 

For example

SNAPSHOT LIFECYCLE POLICY 

In a snapshot policy, a database backup or snapshot can been configured in a way that a snapshot or a backup would take place every 12am midnight daily and each snapshot would be deleted after every 7 days. and on the 8th day, the first one would be deleted and so another one is created. It means that a total number of 7 direct replicals of our database will always be present. So that when there are problem with the database maybe due to file corruption, data can be restored from the snapshots at the s3 bucket. This is an example of a snapshot lifecycle policy.

HOW TO CONFIGURE YOUR SNAPSHOT LIFECYCLE

Let us create snapshot lifecycle policy for dbserver

click on ebs, click on volume, tick on the ebs volume you want to use, click action, click on create snapshot lifecycle policy, under target resources tag, type in the word Name (in where it says enter a key), in where it says enter a value key, type in dbvol because that is the target resource (volum) that we have created, under POLICY DESCRIPTION, put  my policy,  click add, type in the word Name (in where it says enter a key), in where it says enter a value key, type in mySLCP (MENAING MY SNAPSHOT LIFE CYCLE POLICY )  then click next, you will get to SCHEDULE DETAILS. select daily for frequency, select 24hours for every, TYPE 00:00 under starting at (MEANING EVERY MIDNIGHT), select count under retention, type 7 under keep (meaning you want to keep 7 snapshots, meaning since it is 24 hours, if it gets to 8 days, the first one would be deleted) or you can select age under retention type and also select 7 under keep(meaning the datatbase would always have 7 snapshots after 7 days and on the 8 day, the first snapshot created would be deleted)
, tick copy tag from source, click on review policy, and then if you are satisfy with your configuration, then click create it now.

LET US CREATE A SNAPSHOT ON JENKINS EBS VOLUME (jenkinsvol) that we created

click on ebs (elastic block store), click on volume, tick on the ebs volume you want to use (jkvol), click action, click on create snapshot lifecycle policy, under description put jk-snapshot, click on ADD tag, put the word Name under key, put jk-snapshot (as the name under value key), and then click on create snapshot

if you go to volume, click on snapshot, you will see the jk-snapshot that we just created.

what can be done with a snapshot you just created?

If you click on action, you see that you can create volume from the snapshot that you have just created. under action, click on create volume, slect us-east-1a, put the word Name under key, put jkshvol under value and click on create.

if you go to volume and click, you will see that jkvol right there.

This means that from a snapshot, you can create an ebs volume from any availability zone (AZ).

MODIFICATION OF SNAPSHOT LIFE CYCLE POLICY 

You can modify a snapshot life cycle policy by clicking on life cycle manager, tick the SLC (snapshot life cycle) you want to modify, click on modify life cycle. it is also a place you can delete life cycle.,   You can modify anything you want to modify. You can modify the schedule etc.

NOTE: By default, Every ec2-instance created must have a root volume which maybe or may not be ebs.
 
Note: By default, every ec2-instance root volume is deleted as soon as instance is terminated but any additional ebs volume created, would not delete when the instance is terminated.

DETACH AND ATTACH OF EBS VOLUME

You can remove or detach ebs volume from one instance say, instance1 and attach it to another instance say, instance2.

For example.

Let us go to volume, tick on appvol that we created, click on action and click on detach volume or click on force detach volume. It a take a while like 3 minutes for it to be enforced, when the volume you detached shows (AVAILABLE with a blue color), tick the volume and then click on action, click on attach volume, select the instance you want to attach it to (let us select webserver), and click attach

BUT

But note, You cannot detach or attach a snapshot from/to an ec2-isnatnce.

If you go to your dbserver by CLI and run

lsblk   and      df -h

You will notice that the the ebs volume that you detached is still seen


NOTE. If you delete or do anything using GUI and when you come to CLI, and it didnt show, try and reboot the server by running

sudo reboot

and then reconnect back to the server. In our server, we connect dbserver into our webserver. Since we are dbserver inside appache webserver as a proxy to avoif cyber attack or hackers, when you reboot the server, it takes you back to webserver. You can just reconnect back to your dbserver by running

ssh -i Dbserver-Key-pem ec2-user@DbserverPrivateIpAddress

You will see that it will take you to the dbserver

Any created ebs volume that is detached and attached to another server only goes with the volume alone without been mounted on any file system. You need to mount it on a file system for it to be able to capture and receive data

RESTORATION OR USE OF SNAPSHOT.

You can be able to restore or put to use a snapshot by creating another ebs volume from the snapshot. Snapshots are always stored in the s3 bucket.

BENEFITS OF AMAZON EBS RESOURCE

1. Reliability and secured storage
2. Secure: this is because we can encrypt the data
3. it stimultate high performace
4. Easy data backup,  as we see using snapshots and snapshot life cycle policy.

NOTE: The snapshot that is created from an ebs volume is stored in the s3 bucket. You will not see where it is stored in the s3 bucket because aws has already take care of that. It is not of your business to access the aws s3 bucket because in the backend, aws has stored it in their s3 bucket and not your s3 buck but you can only see  and access the snapshots. but you can see the s3 bucket that the snapshot is stored. That is how it works for ebs storage.


    EFS  (ELASTIC FILE SYSTEM)

EFS is a scalable, elastic and networking file system


SCALABLE means (The tendency to increase or decrease its volume or architecture when there is growth or there a shrink,

ELASTIC means (ability to attain scalability with relative ease and fast)


This another storage resource that aws has and it also require mounting.

NOTE: OHIO has 3 availability zones (3AZs), North Virgina has 6 availability zones (6AZs) 

EXAMPLE

Let us create a dbserver or instance on OHIO region which has 3AZ by default and then host our dbserver each of the 3AZ making then 3 dbserver or you can just do only 2 database from 2 regions. 

The advantage of hosting our server in different AZ is because it enhance

a. high availability: in the sense that should incase one of the zone or datacenter is down, it will not affect other datacenters and so our server in those other datacenters would not be affected.

b. security: to avoid cyber attacks on the 3 dbserver at once because once hackers take over one datacenter, others will be spared and so our 2 other dbserver are saved.

c. Latency: It also enhance the ability of your server to connect very fast using the internet base on lesser load.


Sionce we are creating 3 server in 3AZ but same region (Ohio
)
1. Each of the dbserver that you created need to store their data and so, a volume is created and mounted on a root file system for each. 

Let us call the volume dbvol1, dbvol2, dbvol3 and a root file system is mount for each volumes 

2. Let us created another volume called efs/nfs volume (elastic file system/network file systemn) for each of ther server

The idea would be anytime any one of the efs/nfs volume of one dbserver receives a data or info, it will automatically synchronize or be captured in the efs/nfs of other dbservers.

For this to work, there must be securities that needs to be configured.

Example of file system that works like efs/nfs  file system are 

GoogleDrive (is used to store data in google account)

OneNote is (is used to store data in microsoft)

Icloud (is used to store data in iphone)

SharePoint (is used to store data in microsoft)

ShareDrive

Above are all distributed file system

Using Google For Example

You have 3 device like PHONE, TABLET AND COMPUTER and you want your data (contacts) to be or synchronize into these 3 device

For you to achieve this, you create a googlemail account or gmail account and then sign in the same gmail account on your phone, tablets and computer and therefore, every data you have or store in one device will be synchronized to the other devices or captured by other device.

The sign in of your gmail account to each device is called authentication, you need to know the right username and password to be able to sign in. This is where security is in place.

AWS has the above kind of storage called efs/nfs


CREATION OF EFS

let us go to our aws account and type in efs on the search button and then click EFS, click on file system, click on create efs, put myEfs under name, under VPC, select security group which is already selected by default, select or tick multiple AZs and then click on create. An efs is created

You will observe that we did not assign any size for volume, it came with default volume, we did not because efs is highly scalable, the volume would increase automatical when there is an increase in the data or info that goes into the file

Let us go to our apache webserver CLI by exiting dbserver with the command

exit

Let us install nfs-utils by running

sudo yum install nfs-utils

let us go to our efs that we just created. click on the efs name, click on attach, let us select mount via IP, select any of the az, click creat and then copy the command below on the page.

Go to your webserver CLI and create a directory

mkdir  efs

paste the what you copied and the press enter key

It is delayed to run, it means that it refuse to create a mount point. The issue maybe security issue. 

Let us check the security group that you selected when creating this. 

click on file system ID, click on network, click on manage, under security group, you will see that last mount ponit that was created by default when creating efs which would be the first on the page. copy the ID or url of the under security of the mount point

Let us go to our ec2 by typing ec2 on search botton and the click on ec2, scroll down, under network and security, click on security group. You will see that all the security group that you have created in the region will appear. paste the securty group you copied on the seacrh button and thenl click on the security group id or url that comes out, click on the security group ID, you will see that the security group is allowing all traffic, 

It is not supposed to be allowing all traffic. You need to open relevant ports. for security reasons

You all need to do is to go to your security group and found out which of the security group that has ssh vpc with port 22 and if the source is custom, select it to anywhere. If you dont have such security, you can create new one. 

You need to add another inbound rule and select nfs vpc with port 2049 and also select source as anywhere

Once you have doen that, save the change

Then go back to your efs that you created, click on the file system ID, click on network, click on manage, select all the security group to the security group that you just created or added inbound ruls containing nfs and the save the change and then click on attach and the select mount via ip and then copy the commands


Then go to your webserver CLI  or server and paste the commands that you copied once again and then press enter

Then if it works, great

Then if you run

df -h

You will notice that efs mount point or volume has been created

Since we now have efs on webserver instance, Let us also create another efs on another server.

Let us connect to another instance you have in this region (let us use ohio or region). Maybe tomcat server if you have it in your region. Let us creat a mount point on that tomcat server

once you have connected to the server, to know if you have any efs volume before, run

df -h

install nfs-utils by running

sudo yum install nfs-utils

create a directory for efs by running

mkdir efs

let us create the mount point by pasting the mounting commands you copied before and click enter

efs has been mounted on the file system

if you run 

df -h

You will see that it is the same efs that is still mounted on this server



Note: The mounting must always take place in the home of ec2-user. do not change directory.

We now have 2 servers with the same efs mounted upon them. 

Let us see if we add any data in one of the server whether it will synchronize or captured in the other server.

If you cd into efs in server1 and run ls, you will see that there is no file

If you also cd into efs in server2 and run ls, you will see that there is no file

let us touch a file or create an empty file called Info on server1 by running

sudo touch info

if you go to server2, cd into efs and then run ls, you will see that the same file that your created in server1 has sybchronized in server2

In the same vain, if your touch a file called web in server2 by ruuning

sudo touch web

if you go to server1, cd into efs and then run ls, you will see that the same file that your created in server2 has sybchronized in server1

Therefore, If you want all your database info to be captured in all your database servers, you use efs networking file system(nfs) so that any modification that is been done on the databse in any of the server will synchronize into other servers.

In nfs, for mounting to take place, the nfs port (2049) must be opened

CLASS QUESTIONS ASKED.

Can you adopt the synchronizing effect of the efs/nfs for upgrade?

ANSWER: Efs/nfs cannot be used to upgrade servers in a synchronizing manner because version upgrade has nothing to the with file system. efs only used for synchronizing or capturing files/data change in an efs directory/repository.

For example

If you want to upgrade your database server like mysql from one version to another. mysql6.7 - mysql7.5, You recommissioning your mysql server. 

Let say you have 2 mysql servers in different AZ and you want to upgrade mysql server1, if you do that, mysql server2 will not automatically be upgraded because it has nothing to do with files or data storage.

It is only data or files that can synch when you adopt the service of efs/nfs for bother mysql database server.

NFS is a network file system that permits files or data to be able to synch into different servers when there is a data/file change in one of the server or instance.

For this to be successful, an nfs-utils client needs to be installed in all the server that want to partake in this synchronization process and efs directory need to also be create in each of the servers. You can install this by running

sudo yum install nfs-utils    using a redhat linux server

sudo apt install nfs-utils    using an ubuntu linux server

NOTE: efs is an nfs service that is managed by aws. 

aws call nfs as efs for linux OS and nfx for windows OS

IQ: What are the different storage/volume facilities or service that you have used in aws?

Answer: Thank you for that question, As a devops engineer, in our environment, 

1. we have used block storage

2. we have used file storgae like efs/nfs

3. we have also used object storage like s3 bucket.

1. We used block storage with ebs volumes. block storage is very important because we use then for our root file system. They are very fast interms of performance. To access the data or read the data from the block storage is very fast.

But the problem with block storage is that the storage is blocked has the name implies. files cannot be attached to multiple server at the same time. It lack synchronizing effect. It is constrain to a single server at a time.

With block storage, it is important to note that, if we want to do a database backup or replicate or capture data to more than 1 instance  or server, you will first of all create a snapshot from the ebs volume containing your files or data to an s3 bucket and then create another volume from that snapshot and attach the volume another instance. That is how you can repliate your data in multiple servers. This is somehow cumbersome and slow. This why a better way to replicate data in multiple servers are the use of efs in aws.

2. If you want to achieve a real life data synchronization, we will adopt the elastic file system which is a highly scalable, networking file system provided by aws.



    OBJECT STORAGE

     S3 BUCKET

S3 OR 3S means simple storage service

This is a storage resource in aws which can be used to store objects like

audios, videos, pdf files, artifacts, logs, snapshots(to back-up volumes), images.

With s3 bucket, you are not required to create a mount point or mount the storage on any file system o attach it to a server/instance.

BENEFITS OF AMAZON S3 BUCKET

1. used to store snapshots of your data in S3 (means, simple storage service)
 but cannot lock or create permissions on the file like you can do with the normal file.

2.  it has high availability and data loss is nearly impossible.

3. The content in s3 can be served to the public directly via internet. You can even have a full website working directly from s3 without having an instance or vitual aws server or without creating an instance or virtual aws server

4, You can access the content or data from any region but more cost may apply

5. great for story logs or for storing stories like movies.

6. good for storing images and videos.

7. it is more cheaper than EBS

8. great for short term archiving and also good for longterm archiving but more costly to retrieve when it is longterm.

ADVISE: make sure you master these things because when it gets to interview, they may ask you which of the aws resource that you have used.

MAXIMUM OBJECT SIZE

What is the maximum object size that you can store in s3?

Answer: The maximum object size that we can store in s3 bucket is 5000GB = 5TB

FOR REVISION

EBS  = Elastic Block Storage = RW0 (read, write, one) :

  Block storage can't shared data with mulitple servers at once
  FAST
  CHEAP
  DATA Must be mounted 
  We can't access data without mounting
  Maximum capacity = 16000GB = 16TB
      MC<=16TB
    Maximum capacity root volume is 1023GB
  WRO

EFS   = File Storage: = RWX (read, write, many. this means that it can be read and write and can be mounted on multiple servers at the same time, diffrent user can read, write )
Maximum capacity = starts from 6kb and it starts grow because it is highly scalable. pay as you grow
READ  = FASTER
WRITE = SLOW
mounting required
Can be shared/mounted with multiple servers. That means, We can mount efs to multiple servers at once

S3  = object storage = we can r/w (read and write) from anywhere without mounting. And multiple individuals can do that without mounting.

CHARGES IN S3 

Charges depends on data size, transfer and storage class 


without mounting you can access data

each object in S3 will have a unigue end point to access

s3 is a scalable storage in the cloud

s3 is a regional service. The regions we have in aws are

us-east-1 = NV 
us-east-2 = Ohio 
us-west-1 = CA 
us-west-2 = oregon   

CREATE AN S3 BUCKET 

1. s3 bucket name must be globally unigue accross aws and must not contain uppercase letter and spaces. What this means is that 

For example if you create a bucket name called "test", if you have an existing name called "test" that you created before anywhere in aws, the name will not be accepted, It will tell you that the name already exist. That is what it is meant by s3 bucket name must be globally unigue accross aws.

2. s3 bucket is global and its in regions: S3 bucket are not available in some regions like Africa, Some Asia country like HongKong, jarkata and some others. If you select aws region during the course of creating s3 bucket, you will see some of the regions that do not have s3 bucket.

IQ: You maybe asked in an interview that what region did you think your s3 bucket should be created.

Answer: Thank you for that question. It would depend on where our application would be running and where the application would be deployed. If the application would be running and be deployed in Canada then, the s3 bucket should be created in canada region (CA-central-1).

3. Security: In s3 bucket, public access are blocked by default. For example, you have some sensitive or classified materials that you want to store in s3, it is very imperative that public view or access are blocked. So that no person that is not in our company can have access to the material.

So always utilize block public access and choose the access that you want to block

Versioning: Version is good to be enabled when you want to know that exact version of each object that have been stored in s3 bucket should incase you want to redeploy or make sure of the version in future. Using netflix for example, Netflix might have a movie series displaying, each season is like in version. season1 can be version1 and the deployemnt of version2 is season2.

Object versioning enabled makes the material stored in an s3 bucket be in order

You can also disable the versioning if you so desired.

Let us go to our aws s3 bucket by typing s3 in our search button, 

click on create a bucket, let us put test as bucket name, select AWS region you want (let us select Canada-CA-central-1), tick enable, under bucket versioning, tick enable ACL under object ownership, tick block puplic access, tick eable under server side encryption and then click create bucket. 

If there is any name like test that you created anywhere with aws, it will not work. So need to add numbers or anything to the name to make it unique or special or change the name completely until it is accepted.

Once the name is accepted and you are in your new s3 page

click on OBJECT and you will see where you can upload any object you want to store. The object maybe videos, audios, images, snapshots, logs, artifacts, pdf files, movies, pictures etc

MODIFICATION OF YOUR BUCKET

If you click on the bucket name you just created, click properties and then you can click edit depending on the part you want to modify. 

You can also modify permissions by clicking on permission. you will see where it says public access is blocked. You can modify it. You want to determine who can access the object that is in the s3 bucket

Let us upload object like video or image that we have in our computer desktop by clicking on object, click on upload, click on add file, select the image that want to uplaod in your computer, and the press enter.

If you want to access the image that you have just uploaded, click on the object name, you will see the object url, you can click on the url. You can also share the url to someone else 

but if the person does not have the permission, he/she can not access the image. 

You can not also see the image if the access is blocked. You need to grant yourself and the public access.

First of all, you can go to the bucket name, tick the bucket name, click on permission, click on edit under block public access, untick Block All Public Access, click on save changes.

Secondly, go to the object name and click on the object name, click on action and then click on MAKE OBJECT PUBLIC.

NOTE: For you and the public to access or view the object, you need to unblock public access from the bucket settings and also go to the object bu clicking on the object name and click on action and click on make public access via ACL. It will sow green letter saying SUCCESSFULLY EDITED PUBLIC ACCESS 

Let us upload the same file again. Do not delete the old one you uploaded. If you click on the object name, click on version, You will see that another version of that file will be created because you enabled versioning. then click the version you want to view or deploy. Th current ones are always on top.

You can see that one can access object without attaching the s3 bucket to any instance or mounting the s3 bucket to a file system. 

You can access or deploy the object directly to the public using the internet. 

NOTE: The permission that we used in opening this new s3 bucket called test29b is call

ACL (Access control list): This is what we used to regulate the public access.

PERMISSION

You can decide to grant public permission or access to the object in your s3 bucket by ticking on the object, click on object action and the click on make public via ACL, and then click on make public. 

If you go to where the object is and click on the object, click on the object URL or copy the object url and share to people, then you or they can be able to access the object by clicking on it.

BUCKET POLICY

How can we apply bucket policy?

In you go to your bucket, click on management, there is what i called life cycle rule which you can create. 

LIFE CYCLE RULE

let us click on create life cycle rule. There is also what is called STORAGE.

STORAGE CLASS: is the storage type that you have chosen when you create your bucket. The storage class tells indicates how much you would be charged.

When you store object in s3 bucket, you are charged. 

aws s3 charges depends on data size, transfer and storage class 

Example of the different storage class are

1. Standard: 
     The is used to store object that is frequently being accessed. Like a latest movie in Netflix. Netflix may decide to store the new movie here because it is a new movie and number of viewers will be high.  A trending movie. This storage class is more expensive than others.   

     Netflix can decide to leave this movie on this storage class for like 120days and after which, it should be moved to IA (infrequent access) storage class because the demand or frequent visit may have dropped.           

2. IA (infrequent access): 
	This is where infrequent access to object are stored. Like an old movie in Netflix where the number of viewers visiting have dropped due to the fact that it is no longer trending. This storage class is less expensive than the standard class. Netflix may also decide to leave this movie for like 120 days and after which, it may be moved to RRS.

3. RRS (reduced redundancy storage): This is used to store non-critical object, a reproducible object at a lower level of redundancy than STANDARD. The level of visiting or accesing the object here would be very limited. 

Object that is no longer trending or level of visit is very limited can be moved here. Netflix may move their movie here if the number of visit to such movie is now very limited and leave the movie here for like 80 days and see if there would an improvement on the number of visit and if there is not after 80 days, the movie would move to Amazon Glacier

4. Amazon Glacier: This is s3 bucket class is used to store objects in a very cheap way. It is mostly used to archive object that would be kept for a longe time without accessing. 

Netflix keep their movie that can cannot be access for a long time in Glacier storage class. They can just keep the movie here for about 360 days or more without access. Just for archiving

It is the cheapest of them all but costly to retrieve, good for longterm archiving
It takes up to 4 hours to retrieve your data. It is advisable to store object that you will not retrieve for a long time    

As a devops engin., you need to know the right storage class to use or subscribe to in s3 bucket in order to save cost so that your company would not be loosing money.  

TO VERIFY VERSIONING

Let us go back to the bucket that we opened, click on property. If you go to bucket versioning, you will see that we enabled versioning.  Let us go to object and upload the same object i uploaded before from my computer. After the upload is done, go back to object and then click on the object that you just uploaded, and then click on version. You will see that there is a  current version and there is a old version. Then you can now decide which of the version you want to deploy. 

Before you deploy, try and check if you have made the object public. You can do that by going to object action and click on make public. then you can access the object without mounting the object on a file system or attach to any server.    

VIDEO 98 

AWS VPC   

With VPC, You can control who access your aws resources like EC2 instance, S3, etc

VPC is automatically created for you by default as soon as you created aws account.

Whenever you want to create any resource in aws like database instance. You can utilize what is called VPC (virtual private cloud) which is inside aws cloud by default as soon as you create an aws account.

You can also create your custom VPC

VPC is a private network.

Note: aws itself is a public network. Anytime you want to create a resource from aws, you are creating it from a public network.

We can create our own private network (VPC) inside aws which is public network.

This means that when you are about to create an instance (server/VM), you can create the instance inside the VPC that you have created by custom

HOW IS A SERVER CREATED USING PRIVATE NETWORK (VPC)

When you create an AWS account, a default VPC is created for you. But you can actually create your custom vpc network to host your resources in aws.

Once you have created your VPC inside aws, you can now create an instance directly inside your VPC network that you have created by custom. 

Inside VPC network, there what is called subnet (sub-network) which you can also create. It is from inside the subnet that you can now create your instance.

You can create multiple VPC in aws cloud.

You can also create multiple subnets inside one VPC network. That is, more than one subnet inside a VPC.

You can also create multiple instances or servers in one subnet.

NOTE: You cannot directly create resources from VPC, you can indirectly do that via its subnet. 

Some examples of instances/server you cam create and labelled are

databse server, Elastic load balancer (ELB) server, etc

FIREWALL/SECURITY INSIDE OUR VPC

Before you can access your server inside your subnet, you would have to pass through different firewall or security. The first firewall you would have to cross is

1. NACL (network access control list): This is the security or firewall at the subnet level. This is the security just before the subnet where your server/servers are located. You would get authenticated before you can access the subnet environment. This controls who can have access to your subnet environment

2. SECURITY GROUP: This is the firewall or security at the instance or server level. It is firewalkl that you must cross before you can have access to the instance or create instance

ROUTE TABLES:
Route tables defines or controls how traffic should be routed to/from in subnets inside a VPC. We will have to define some routing rules in route table.

ACCESS CONTROL LIST: This is the list that filters network between subnets

INTERNET GATEWAY (IGW): This is a logical device like your internet router that controls how traffic should be routed to/from the internet. This is what allows external traffic into or out our VPC. it also what permits us to communicate with the internet

Basically IGW allows our VPC to receive traffic from/to the internet.

NAT (network address translator): This provide network address transflation to our private instance 


TYPES OF SUBNET

1. Public subnet: IF a subnet has a route to an aws internet gateway, that subnet is called a public subnet. This subnet is exposed to the internet. It can be accessed from the public.

2. Private subnet: iF a subnet has no route to an aws internet gateway, that subnet is called a private subnet. It is not exposed to the internet directly

NOTE: Any VPC that does not have an internet gateway can not receive any traffic from or to the internet or to the external environment


EXAMPLE

let say that you work with BOA, and BOA has a network that works with or to access all the departments. Department like

account
payroll
logistics
customer service

You will create a network like VPC that can access all these department and then, you will create a subnet for each to separate these departments from each other but all of the departments are in one VPC network. That is

account subnet
payroll subnet
logistics subnet
customer service subnet

What type of resources can you create inside your VPC network? 

Some of the resources that we can create is 

1. ec2-instances: 

The management of BOA need to determine how many ec2-instances can be created IN each offices where BOA is located. So an infrastructure planning meeting must be set up. 

In the meeting. they want to determine how many instances or servers would be created in offices like

Offices - VPC with private IPaddress 10.10.0.0/24

      MarryLand - Subnet 
         200 ec2-instances [200 ipAddress ] 

      TX - Subnet
         210 ec2-instances [210 ipAddress  ] 

      NY - Subnet
         500 ec2-instances [500 ips ] 

      AG  - Subnet
         180 ec2-instances [180 ipAddress ] 

The total ip Addresses and instances would be = 200 + 210 + 180 + 500 = 1090


HOW DO WE ENSURE THAT WE HAVE THOSE NUMBERS OF IPADDRESS IN EACH OF THE OFFICES

We are sure of attaining our expected target by using what is called

CIDR block: This stands for classless interdomain routing block. Classless Inter-Domain Routing

This is a method for allocating IP addresses and for IP routing

 If you have a private IPaddress 10.10.0.0/24

This is called IPV4 CIDR BLOCK

IPV4 means internet protocol version4
IPV6 means internet protocol version6

Infrastructure planning meeting:
CIDR block 
  vpc :

  Each number that make up the VPC private IPadress has 8 bits. Therefore. The ipadresses below has 32bits for each role
     
      PrivateIP/subnets
      8  8 8 8 = 32bites
      10.10.0.0/n                  where  n =  subnet which ranges from 16-32 [16,17,18,...32]
      172.10.0.0/n
      10.10.0.0/16 

32bites is an example of IPV4

For you to know the number of resources that can be created depends on the subnet prefix

no of resources = 2^(32-n)

where  n =  subnet which ranges from 16-32 [16,17,18,...32]

Now n starts from 16

if n=16

 no of resources = 2^(32-16) = 2^(16) = 2^16 = 65536. That is 

IPV4 cidr block     n      32-n      no of Resources 
  10.10.0.0/16   16  [32-16] 16   2^16 = 65536
  10.10.0.0/20   20  [32-20] 12   2^12 = 4096
  10.10.0.0/24   24  [32-24] 8    2^8  = 256
  10.10.0.0/32   28  [32-32] 0    2^0  = 1

  NOTE

ipv-4  is always = 32 bites  its means IPV4 cidr block  is  like  10.10.0.0 (with 8bits for each number)

   no of resources = 2^(32-n)

ipv-6  is always = 128 bites  

   no of resources = 2^(128-n) 

   if n = 16, resources = 2^(128-16)= 2^112


IQ
Now, if you are asked to determine the number OF resources that would be created and the cidr block in the above VPC of all the BOA offices in different location which has about 1090 ipaddress. What do you think would be the cidr block and the number of IPs?

Answer: if yoU KNOW THE NUMBER OF ips OF EACH LOCATION AND THE SUM EVERYTHING TOGTHER. Says all the IPs is 1090 Ips in total, You can use what is called  CIDR BLOCK GENERATOR that you can search with the internet by going to the internet and type in  CIDR BLOCK GENERATOR and then it brings out CIDR blocks. You can determine the closest or exact total IPS of BOA. You can keep putting in the subnet PREFIX numbers like 16, 18 to 32 for IPV-4 until you can reach the best that is exaCt or closest to the IPaddress of the total IPS of BOA.

NOTE: VPC is a private network in AWS. For you to create vpc, you need to determine the CIDR block of your infrastructure. And for you to know the CIDR BLOCK, you would have to know the number of resources that you want to place on that VPC



IPV-6 is represented as 2001:4860:4860::8888  with each number = 8 bites

when you count how many digit number for IPV-6 is  16

The total bites for 1PV-6 is 8x16 = 128  (16, 17, 18-----128)

IPV-4 is represented as  10.10.0.0/16  with each number = 8

when you count how many digit number for IPV-4 is  4

The total bites for 1PV-6 is 8x4 = (16, 17, 18-----32)

Let us create a VPC in aws and create a resources inside that VPC by typing VPC on search botton in aws 

and then click on VPC and click create VPC, select the region that you want, tick on VPC and more(this will create public subnet and private subnet), under autogenerate, type the name of your project in project (vpc1), under IPV-4 CIDR block(you will see that its like a generator that if you know the number of IPS or resouces that you want to create in your project, then you would be able to determine how the CIDR block would look like by inserting different number on the subnet prefix part until the number of resources or IPaddress that you ant comes out or something closest to it. for now, let us just choose 10.0.0.0/16), tick no IPV-6 (because if you choose IPV-4, it may determine you to create resources that would range to millions and billions. That would be a very large network but all of our projects has been limited to IPV-4 CIDR BLOCK), under tenancy, select default (if you select dedication, you will pay more), under availability zone select 2 (2 AZs will create 1 pub subnet and 1 private subnet in one zone and then another  1 pub subnet and 1 private subnet in another zone which have a total of 4 subnets by default, if you select 3 AZs =  1 pub subnet and 1 private subnet in 3 different avilability zones which have a totla of 6 subnets) let us choose 1-AZ for now, under NAT gateway select in 1AZ, under VPC endpoints select none for now, select enable DNS resolution and then click on create VPC.  If you click your VPCs, you will see the VPC that we just created

Note: It usually takes time to complete creating the VPC. So be patient.


Note: The VPC that we just created is not a default VPC. This is a custom VPC. It your own created VPC. 

Where we created all of our ec2-instances before is in default VPC that was automatucally created by aws when launching a server.

In a company environment, is it a good practise to create your server or resources in a default VPC?   The answer is NO. It is not a good practise.

We can create resource under this VPC that we just created.

MANUAL CREATION OF custom VPC

Let us also manually create another VPC and subnets on the VPC we created before by clicking on create vpc, select VPC only, put vpc2 on the name tage, select ipv4 cidr block manual input, let us create our own custom cidr block (enter 172.0.0.0/20 on ipv4 cidr), select no ipv6 cidr block, select default for tenancy,  and then click on create VPC AND THE vpc WILL BE CREATED.

If you click on my VPCs, you will see the vpc2

Now, in this vpc2 that i created from vpc1, I want to create my subnet manually from the vpc2 

vp2  cidr   = 172.0.0.0/20   =  This can have ips of 4096. This also means 4096 ips. That is 

type ipv4 cidr generator in the internet browser and put 172.0.0.0/20 and click generate. see the number od IPS it can give you. Remember that the number of that determines the number of instance or server).   you will arrive at 4096

 172.0.0.0/20   =  4096 ips (this means that the ips starts from 0, 1, 2, 3-----4096)

 SUBNET CREATION

 Genrally, to know the number of subnet that can be created in a vpc, you would divide the number of ips that came out from the cidr block internet machine with  256.   That is 

     4096/256 = 16

This means that you can only create not more than 16 subnets in vpc2

The ip address of a subnet ranges from 0 to 256. That is 0,1,2-----256

ips addresses of a subnet should not be more than 256 and the cidr block that can give you 256 must end with /24

ipv4 cidr block for 256 is  172.0.0.0/24

Therefore

privateSN 172.0.0.0/24              0,1,2-----256    the last digit must start with 0, next is 1
    
		172.0.0.0, 172.0.0.1, 172.0.0.2, -------172.0.0.256

publicSN 172.0.0.0/24   

		172.3.0.0, 172.3.0.1, 172.3.0.2, ---- 172.3.0.256   

Let us view the subnets that was created before by clicking on VPC and the click on subnets.

note:  172.0.0.0  is fabricated. You can choose yours provided is ipv4 cidr block and you can choose your subnet prefix

LET US CREATE A SUBNET IN VPC2

click on create subnet, select the VPC that you want to create the subnet from (delect vpc2), put private-sn on subnet name, select the AZ (let us select us-east-1b),In the case of the subnet ipv4 cidr block(the maximum number of ips that can be created for a subnet is 256 and so the ipv4 cidr black that can give you 256 is  /24. That is 172.5.0.0/24), let us create or add another new subnet by clicking add subnet, put public-SN on subnet name, select another AZ like us-east-1e on AZ, ipv4 cidr block can be 172.3.0.0/24 (we change one of the digit to 3 to different priv subnet from pub subnet), and then click on create subnet. You will see that it has created pub and priv subnet.

CONDITION: 

1. Your vpc ipv4 cidr block must be different from your subnet ipv4 cidr block. that is


vpc 172.0.0.0/20

private subnet 172.5.0.0/24

public  subnet  172.3.0.0/24

2. Your private and public subnet prefis /24 must be the same

3. Your vpn prefix /20 must be different and lower than your subnet prefix  /24

4. what determines the number of resources that can be created in a subnet is subnet prefix (n)






Let us create an internet gateway (IGW). This IGW will permit internet in and out of this VPC2.

click on internet gateway, you will see that there is an internet gateway that was created automatically when creating our vpc1, let us create another internet gateway by clicking on create internet gateway, put igw2 in name, and then click on create. (if it tells you that internet gateway is created with green coloration, it means that has been created).

The igw2 that was just created is like a virtual router device like your isp (internet service provide) router device that you have in your home.

ATTACH IGW TO VPC 

The igw2 that you have created, you can attach it to your VPC2 so that your VPC2 can have access to the internet.

Let attach the igw to vpc2 by ticking on the igw2 that we just created, click on action, select the vpc you want to attach it to (let select vpc2), then click on attach internet gateway.

NOTE: The first vpc called vpc1 that we created has all configuration in it. Like the subnet, cidr block, igw, route-table etc because we selected vpc that we come with subnet, route-table etc and so everything was created by default

The second vpc called vpc2 did not come with these things because we selected vpc only. We now had to configure and create these things (route-table, igw, cidr block, subnet ets ourselves.

CREATION OF ROUTE-TABLE

Route-Tables: This controls how traffic is routed in and out from our subnet or how traffic that goes in and out of our subnet. 

click on route-tables, click on create routetable, type privateRT in name, select vpc2for VPC, then click on create VPC

Let us create another routetable by click on route-tables, click on create routetable, type publicRT in name, select vpc2 for VPC, then click on create VPC

We now have pub and priv routetables. 

ATTACH ROUTETABLES TO SUBNET

1. Public routetable (publicRT).  if you click on the public routetable that you just created, click on action, click on edit subnet association, tick your public subnet (public-SN) and then click save association.

This means that you have attach public routetable to public subnet

2. Private routetable (privateRT).  if you click on the private routetable that you just created, click on action, click on edit subnet association, tick your private subnet (private-SN) and then click save association.
This means that you have attach public routetable to public subnet


CONNECT OUR PUBLIC ROUTETABLE TO IGW

In routetables, some routing rules has to be configure.

For exmaple. 

Our priv and public routetables are connected to our subnet locally. This means that any server that is created in priv subnet can communicate with a server that is created in pub subnet with the help of the routetable because they are the same cidr block and are connected locally. 

The routetable would not be able to route traffic externally. It cannot route traffic from the internet to our vpc or from our vpc to the internet.

We need to connect our internet gateway to our public routetable so that the routetable can route traffic to or from the internet.

This can be achieved by going to your public routetable, click on action, edit route, click on add route, under destination put 0.0.0.0/0  and then select internet gateway that you created (igw2) under target, click on save changes.

Note: 0.0.0.0/0  is the ipv4 cidr block for internet gateway. This means accept traffic from everywhere.

This means that your public routetable has route with the internet and also route locally.

In the case of private routetable, it only route local traffic. This is why it is private. it cannot route traffic to or from the internet.

This means that, an external network can be able to communicate with our VPC because of the fact that a public subnet and pub routetable inside the VPC has an igw.

NOTE: 

1. In a working environment, Servers that can be created in our private subnet are

Application server
Database server
Nexus server
Mavin server
sonarqube server
Sandbox
etc

An external traffic can not talk or access the resources that is created under private subnet.

The route to the private subnet is only internal

webserver
jumpserver
elastic load balancer (ELB)
ect

The resource in the public subnet can talk to resources in the private subnet provided both of them are in the same VPC network.

TESING OF OUR custom VPC (vpc2)

Let us launch a jumpserver (this can be created in our public subnet)

Jumpserver is a secured channel that runs traffic directly from users to our application server.

Note: Since jumpserver and webserver can be launched in the public subnet because they accept both external and internal traffic.

 WHAT TO DO FIRST

we need to go to our public subnet, click on action, click on edit subnet settings, and select enable auto-assign public ip ipv4 and click save. This is important so that if we create any resource here, it will assign a public ip to the resource.

Let us launch an ec2 instance called webserver by typing ec2 on our aws search and click ec2.

click on launch instance, key-name as ws, select rehat8 as the AMI (operating system) , click on edit network, select vpc-new under network, select public-SN as subnet,  create security group, click on create a new security group, jSG as security group name and put cliff access as description, select ssh and then select my ip on source (my ip selected is a secured option because if you select anywhere, it means that it will allow traffic anywhere in the world which is not a secured option. This will assign you a public IP as cidr block), put accept external traffic in description, click on add rule (to add another rule),  select ssh and then select custom on source (paste your vpc cidr block here which is 172.0.0.0/20), put accept local(internal) traffic in description, you can still add the first vpc by adding another rule clicking add rule, select ssh and then select custom on source (paste your first vpc (vpc1) cidr block here which is 10.0.0.0/24), put accept local traffic from customer service in description, select keyperm (key29)

Let us launch another a database ec2 instance also. Let us call this instance database, Sandbox-server. 

A SANDBOX is a type of software testing environment that enables the isolated execution of software or programs for independent evaluation, monitoring or testing.

sandbox is also a form of database.  

let the AMI be redhat, instance type to be t2 micro, let the key name be Sandbox-server, , select vpc2 under network, select private-SN as subnet, click on create a security group and let the name be sandboxSG, select ssh with port 22 under type. select custom for source and type in your vpc (vpn2 ) cidr block, put local access only under description. select key29 as your ssh-key and the click launch.

This sandbox or database instance has no public IP because we selected only private-SN (which has no auto-assigned public ip) only has a private IP.

Note: If you do not select vpc that you want, it will automatically choose the default vpc that you created in VPC network.


ACCESSING THE WEBSERVER SERVER or JUMPSERVER FROM EXTERNAL NETWORK

Using ssh client like mobaxterm CLI to access the webserver that you just created, click on the ec2-webserver in in aws, click on connect and then copy the command below

and then go to your mobaxterm, terminal, etc  (let us use gitbash. That is using window OS), if you are in gitbash, cd into downloads or anywhere your key pem is, and run   key*  to search for the ssh key that you want to use. If it is here in download, then run you have to hide it from being accessible by the public by running

sudo chmod 400 sshkey-name

Then 

paste what you copied from your webserver or jumpserver connect and paste it here and the press enter. You can now ssh into your webserver using CLI in gitbash. You have connectd to your jumpserver. You can name ther server jumpserver by running

sudo hostname jumpserver

sudo su - ec2-user

Note: If you use your private ip to connect to your instance in CLI, it will not work because your ssh-clients like mobalxterm, terminal etc  are not on the same network with your aws ec2-instance.

ACCESSING THE SANDBOX SERVER FROM LOCAL (INTERNAL) NETWORK

When you want to access your sandbox or databaserver, you can access it via your jumpserver or webserver so that users or external users can get to sandbox via jumpserver in order to secure our database of sandbox server.

You can connnect to sandbox by going to your sandbox server in aws and click on connect and then copy the ssh command and then come to your jumperserver cli and paste the sandbox command and then click enter. You will now b in sandbox server

You can name the server sandbox-server by running

sudo hostname sandbox-server

sudo su - ec2-user

INTERVIEW QUESTION.

1. What is responsible for the inability to successfully ping google.com or get access to the internet or to update (inside the server, run sudo yum update) in your sandbox server or db-server or any of your server? 

2. What would you do you for your app. server or sandbox server or database server to have internet access?

ANSWER:

1. Thank you for this question. for best practise, database-server does not communicate with external audience or server and so, If database do not receive an external traffic like the internet, it is simply because the database does not have a public IP address and the server is connected to a private subnet on the same vpc network.

You can know if a server can receive traffic from ther internet or public by pinging google.com inide your server CLI. That is 

ping serverName   like     ping google.com


You can know if a server is dead or alive by pinging the server via its IPaddress inside the server CLI. That is 

ping serverIp-address  Like  ping  17.0.2.3.30

and so, if the pinging does not work, it maybe that the protocol resposible to ping a server is not open.  The protocol to ping a server icmp (it is used to check if a server is alive or working).

You can go to your server, click on security, click on action, click on edit inbound rule, of your server in aws and click on add rule, select icnp, select anywhere in source and then click save.

After you have done this, try to ping your server and you will see that your server is alive but when you ping external server like google.com, you will seee that it will not still ping because it has no public Ip address.

2. For the database to have an external traffic or have internal access, you will need to employ the service of NAT gateway because you can not run anything that has to do with the internet (like updating your server or to install package like tree on your database sever, it will not work because tree is gotten from a package in the internet) on database server.

NAT gateway = network address translation gateway

Nat gateway is used to create a one way internet traffic to a private subnet where database server was created from.  

Nat gateway is created from the public subnet because public subnet is exposed to the internet and so if NAT is created, then the private route table would be configured in such a way that the NAT gateway that was created via public subnet would be selected by adding another route or rule in the private routetale. This private route table would be the one to route a one way internet traffic via NAT gateway to the private subnet and thereafter, you jumpserver or database server can now access the internet.

HOW TO CREATE YOUR NAT GATEWAY AND ALSO ATTACH IT TO A SERVER

click internet gateway under aws VPC, click on NAT gateway, click on create nat gateway, enter jps-nat as the name, select public subnet (public-SN) that you created, tick public under connectivity, click on allocate elastic IP, then select epalloc under elastic ip address (that is the only thing that is selectable here), click on create NAT gateway.
The Nat gateway has been created.

Now, go to your routetable and click, click on your private routetable, click edit route, click on add route, put 0.0.0.0/0 under destination (0.0.0.0/0 is everywhere piv4 cidr block), select and click nat gateway and click on the nat that you created (jpsnat) under target and click save. 

Then go to your private routetable and connect it to your private subnet by clicking on action, select edit subnet association, tick private subnet and then click save association.

Internet can now get to the private subnet because your via the nat gateway. 

If you now go to your sandbox or database server and ping google or anyother server, it will ping if the server is up and running. it is only the sandbox server that can talk to external environment but the external environment cannot talk to ther sandbox server.

Note; Before now, when your run

curl ifconfig.ca

you will see that jumpserver does not have any public ip address because it was created initially under a private subnet but after i have connect the Nat gateway to the private subnet of jumpserver,  and i run

curl ifconfig.ca

the jumpserver now have a public ip address.

Basically, if you ping sandbox server using its private IP address after opening icnp port on the server, it will ping but if you ping an external server FROM SANDBOX like googl.com, it will not still ping. This is because the sandbox only have a private ipaddress and also a private subnet that is not connected to a NAT gateway

If the NAT gateway is connected to the private subnet, the private routetable would be able to route one way communication from the private subnet to external environment. It can now ping external evn but sexyerna; evn cannot ping sandbox or dbserver except via webserver or jumnpserver.

NOTE: tHE SANDBOX OR DATABASE can ping or talk to the external environment using NAT gateway but cannot receive traffic directly from external evn except via a jumpserver or webserver.

This became possible because we connect our private subnet with Nat gateway that route traffic from internet gotten from public subnet to our private  subnet.

NOTE: It is very important to know that you dont always want to configure NAT gateway to route internet to your dbserver or app server because it is not secured to do that. You only connect NAT gateway temporally to your dbserver when you want to perform some certain task that would require the use of the internet like when you want to do a yum server upgrade or update but as soon as you are done doing that, you have to delete the Nat gateway for security reason. You resources are highly secured.  needs absolute security.

NOTE: You can read the aws note that prof shared in the first class of this program.

PLEASE ASK YOUR MATE TO SEND YOU PROF VIDEO THAT TALK ABOUT SSH PRIVATE KEY RECOVERY. TO CONNECT TO SERVVER WHOSE SSH PRIVATE KEY CANNOT BE FOUND. uSING ANOTHER SERVER SSH KEY TO CONNECT USING EBS VOLUME

IQ

What is the difference between NAT gateway (NGW) and Internet gateway (IGW)?

Answer:

Internet Gateway (IGW) allows instances with public IPs to access the internet. 

NAT Gateway (NGW) allows instances with no public IPs to access the internet.

What is the difference between ROUTE-TABLES (NGW) and Internet gateway (IGW)?

Answer:

Route Tables (RT). This controls how traffic you be routed to/from the subnet.
RT is usually attached to subnet

INTERNET GATEWAY (IGW): This is a logical device like your internet router that controls how traffic should be routed to/from the internet. This is what allows external traffic into or out our VPC. it also what permits us to communicate with the internet

IGW is usually attached to vpc

VIDOE 99

WHAT IS PROVISIONING?

Provisioning is the process of creating and setting up IT infrastructure, and includes the steps required to manage user and system access to various resources. Provisioning is an early stage in the deployment of servers, applications, network components, storage, edge devices, and more

In other words, Provisioning refers to how we can create or settup our IT infrastruiction and also how we can manage the resources in our IT resources.

Provisioning of a server. This means to create, to setup and manage a server.

note: When talk about creating a VPC that would host all our resources, we are talking about creating a private network in an aws public network that would host our resources. 

VPC is a apart of your IT infrastructure.

NOte: what determines the number of resources in a subnet depends on the subnet prefix (n).

HOW TO CALCULATE NUMBER OF RESOURCES TO BE CREATED IN VPC

let say your ipv4 cidr is  12.33.41.0/23

23 is called a subnet prefix (n)

to know number of resources that can be created. You can manually calculate it by using the equation

2^(32-n)

32 is the number of bits for ipv4 because each integer in ipv4 has 8 bits. ipv4 has 4 integer and when you multiply 8 and 4 places, you have 32 bits.

since n = 23

2^(32-n) = 2^(32-23) = 2^(9) = 512

This means that 512 resources and 512 ips can be created in the above VPC with cidr block 12.33.41.0/23.

HOW TO ARRANGE YOUR SUBNET IPV4 CIDR BLOCK

If you start with cidr block

if VPC --- 256 resources (IP Addresses) 
      10.0.0.0/24 

  Subnets 
 If first is  Subnet1 --- 64 resources

 then the cidre block will be in the below format

       10.0.0.0/26  [.0, .1,... .63 ]

       meaning the last intger ranges from .0, to .63   you cannot say .64 bcos it will overlap

 If second subnet is  Subnet2 --- 128 resources. 

       10.0.0.64/25 

That means that the last integer will start with 64 of the resources of the first subnet (it will the number of resoirces of the first subnet)

If third subnet is  Subnet3 --- 256 . it measn that the prefix is /26

That means that the last integer will start with 64 of the resources of the first subnet and 128 resources of the second subnet (it will the number of resoirces of the first subnet and resources of the second subnet)

10.0.0.192/26 

You can also do these calculator online by using what is called ipv4 cidr block calculator and a subnet calculator

PAIRING OF DIFFERENT VPCs

Server in VPC1 and server in VPC2 communicate with each other with their public ips, since the servers are in different VPC network (private network).  For security reasons, this would not be advised. It is not secure for severs to communicate with public ip. The use of their private ip would be more secured.

For server in VPC1 to communicate with a server in VPC2 with private ip, a PAIRING connection of the two VPCs has to be done for VPC1 and VPC2 to have a private connection as if they are in the same VPC network.  This is called a VPC pairing.  It is 2 different VPCs. That means it is different network. Both are still being hosted by aws public cloud.

BUT

When you own a company and you decided to own a datacenter by having an on-prem infrastructure that has a server like the mongo-server (mongo server is an example of a db server). 

Now, you want your mongo server in your on-prem infrastructuo to have a private communication or connection with another server like webserver in another vitrual private network. In this case, a virtual private network called VPN is required.  This is because an on-prem infrastruction and a cloud infrastruction are involved. That is connecting an on-prem datacenter or infrastructure with a private ip in order to have a private secured communication.


HOW CAN WE CONFIGURE VPC PEERING AND HOW IS VPC PAIRING IMPORTANT?

Understanding aws VPC PEERING.

1. aws vpc pairing is used to set communication between 2 or more aws VPCs. That VPC1 and VPC2
2. vpc peering can be done inter or intra regions. This is means that the peering can be done between vpc of different region. VPC1-us-east-1a-north virginal can also peer with vpc2-us-west-1a-California (inter), VPC1-us-east-1a-north virginal and VPC1-us-east-1a-north virginal peer (intra)
3. You can connect a single VPC to multiple VPC.
4. with vpc peering, there is no need to have IGW or VPN connections between VPCs.
5. VPC peering is free and 100% securd. no need for vpn

in aws, under what zONAL resources does ec2-instance falls on.


     AWS Global Resources
          IAM is a global resource

        When you create your aws account, it is created under global resources.

          IAM stands for Identity and accesss managemeont.  


    Regional Resources:  This are resources that are in regions
         VPC = us-east-1 = NV = When opening vpc, you usually select regional. This means that vpc is a regional resource.

         s3 is a regional resource
         EFS is a regional resource
     
  Availability zones resources[DATACENTERS]: This are resources that are in AZ or. datacenters. Resources like:
         Subnets - us-east-1a. You usally select AZ when creating a subnet
         ec2-instances: You usally select AZ when creating a instances
         EBS: You usally select AZ when creating a ebs


CREATION OF A PEERING CONNECTION BETWEEN VPC1 AND VPC2

Peering can be inter region or intra region

Inter region Peering: This is when you create a peering connection between 2 VPCs (VPC1 and VPC2) that are in different region.


Intra region Peering: This is when you create a peering of a private connection between 2 VPCs (VPC1 and VPC2) that are in the same region. This is, the vpc1 and vpc2 peering are in region like us-east1-north virginia.

vpc peering in aws is free and very secured while. It is used to connect via priuavte ip of 2 vpcs.

VPN do a private connection between aws public cloud with another cloud computing company like GCP or with your on-prem infrastructure. It is not free. you would have to pay.

The essence of VPCs is that you can have different VPCs for departments like finance, marketing, database, accounting. with VPC peering,  These vpcs can be able to privately communicate with each other.

HOW TO DO VPC PEERING.

Let us lauch an amazon linux2 server in the vpc1 that we created. name as app-web-server, let the AMI be ubuntu,  instant type be t2 micro, select the vpc1, select public sebnet1b, enable auto-asign public ip, use the following script and paste in in thecopy the below script

#!/bin/bash
sudo yum install httpd -y  
sudo service httpd start  
sudo chkconfig httpd on
sudo echo "This is my web server" > /var/www/html/index.html  
echo successful

paste it in the user data, security group name (wsg1), ssh select from custom,  let us found out the vpc cidr block of the vpc1 that was created. Remember the vpc and its subnets was created by default because with ticked the VPC that comes with subnet and others. let us use the vpc1 cd block to allow traffic within vpc1. The vpc1 cidr block was 10.0.0.0/16
. then paste the vpc1 cidr block to where the cidr block is that will alow local traffic within vpc1, after custom and then write local access within the network on description(this means server or resources access is within the vpc network. If another external env wants to access this vpc, he will not be able to do that because the firewall has already spoken about who can access the server via its cidr block. Even when you use a VPC PEERING OR VPN, external env can not still run traffic to this vpc because it is meant for only internal env ), for this vpc to receive traffic by another external env., you need to add another rule by selecting ssh and my ip under source and put system admin access, add another rule to open the apache webserver you want to install via user data by selecting http, and select everywhere under source and write user access under description, let us still add another rule and select NFS for efs ( so that we can sychronize data between servers), select custom and paste vpc1 cidr block (so that we can only synchronize resources that are within vpc1 only  10.0.0.0/16) and then launch.

We have created a resource called ec2-instance in VPC1

Our major focus is to establic a private secured connection between VPC1 and VPC2 by creating a peering connection.

lets say the cidr block for vpc1 is  10.0.0.0/24

lets say the cidr block for vpc2 is  172.0.0.0/20

NOTE: Whe you are doing a peering connections. You have the dos and dont.

1. one of the principal requirement is to avoid cidr block overlapping.

QUESTION: When can 2 VPC cidr block overlap?

ANSWER: 2 vpc cidr block can overlap when a resources (servers) created in the 2 VPC have the same cidr block. 

When 2 vpc have the same cidr block, an overlap will occur and that will result to an ip address conflict. it will be impossible to peer them.

lets say the cidr block for vpc1 is  10.0.0.0/24
 
and the ip address of the server is 10.0.0.25

lets say the cidr block for vpc2 is  172.0.0.0/20

another cidr block for vpc3 is  10.0.0.0/24

and the ip address of the server is 10.0.0.25

VPC1 and VPC3 have the same cidr block and so it is impossible to peer them. The servers also have the same ip address. So we must avoid ip address overlapping.

VPC1 and VPC2 have different cidr block and so it would be easy to peer them.

Let us use moba xterm to access our jumpserver that we created before which is running in VPC2 and try to do a private connection with appserver-vpc1 that we just created which is running in VPCi.  Let us see if we can establish a private connection between them using their private ip addresses.

NOTE: if you want to access by ssh into the appserver-vpc1 via the junpserver-vpc2 using public IP of the appserver-vpc1 by running the command

sudo ssh -i ssh-keyName ec2-user@public ip

it will work if only the firewall ssh source selection is EVERYWHERE but if the vpc2 cidr block is pasted or selected on the ssh source of the jumpserver, you will not be able to connect except you add another rule by selecting the ssh source to be everywhere.

in the case of using the private ip of the appserver-vpc1, it will not even connect all. Even if the source is everywhere. this is because it is a private ip in 2 different vpc network (vpc1 and vpc2)

This means that the configurations that takes place in the firewall security or in the security group is very important.

HOW TO PEER VPCs (VPC1-VPC2 peering connection)

Let the VPC1 be BOA Maryland branch (boa-MD) and VPC2 be BOA new-york branch (boa-NY)

1. If vpc2 is requesting for a private peering connection from vpc1

Go to your VPC console, click peering connection, put boa-peering-vp1-vpc2 in peering name, select vpc2 under local vpc to peer with, select my account on account, select this region for region (if the 2 vpc are from the same region like us-east-1a-NorthVirginia), select vpc1 under vpc ID accepted, and then click on create connection.

2. Then vpc1 can accept the peering by going to peering connection console, click on action and then click on accept request.

Now, For the jumpserver-vpc2, if you ssh into appserver-vpc1 using public ip, it will work but when you use the private ip, it will not work. The main purpose is to establish a private connection.

3. edit the route in the route-tables

a. Go to the vpc console, click on route tables, click on vpc1 public route tables, click on action, click on edite route,  click on add route, copy and paste the vpc2 cidr block on the cidr block portion and click save route. Also go to the vpc1 private route table and paste the vpc2 cidr block

b. Go to the vpc console, click on route tables, click on vpc2 public route tables, click on action, click on edite route,  click on add route, copy and paste the vpc1 cidr block on the cidr block portion and click save route. Also go to the vpc2 private route table and paste the vpc2 cidr block

Both the private and public of vpc1 and vpc2 route tables are now connected.

If you use jumpserver CLI to ssh into appserver using private ip, you will see that it will now connect,

VIDEO 107

Try and go through the last video of Devops end to end deployment. The video before this very video. There is a diagran that illustrated vividly what devops end to end is all about.

Initially when we were looking at storage system in aws. we spoke abouth

ec2-instance ebs, efs, s3 

NOTE: ec2-instance has a block storage called ebs (elastic block store) volume which you can increase the size of the ebs volume when creating ec2-instance or also can even decide not to increase. You also also create another ebs volume and attach it to the ec2-instance that you have created.   ebs is a block storage.

for EFS (elastic file system), This is a file storage, you do not have to increase or expand the size of the file because the file will expand automatically when there is a growth or increase in your data or information. It is highly scalable.

For s3 bucket, This is an object storage. you dont also decide to expand the size of your storage because it is highly scalable as well. What is the determinaing factor here is how your objects stored in your s3 bucket is being accessed. If it is what is frequently accessed or not. That would determine the class of your s3 bucket. in s3 bucket, we have the stanstard class,  and others. please find out. for revision purpose.

The objects that can be stored in an s3 buckets are pdf files, audios, logs, videos, artifacts, images, movies, etc.

You can create a bucket called Movie-Netflix and then store a movie series like RAMBO. and then apply what is called versioning to break the movie into series or episode like rambi1, rambo2, rambo3 etc.

aws ELASTIC LOAD BALANCER (ELB) AND AUTO SCALING

aws ELB authomatically distributes incoming applications traffic accross multiple targets such as aws-ec2 instance, containers, or ip addresses in your VPC

ELB is a load balancing resource available in aws cloud that is used to route incoming user requests/traffic to multiple servers, containers, or ip addresses in your VPC

aws ELB can also distributes incoming user applications requests or traffic accross multiple targets in different AZs in the same region.

NETWORKING

How to create your resources in your VPC

1. s3 bucket: s3 is a regional resource and so it can be created directly inside your VPC. Your vpc is also a regional network.

2. ec2-instance: To create your ec2-instance in vpc, you would have to create what is called public subnet and a private subnet because instance is an AZ resource and so it can not be created directly inside the vpc without a subnet. ec2-instance or server is created inside public or private subnet.

Let us assume you created a server called ngynix by installing Ngynix inside the server in public subnet where external users can route traffic or requests and then you created 5 app-server where ngynix can route traffic in a balance way depending on how the ngynix server was configured to distribute the requests from external users. You have to configured the rules in Ngynix.

Question; since Ngynix is the only access to which external users can indirectly get response from the appserver, what happens if the Ngynix server is in-responsive or dead?

Answer: It would be a serious problem if the Ngynix server goes down. This is why it is better to utilize the ELB in aws where the ELB is a PAAS (platform as a service). Using ELB you will not be bothered about installing any multiple webservers because everything has already be installed in the ELB from aws. This is a higher guarantee that your load balancer not failing because aws is managing the load balancing for you.

What aws does at the backend to ensure that their ELB would not fail is to create multiple load balancers or webserver in different AZs in the ELB so that if any of the webserver failed, the other one will automatically takes over to ensure maximum availability, continuation and reliability of load balancing when routing traffic to multiple targets.

NOTE: ELB can route incoming external users requests to multiple targets in different AZs but in the same region.

Let us assume that we have 2 vpcs that is created in us-east-2 (Ohio) region and public subnet is created in us-east-2a AZ and the private subnet is created in the us-east2b Az. You can also decide to create another public subnet in us-east-2c and also created private subnet in us-east2d. This is to increase a high available to eradicate subnet failure and increase reliability.

There are different kinds or type of load balancer: These are 

Networking load balancer, application load balancer, classic load balancer

a.  Networking load balancer: 

This is used to route connection requests or traffic to it target like ec2-instances, microservices and containers within the same VPC based on ip address protocol. 

It route connection request to just one target. the target could be an ec2-instance or microservice or ip protocol.  It maybe be many instances etc but must be the one type of target like jumpserver1, jumpserver2, jumpserver3 etc.

Another separate type of target cannot be here so long its a network load balancer. It only used to configure only one type of application

It supports TCP and UDP protocols

It is capable of handling millions of requests per second while maintain ultra-low latencies.
It mainly operates at the connection level (called layer 4).

Layer 4 is called transport layer which it used to transport data from source to target, according to the OSI model table. Network load balancer fall on this category.

The OSI model different layers are

Physical layers      	- layer 1
Data Link layer     	- layer 2
Network layer       	- layer 3
Transport layer      	- layer 4   = network load balancer
Session layer       	- layer 5
Presentation layer  	- layer 6
Application layer:   	- layer 7 	= application load balancer

IQ 

Explain your experiance in layer 4 or layer 7 supports?

Answer: 

According to the OSI model. Layer 4 is a transport layer where data requests are being routed from source or user to one target. It route connection traffic to just one target. 

layer 4 supports tcp/udp

My experience with layer 4 includes configuring Network load balancer to route connection traffic to one backend target like ec2-instances or containers or microservice within the same VPC. 

Network load balancer are very fast and can handle millions of requests per second. It supports tcp/udp

tcp= transmission control protocol
udp= user data protocol

According to the OSI model, Layer 7 is an application layer where users requests are being routed to multiple targets like ec2-instances, containers, ip addresses etc. Example of layer 7 resource is called application load balancer which supports http and https

layer 7 supports http/https

Application load balancer supports layer 7. It supports http/https. Example of application load balancer is apache webserver.


b.  application load balancer: 

This supports htpp.https protocols. It operates on a requests based level (called level 7). It is used to route content requests or traffic to targets like ec-instances, containers, ip address etc. 

It routes requests based on the content of the requests.

It is a content based routing

It is used to route requests or traffic to different types of applications based on the content of the requests. 

HOW APPLICATION LOAD BALANCER WORKS

Let us assume we have the below targets 

webapp, boa-app, td-app etc.

How it works is that once application load balancer receive requests, it will first of all process the request and try to determine the destination of the request, based on its content and then route the traffic or request to the exact application that it was meant to be

It supports host based and path based routing

Target: target is the endpoint or where traffic is routed to. Targets are expected to be grouped. A group of targets are called targetGroups.

TARGET GROUP

TargetGroups; You can create a targetGroup like

-  Webpps (This can be a group that contains of all your web applications like webapp1, webapp2, webapp3). webapps here is a targetGroup containing just one target

-  boa-app (This a targetGroup that contains of all your boa servers or applications that are running your boa applications).   boa-apps here is a target containing sub-tagets which are all the servers in your web application

-  td-app (This can be a targtGroup that contains of all your td servers or applications like td-app1,td-app2, td-app3 etc, that are running your td applications).

what load balancer will do is to route traffic to health targets

Healthy targets means targets that are up and running. ELB will be able to know healthy targest when it perform a health check on the target

Generally, For an application load balancer to be able to route traffic to multiple targets, it must first of all listen to the requests or traffic via its port (port 80) or port 443 depending on the type of ELB it is. IT will listen with port 80 and ports 443 because its support http and https respectively.

For ELB to route traffic. A routing rules would have to be determined also.


Load balancers can also perform what is called HEALTH CHECK. That is

ELB can determine if a server or application deployed is healthy or not.

HOW CAN YOU PERFORM A HEALTH CHECK?

Using a server as an exmaple. Normally, you can perform a health check on a server by

a. performing health check on server. 

This can be done by using pinging methods

If you want to ping the server that you are currently on via CLI, Make sure change the hostname to any name you want and then switch user to ec2-user and then ping the hostname that you used. let say i want to change the hostname to app. I will run

sudo hostname app
sudo su - ec2-user
ping app

If you want to ping another server. just run

ping servername or ping serverIpAddress

To ping another server would be possible if the server pinging protocol is opened (ICMP) in the server firewall`

b. performing health check on application.

This can be done by using a curl. by running the command

curl publicipadress:portNamuber

For example if you want to perform health check on your tomcat server. Tomcat server listens on port 8080. If you are inside the tomcat CLI, you can run

curl localhost:8080 or curl tomcatIpadress:8080 

CREATION OF ELASTIC LOAD BALANCER IN AWS.

Let us create ELB inside our VPC in aws. 

Let us first of all create another VPC in another region like canada central-1 by typing VPC and then select ca-central-1 at the region console. click create VPC, select "VPC and more" and not VPC only, so that the VPC will be created along with subnets, IGW, NGW, RT etc, name the VPC to be something like fintech-vpc, change the vpc cidr block to what you prefer, select 3 under subnets so that it can create 3 public and 3 private subnets, just follow the step that we used in creating the first vpc1 that we created.

After you have created the VPC, 

Let us create a 1 server and name it webapp1, let us use amazon linux 2 as AMI or OS, select t2 micro, select fintech vpc, select public-subnet1 for this server, create security group, open ssh with everywhere as source, select customTCP for tomcat by putting 8080 for port number and then select anywhere for source, select customTCP or http and put 80 as port number and also select anywhere as source, let us install tomcat server while launching the server by pasting the below script on the userData and also select allowed tags in metaData enabled. 

Please make sure you download the latest version of tomcat from the internet and replace it with the one that is in the script below because what is in the script is an old version. This can be done by pasting wget https://dlcdn.apache.org/tomcat/ in google, click on the higher version and then on archive, the newest version will be the last box after the question mark sign. click on the box, you will see tar file and zip file, let us choose the zip file which is located below which is the one that end with just .zip  copy the zip absolute path and then

#!/bin/bash
# Use this script to install tomcat in rehat servers
echo assign a hostname to your server
sudo hostnamectl set-hostname tomcat
sudo su - ec2-user
# install Java JDK 1.8+ as a pre-requisit for tomcat to run.
cd /opt
sudo yum install git wget -y
sudo yum install java-1.8.0-openjdk-devel -y
# Download tomcat software and extract it.
sudo yum install wget unzip -y
sudo wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.78/bin/apache-tomcat-9.0.78.zip   
sudo unzip apache-tomcat*
sudo rm apache-tomcat-9.0.78.zip 
sudo mv apache-tomcat-9.0.78 tomcat
sudo chmod 777 -R /opt/tomcat
sudo chown ec2-user -R /opt/tomcat
sh /opt/tomcat/bin/startup.sh
# create a soft link to start and stop tomcat
sudo ln -s /opt/tomcat/bin/startup.sh /usr/bin/starttomcat
sudo ln -s /opt/tomcat/bin/shutdown.sh /usr/bin/stoptomcat
sudo starttomcat


select allow auto-assign public ip address, tick enable resourced based ipv4, and the click launch.

Let us launch another server called webapp2 and select fintech vpc that i created, select public-subnet2, install tomcat server using the shell script above and also do the same configuration i did to webapp1 and then launch.

Note, you can use another server CLI to health check the 2 tomcat server that you just created now to see if they are healthy or if it is up and running by curl method,  by running

curl tomcatIpaddress:8080

If it run, it means that the tomcat server is healthy and it is up and running

If you also run

ps -ef | grep servername

above can also be used to check if a server is healthy or not

TARGET GROUP CREATION

Let us create a target group by going to the ec2 console and click on target groups, click on create a target group, tick instances under SPECIFY A TARGET TYPE, write webappsTG as the name of the target, put tcp protocol under protocols (trhis is bbecause network load balancer supports tcp protocol. The reason why it is a network load balancer is simply because it was 2 tomcat server that we installed inside the 2 instances that we just created which now makes these instances to become a tomcat server, which is just one target but in 2 phases (tomcat1 and tomcat 2). When the resource is just one type of resource but has many parts, the network load balancer would be used to route or transport connection traffic to these targets (tomcat1, tomcat2 that we created) that has just one type of target called a tomcat server), select port 80 under tcp, select fintech-vpc under VPC, select tcp under health check policy, tick traffic port under port, tick the target or instance that you want, enter or type in the port number of the target(the selected instances. the targets or the 2 targets that we created listens on port 8080 because they are tomcat server), theN click on INCLUDE AS PENDING BELOW but since the two instances are the same tomcat which listens on port 8080, let us tick the two isstances, type in the port number of the instance ), theN click on INCLUDE AS PENDING BELOW, and the click on create target group. This means that you have created a target grooup fpr webapps1 and webapps2.


CREATTION OF NETWORK LOAD BALANCE AND ATTACHING TARGET GROUP TO IT

Let us place the target group webappsTG behind a load balancer.

scroll down under the ec2 console to newtwork and security and click on load balancing, click on load balancer, click on network load balancer, click on create a load balancer, click on create under the network load balancer, under load balancer name put nlb29, tick internet facing under scheme, select fintech-vpc as the VPC, selects the subnets by ticking ca-canada-1a (you know we chose canada central as region), ca-canada-1b, and ca-canada-1c, select tcp and port 80 because it is a network load balancer, select webappsTG under default action (webappsTG is our target group) and click on create load balancer.

Give some time OR WAIT FOR FEW minutes for the loadbalancer to provision or to set up or create because it usually taks few minutes to create. It takes few minutes. then try to access your tomcat server in the internet browser by copy the ipadress:portnumber on browser and it will display a tomcat webpage.

NOTE: When you use a private subnet to create your load balancer and configure it to your targetGroup, Your targets can not be accessed on your internet webpage, because it was private. You must choose a public subnet for it to be able to accessed via the internet.

If you mistakenly create your load balancer via a private subnets that does not have route to the internet. you can edit the private subnets by adding route to the 3 subnets that you chose (that is, click on the fintech-vpc, click on subnet, click on the subnet1a, click on action, click on edit route, click on add route, select everywhere as source and select internet gateway so that it can allow internet to go in and out of the VPC) and save, also so go to the private subnet1b and do the same,  go to private subnet1c and do the same. If you try to access the target or tomcat server on your browser, it will be work.

WHAT CAN MAKE YOUR SERVER UNHEALTHY?

A. Wrong ports
b. When your server is not running (whene is ist down
c. When your application refused to start)
d. insufficient resources like CPU
E. The firewall or security group can make a srerver to be unhealthy if not properly managed.

IQ

Explain your experiences you have using loadbalancer and problem resolved?

Answer: Thank you for this question. We have used a network load balancer in our environment to route connection traffic from users to our backend or target applications like several tomcat-servers. 

One of the problem that was encountered in one of our deployment was that one of the tomcat targets was unhealthy. Based on this, the load balancer could not route traffic to the unhealthy traffic. LB only route traffic to healthy target.

After troubleshooting, it was discovered that a wrong port was passed on the tomcat server. instead of the tomcat server to listens on port 8080, port 80 was selected. based on that wrong port selection, our tomcat server could not pass the health check.

We also had a situation whereby a load balancer was created via a private subnets that does not have route to the internet. Because we try to access the server on the internet browser, it failed, what we did to resolve tjhe issue was to go to the exact vpc, edited the private subnets by adding route to the subnets and add internet gateway to all the subnets involved and then it became accessible by internet browser.


LET US DEPLOY APPLICATION INTO THE TOMACT SERVER WE JUST CREATED.

The command used in deploying application to tomcat server or application servERvia CLI is the scp command

scp = secure copy   

To deploy application to to tomact server, we use the below command

scp -i sshkey absolutePathToTargetDir/builtApllication/ec2-user@tomcatIpaddress:tomcatHomeDirectory/webapps

scp -i sshkey Jenkins/workspace/projectName/target/*war/ ec2-user@tomcatIpaddress:/opt/tomcat/webapps

After the deployment of the app to tomcat server. You can access the app through an internet browser by adding the application/url of the load balancer and the paste it on the web browser. That is

load balancerDNS/deployedArtifacts     in the form of below

http://nlb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com/maven-web-app/

nlb29-b1f1f9b9767b784a.elb.ca-central-1.amazonaws.com/99.79.193.51:8080

You can get to the load balancer DNS by clicking on the load balancer name, scroll down you will see where it says DNS name and then click on the copy sign.

The above will give you or show you the content of the artifacts.

http://nlb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com    

above is the DNS of your load balancer     

DNS means Domaine Name System

CREATION OF DNS:

Below are the domaine name that you can create

aliasName also called cName records: This route traffic to another domaine name and some aws resources

a-records: This route traffic to an ipv4 addresses 

ptr-records: This maps an ip addresss to a domaine name
etc

In aws, there is a service called Route53 where you can buy a domaine name. The Route53 can be found under service in aws console

Route53: Prof created and bought some domaine name or hostname in Route53. This is as follows below.

HOW TO CREATE DOMAINE USING ROUTE 53

When you are inside Amazon Route53, click on get started

1. You can tick register hostname if you want to register a new hostname
2. You can tick transfer hostname if you have a hostname in somewhere else before and not in aws and you want to transfer the hostname to aws.
3. You can configure health check by ticking configure helath check
etc

Let us create a host name by ticking register hostname, typing in the hostname you want to register in route53 on choose a domain name and register it.

CREATION OF SUB-DOMAINE NAME CALLED  RECORDS

If you already have a domaine name that you bought,  or someone bought like that os prof., and you want to create a sub hostname on an hosted zone. This means created a sub hostname on already existing hostname in aws like the creation of an aliasname also called a records, 

1. First of all create the domaine on your hosted zone in route53 by going route53 console and click on dahsboard, click on hosted zone, click on create hosted zone, type in the existen domaine you under domaine name, select public hosted zone, click on create hosted zone. 

dominionapps.net

 Above are already existing domaine that was bought by prof on route and so we are re-suing it as our hosted domaine name and we will create a sub-domaine name from it and then alias it to our nlb.

2. CREATION OF SUB-DOMAINE NAME OR A RECORD NAME

Let us make dc.dominionapps.net to be a sub domaine of dominionapps.net and also an aliasname to nlb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com 

by clicking on the domaine name that you have created, click on records, enter app or anything you like on the records name, select a records in the record type, tick aliasname (to create aliasname), under route traffic select alias to netwoork load balancer, select ca-canada as your region and the click on the region box to bring out or identify the load balancer dns that you have created in that region, click on create record.

Now, you can access this sub-domaine or record in your internet browser by

dc.dominionapps.net

Above will show your load balancer but when you add a path to the application like

dc.dominionapps.net/maven-web-app

the content on the web application will display

NOTE: Any target that is found not healthy can be drained or brought down or deleted and create another healthy target.

ADVANTAGES OF LOAD BALANCER

1. Load balancer route or distribute traffic or requests evenly or equally accross multiple availabily (AZs)

2. Load balancer blocks or allow traffic or requests based on conditions such as ip address

3. Load balancer has a preconfigured protection that blocks common attacks like SQL injection

4. Load balancer Monitors web traffic or requests and protect web application against malicious traffic. 


What happens whn you type landmark.com in web browser?

What happens when you do that is that, it will go through what is called DNS resolution to search for a server or ec2-instance leading to landmark.com

NOTE: Go through the TERRAFOM video. very important because you get paid higher when you can use terraform to provission (create or setup) your resources in aws instead of using the aws console to provision your resources. For example, provisoning an ec2 instance with a written code inside a file.

Terraform is an infrastructure as code (IaC)

what is terraform?

Terraform is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently.

HashiCorp Terraform is an open source infrastructure as code (IaC) software tool that allows DevOps engineers to programmatically provision the physical resources an application requires to run. 

VIDEO 108

NOTE: LOAD BALANCER AND DIFFERENT FROM ELASTIC LOAD BALANCER.

Load balancer is the type you actually create and configure by yourself by installing a load balancing software like apache or Nginyx in an ec2-instance in aws that will help to route traffic to the backend applications.

Elastic load balancer is the load balancer that is provided to us by aws where you do not have to bother about the load balancer in terms of security or availability or others because aws would be the one that will manage the load balancer.

ELB falls under PAAS (platform as a service)

It helps us to route traffic or requests to backend multiple application server.

ELB can be layer 4 support (network load balancer) or layer 7 support (application load balancer)

NOTE: backend multiple application servers can be grouped in a group called TargetGroup

ELB can route traffic to each of the target or app. server in the TargetGroup

laod balancers listens with what is called listener. They listens with either a port and a protocol.

End-users traffic or requests has to go through a load balancer and the load balancer will now route the request to the backend app server.

In our last class, we created a network load balancer and make sure that if you want to access the web applcation that was deployed to tomcat, you would have to pass through the network load balancer to access the web application. This in the form of

http://nlb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com

Above url or DNS of a load balancer will route traffic to tomcat server or takes you to a tomcat server. It is the Network Load Balancer URL or DNS that was created

To access the web application via NLB that was created, we can use

http://nlb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com/maven-web-app/ 

The above is not the best option when it comes to users. You can not send the above url to customers or end-users for them to be able to access the web application.

This is why a domaine name was bought in our last class from an aws DNS service called route53 where you can buy a domaine name or configure a sub domaine name also called record name under already existen domaine name.

We configured a name on already existen domaine name (app.dominionapps.com) and then set it as an aliasname to the network loand balancer url that was created. That is 

dc.dominionapps.com = http://nlb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com

So that it will be more easy and ideal for user to type in dc.dominionapps.com/maven-web-app/ in their webpage and then it will route the the request or traffic to 

http://nlb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com/maven-web-app/

The network load balancer is used to only route traffic to a single targetGroup like TDapp1, TDapp2, TDapp3, TDapp4.. These target falls into a single Group by creating a targetGroup like TDappTG and the attach Dapp1, TDapp2, TDapp3, TDapp4 to the group because its just TD app servers.
      BUT
The Application load balancer can route traffic to multiple target or target-groups like (TDapp1, TDapp2, TDapp3, TDapp4), (boa.app1, boa.app2 , boa.app3), (bmo.app1, bmo.app2, bmo.app3, bmo.app4). Each bracket represents a targetGroup. We have about 3 TargetGroups here. 

The Application load balancer can route traffic to the 3 TargetGroup but NLB cannot. It can only route to just one TargetGroup

The network load balancer listens with a protocol called tcp or udc protocol but application load balancer listens on a protocol called http and https and their port numbers are 80 and 443 respectively.

APPLICATION LOAD BALANCER

The Application load balancer route traffic to multiple targetGroups like (TDapp1, TDapp2, TDapp3, TDapp4), (boa.app1, boa.app2 , boa.app3), (bmo.app1, bmo.app2, bmo.app3, bmo.app4). Each bracket represents a targetGroup. We have about 3 TargetGroups here. 

The Application load balancer can route traffic to the 3 TargetGroup but NLB cannot. It can only route to just one TargetGroup

CREATION OF APPLIOCATION LOAD BALANCER

Application load balancer has higher level of security than the Network load balancer because ALB would require a security group or firewall whemcreating ALB but in creating NBL, no such thing like securityGroup

To save cost, let us delete our network load balancer and create our application load balancer.

Once you delete a load balancer, you can also delete the load balancer targetGroup but make sure you delete the LB before deleting the TG.

TO CREATE APPLICATION LOAD BALANCER

Click on create under application load balancer, type in appLB in the load balancer name, select VPC you want to create it from, select different AZs for high availablity, you can use the security group that was issued to us or you can create a custom security group by clicking on create security group, enter ALB-sg as the security group name, enter allow htpp under description,enter port 80 and select everywhere under sources , also add another rule by clicking on add rule, select https,  enter port 443 and also select everywhere on the source so that we can receive traffic from users anywhere in the world and then click on create security.

Go to where you are registering you application load balancer (ALB), under listeners and under protocols it will tell you port 80 and then click inside the targetGroup box to see the targetGroups that you have if you already have existen targetGroup you want to use but if you dont have, you can click on create target to create a new target group. You can delete the Target group we created before when we were initiating NLB.

click on create target, tick ec2-instance, targetGroup name as webappTG, select the protocol which is http or https, select the right vpc you want to use for the targetGroup, 
put / under health check (health check always starts with the /  this means that you are instructing the root to run the health check)

For example, In a tomcat server on CLI, when you run the command

curl -v localhost:8080

Doing the above means you are running a health check on tomcat server. You are checking if the server is up and running.

The health check was passed because tomcat server was running on it.

When you run

curl -v localhost:8080/maven-web-app

The health check was also passed because there was an application that was deployed to tomcat server called maven-web-app. You informed the root to run health check on maven-web-app

But if you run

curl -v localhost:8080/maven-web

The health check will fail because there was no such application like maven-web that was deployed to tomcat

under register target under instance, select the target or application-instance that you want to add to the target (you can add multiple targets on this target group), type in the port number of the targets you selected under ports for selected instance (if you have more than one instance or targets, always put comma between each port number for each instance that you selected. like example 8080, 80, 443 etc)(but Prof selected 2 application server and a jumpserver as the targets but only put port 8080 that represents the 2 tomcat application server), 

click on include as pending below, then click on create traget group,

Go to where you are registering you application load balancer (ALB), select the target group that you have just created, click on create load balancer. Give it a bit time to fully provision or fully setup. Like maybe 2 minutes.

NOTE: You can always create a target group for each listener that you select. The listener that will select here was http on port 80 and we created a targetGroup for that listener.

NOTE: A Listener is the part that shows the protocol and the port number in which the protocol listens on 

If you now click on load balancer console, you will see that our appLB is now created.

If you click on LISTENER, you will see that the http port 80 listener forward traffic to the targetGroup (webappTG) that we have created.

If you click on description, you will see our ALB DNS like below

alb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com

if you want to access the web-application via the alb, you can now run

alb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com/maven-web-app

ofcourse you can create a domaine name or sub domaine also called record on an existen domiane name and route the application load balancer DNS to the domaine or sub-domaine name that you bought, using alias. That is 

alb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com ----> app.dominionapps.com

app.dominionapps.com/maven-web-app   

This will make it more appealing to endUsers.

NOTE: If you want to use the domaine name as alias to the ALB DNS, you would leave the alias name blank with filling in anything so that it will only the domaine name that would be registered as an alias name. After creation, it will now be

dominionapps.com ---->alb29-6388dc50379f696b.elb.ca-central-1.amazonaws.com 

dominionapps.com/maven-web-app   

This will make it more appealing to endUsers.

If you copy and paste the above web application URL in a browser, it will display the web application content via its target or an applicationServer and also display the ip address that is assigned to that target. If you refresh, it will route traffic to another target which implies the ip address would change.

Let us create another targetGroup and call it myappTG, also select http 80, follow the same procedures with the first one but select just one target(application instance), and then click on create target group. 

Also, let us created the 3rd tagetGroup and call it tdappTG, and follow the same procedures with the first and secondTG but select the 3 targets and click on created targetGroup. 

NOTE; We are talking about the targets or application instances that was created by prof. It was 3 of them. Yours maybe different.

Now when you go back to your targetGroup, you will discover that we now have 3 traget group.

also, If you copy and paste the above web application URL in a browser, it will display the web application content its target or an applicationServer and also display the ip address that is assigned to that target. If you refresh, it will route traffic to another target which implies the ip address would change.

The implication of this is that it will obstruct user when accessing and registering something with the application and then maybe there was a network interruption and it refreshed, it will make the user to start the registration all over again because traffic has be routed to another app server.

How would you solve this back and forth situation?

Answer: This problem can be solved using what is called STICKY SECTION.

HOW DO ALB ROUTE TRAFFIC TO MULTIPLE TARGET GROUP?

You can buy a domaine name in aws route53 like

  simonlegah.com    
  olu.com   
  osasdevops.com   etc

applicationS        TargetGroupS
 maven-web-app      webappTG
 tdapp              TDappTG
 myApp              PayPalappTG


We can created a-records or alias name to route our ALB URL or DNS to domaine name that you bought and also create a sub domaine name or alias out of the doamian name that was bought . That is 

HOST
  	dominionapps.net   

SUB-DOMAINES
  	td.dominionapps.net
  	pay.dominionapps.net  

You can create  multiple subdoamine name and configure them in such a way that each esub-domaine name is attached to or forward traffic to a particular targetgroup. 

Host  
Rule:
  dominionapps.net/maven-web-app           ---Forward TO: webappTG
  td.dominionapps.net/maven-web-app        ---Forward TO: TDappTG
  pay.dominionapps.net/maven-web-app      ---Forward TO: PayappTG


This means that anytime a user type dominionapps.net/maven-web-app  in his browser, the ALB will route the user to webappTG  targetGroup

anytime a user type td.dominionapps.net/maven-web-app  in his browser, the ALB will route the user to TDappTG  targetGroup

Also anytime a user type pay.dominionapps.net/maven-web-app   in his browser, the ALB will route the user to TDappTG  targetGroup

According to prof., he has already created records or sub-domaines for dominionapps.net that route traffic to webapp-TG

Let us create records or sub-domaine for td.dominionapps.net and pay.dominionapps.net by using the same procedures in creating a record or alias.

NOTE: You can also edit a record or sub-domaine name by ticking to the sub-domaine or record under record name console, click on edit record, and do the neccessary edit. 

Like what prof did, he edited one of the record that is alias to NLB and want it to be alias to ALB, click on the edit record, select the alias to ALB that you have created, select the region that you want, select the ALB DNS and click on save.

After the pay.dominionapps.net was created by prof, he paste the url or DNS on web browser and a tomcat server was accessed.


LET US DECIDE HOW TRAFFIC SHOULD BE ROUTED. in the following order below

Host header
Rule:
  dominionapps.net        ---Forward TO: webappTG
  td.dominionapps.net      ---Forward TO: TDappTG
  pay.dominionapps.net    ---Forward TO: PayappTG

The above means routing traffic by host-header to the targetGroup

Path
Rule:
  dominionapps.net/maven-web-app           ---Forward TO: webappTG
  td.dominionapps.net/maven-web-app        ---Forward TO: TDappTG
  boa.dominionapps.net/maven-web-app       ---Forward TO: boaTG

The above means routing traffic by path to the various targetGroups, this means that is end-user type dominionapps.net/maven-web-app, td.dominionapps.net/maven-web-app, boa.dominionapps.net/maven-web-app on the webpage, it will route or forward to webappTG, TDappTG and PayappTG respectively.

NOTE: also note that each targetgroups contain one or more targets. 

For example:

TDappTG may have more than one target like td1, td2, td3

Them if a user type in td.dominionapps.net/maven-web-app on his browser, the traffic maybe be routed to td1 and if the user refresh the page, he can be routed to td2.  or maybe the system was idle or out of network and decide to come back and re-enter the td.dominionapps.net/maven-web-app DNS in his web browser, he can be routed to td3. So every of your new entrance to the DNS may route you to different td instance or server or target inside the same targetGroup

Let us decide how traffic should be routed by path in our ALB that we just created to our corresponding targetgroup as show above.


Note by default dominionapps.net/maven-web-app can still route traffic to webappTG, TDappTG and PayappTG because the 2 sub-domaines or records were created from dominionapps.net

LET US DETERMINE HOW THESE RECORDS OR SUB-DOMAINE CREATED AND HAVE ALIAS TO OUR ALB TO ROUTE TRAFFIC TO OUR DIFFERENT TARGET-GROUPS BY ADDING A RULE IN OUR ALB VIA LISTENER

Let us now add a rule and route their traffic to another targetGroup as stated above.

1. first of all configure ALB listeners to forward traffic to the target Group you want to forward to. 

Since ALB listens on http and https protocol, let us listener create a listener with http protocol and route or forward traffic to 3 target group that we have, since we have 3 targetGroup here. If you already have a listener that route traffic to just one targetGroup, you can edit the listener and adding more target group to it if thats your aim by clicking on the lsitener protocol (http), click action and click edit listener,  click on add target group, seleted the first targetGroup webappTG under forward to targetGroup, selected another targetGroup TDappTG, and selected the third TDappTG and then clicked on add and then click on save changes.

2.  ADDING RULES 
Go to your ALB, click on the protocol (http), scrol down, click on add rule, type in the name of the rule (alb-td), click next, click on add condition, select path under choose condition, copy and paste your alb DNS subdomaine/weba-application like td.dominionapps.net/maven-web-app click on confirm, click next, select the target group such alb dns would route traffic under forward to target (select tdTG), click next, select 1 under priority and thyen clickn next, and the click create.

To add rule that we route traffic from alb-boa DNS to boa targetGroup (boaTG), also go to to your ALB, click on the protocol (http), scrol down, click on add rule, (alb-td), click next, click on add condition, select path under choose condition, copy and paste your alb-bao DNS or subdomaine/weba-application that is boa.dominionapps.net/maven-web-app click on confirm, click next, select the target group that one to receive traffic from boa-alb DNS (select boaTG), click next, select 2 under priority since we have select 1 in our previous and then click next, and then click create.


 (  NOTE: dominionapps.net as the principal dommaine would always route traffic to webappTG, TDappTG and PayappTG  by default which may not be editable. You can always edit the sub-domaines and targetGroup

PROBLEM OCCURED AFTER THE ABOVE CONFIGURATION

When prof try to re-access pay.dominionapps.net/maven-web-app and td.dominionapps.net/maven-web-app after the routing to other targetsGroup, it failed.

SOLUTION

Prof went back to the ALB listener and deleted the listener  of http protocol by CLICKING ON THE LISTENER and then clicked on delete LISTENER, then he REcreateed another listener of protocol http port 80 and then clicked on add listener or you can edit the first listener abd adding more target group to it if thats your aim by clicking on the lsiterner, click action and click edit listener,  click on add target group, seleted the first targetGroup webappTG under forward to targetGroup, selected another targetGroup TDappTG, and selected the third TDappTG and then clicked on add and then click on save changes.

and then he reconfigure the rules again by click on insert rule, click on add condition, click on path paste pay.dominionapps.net/maven-web-app under (is), click on add action, paste the targetGroup(PayappTG) under trafficGroup (this means that if someone type pay.dominionapps.netmaven-web-app, the traffic would be directed to PayappTG) and then click save, added another rule by click on insert rule, click on add condition, click on path paste td.dominionapps.net/maven-web-app under (is), click on add action, paste the targetGroup(tdappTG) under trafficGroup (this means that if someone type td.dominionapps.netmaven-web-app, the traffic would be directed to tdappTG) and then click save.

TAKE NOTE: when prof now browse with the sub-domain on the web browser, it opened using the host header but didnt work using the path. HE SAID HE WOULD GO BACK TO IT  )

SECURITY

(HTTP/HTTPS)

http protocol listens with port 80

https protocol listens with port 443

http is not always a secured option. HTTPS is the secured version which has a lock sign. 

THERE IS WHAT IS CALLED MAN IN THE MIDDLE ATTACK

The man at the middle attack is the SSL/TLS certificate

When you type a url that starts with https, it gives you an option like SSL/TLS 

SSL/TLS is an encrypted communications between end-user and the application server

SSL(secured sockets layer). This is an encryption that is more modern and secure replacement

TLS (transport layer security). This is an encryption that protect data sent over the internet or a computer network.

Companies uses https to transmit their data and so when user wants to access the application server or backend server via http, it would not be processed because the backend server or app. server only suport SSL/TLS

When users enter their info or data like username and password to login to companys login page like td, their data or info is encrpted, 

HOW IS DATA ENCRYPTED?

user data is encrypted by the use of a software that can hide the data. That software is called CERTIFICATE. 

in aws, this certificates are SSL and TLS certificates.

This certificates are issued by a certified certificate authorities.

We can use multiple certificates issuing comnpanies. An example of certificates issuing comnpanies iks aws certificate manager.

Note: As soon as the encrypted data gets to the td bank or company, the data would be decrypted so that td application server can understand the data and respond accordingly.

We can use the aws certificate manager to request a certificate like SSL and TLS certificate for our DNS

Type in certificate manager in aws search and click certificate manager. You will see wherew it says request a certificate, import a certificate (this means that if you have a certificate with another certificate compamny, you can import it to aws)

LET US REQUEST FOR A CERTIFICATE IN AWS

inside certificate manage, Click on request a certificate, tick on request public certificate, paste your domaine name or sub-domaine that you created before under fully qualified domaine name, you can click on add another domaine name in order to paste another sub-domain name that you created, you can all the sub-domains that you created or *.dominionapps.net (this will cover all the domains that has .dominionapps.net that you have created or that you will create in the future)

NOTE: you can only be able to request for a certificate if actually you own the domain name that need a certificate for. aws need authentication from you to ascertain if actually you own the domain name before they can issue out the certificate.

scroll dowm, select DNS validation under validation (you can also choose email validation if you so desired), click on request, you will see that it says the certification is pending validation, TO VALIDATE the domain name or sub domain name, click on the url url that is highlighted in blue behind the domain name, scroll down, click on create records in route 53, the domains would be ticked and then click on create.
 Again, you can not request for a sub-domain certificate if you are not the owner of the certificate.

Since you now have a certificate, you can now go to create a listener of the ALB that has a secured option like https protocol.

click on the load balancer (ALB), lick on add listener  under listener, select the prtocol to https, under action type, tick forward to target group, seleted the first targetGroup webappTG under forward to targetGroup, selected another targetGroup TDappTG, and selected the third TDappTG ), select elb recommended under security policy,  select from ACM (amazon certificate manager under default ssl/tls certificate. You can also import a certificate from somewhere else by clicking import a certificate), click on the search sign in the next box to search for the the certificate of the domain, and then click add.

then you can now go to access the domain or sub that you have its certificate on the internet by starting with https. That is 

https://dominionapps.net/

You will notice that it now have a lock. meaning it a secured site which hackers can not just hack easily because of the SSL/TLS certificate that is used to encrypted every info or data that is being entered by end-user.

IQ

Explain your experience with SSL/TLS certificate

ANSWER:

We use SSL/TLS certificate to ensure that our end-user data or info are encripted or hidden by utilizing aws certificate manager to request for a certificate for our hosted domain that we bought in route53.

HTTP REDIRECTION OR FORWARD TO HTTPS (SSL/TLS )

When someone is trying to access our company application from an unsecured protocol like http . for example  http://dominionapps.net/, you can configure the listener in away that it will redirect the user to a more secured protocol like https (https//dominionapps.net/).

Let us know how it works.

Go to your ALB, scrol down, click on the protocol (http), click on the rule-name you wamt to edit or redirect, click on action, click on edit rule, tick redirect, enter https port number (443), click save changes. Do the same to every rule-name that you have under http and it will route user traffic in https.

( CREATE NEW RULE CONDITION FOR HTTP

click on the http listener that you have created, scroll down and click on add rule, type in the name of the rule (alb-td), click next, click on add condition, select host header under choose condition, copy and paste your alb DNS subdomaine/weba-application like http://td.dominionapps.net/ click on confirm, click next, select the target group such alb dns would route traffic under forward to target (select tdTG), click next, select 1 or 2 or 3 or 4 depending on which one would be accepted under priority and then click next, and the click create.

Go to our ALB listener, tick http protocol, click on view/edit, click on insert rule or click on edit rule (that looks like a pen), click the edit sign on the domain you want to edit, delete the condition that was there before and then click on add condition, click on host header, delete the forward action that was there before and click on action, click or redirect, select the protocol (https) and type in the port number to the protocol (443), click save,
This means that you are coverting unsecured traffic to a secured traffic).

when you type in http://dominionapps.net/ on your web browser, you will see that it becomes the secured version  https://dominionapps.net/   redirection has taken place.

STICKINESS

This is a section that determines how long a user can stay idle to a particuliar server or target before being loged out and therefore may need the user to start all over again. You can enable to disable the stickiness.

If you are refeshing your page and the ip address keep changing, this means that your system is moving you from one target to another if you have more than one target inside the targetGroup.

This means that the stickiness is not enabled. If the stickiness is enabled, the user would remain on that particular server or targat that he was assigned to, even if he refresh the page or when the system or page is idle, the ip addresss will not change and the page will not be loged out.

HOW TO CONFIGURE STICKINESS

1. configure stickiness on your listener where the tragetGroup is.

First of all, go to your listener tick protocol http and delete the unsecured protocol and living protocol with the secured https.

select protocol https where the target group was configured, click on edit sign, click on th edit sign of the targetgroup, and delete the forward action that was there, click on add action, click or tick on group stickiness, tick on enable, select minutes and type in 5, then click on update.

2. Configure the stickiness in your TARGET GRUOP
GO TO WHERE ALL YOUR TARGET GROUP ARE, click on the targetGroup that you want, click on attributes, click on edit, tick sickiness, uder stickiness duration select the number of minutes or days that the user can be idle without being logged out and losing his info. let us just select minute and type 5, click on save.

If you now enter url of the domain that stickiness is enabled and giving, you will notice that the ip address no longer change when you refresh. this is because you assigned 5 minutes as the stickiness duration for the user to stay put on a particular target or instance. 
This implies If the server stayed idle more than the 5 minutes, the ip address will change MEANING THE TRAFFIC WOULD BE ROUTED TO ANOTHER TARGET IN THE SAME TARGET GROUP.

  STICKY SESSION
STICKINESS or sticky seesion is a term that is used to describe the functionality of a load balancer to repeatedly route traffic from a client to a single target or application server, instead of routing the traffic across multiple targets. 

It enables the user traffic to stay on a single target until its session is over during idleness.

This means that stickiness would make the user traffic to remain in the same target for a set period of time based on the time it was configured on, when the user is idle.

NOTE: Elastic Load balancer by default can route traffic to just a single region. If you want a load balancer to route traffic to multiple region, you must integrate the load balancer with ROUTE 53 which we have seen.

If have seen it when we created a application load balancer and also bought and created a record set or domaine name in route53 and then integrated them together by aliasing or forwarding the ALB DNS to route traffic to the route53 records set or domaine name or sub-domaine name that we bought so instead of introducing the ALB DNS to end users, we can now inroduce the domaine name to users and what they enter that domaine name on their web page, it ALB will route the traffic.

AUTO SCALING GROUP (ASG)

Auto scaling ensure that there is enough amount of backend server or application server to handle the amount fo traffic or load coming for the end users.

Auto scaling has the capcity to automatically create another application server or instances if there is a sudden increase in traffic or request by users in order to prevent the present ones from being overloaded.

Auto scaling can also terminate a server if there is a shrink in traffic from users in order to save cost.

Auto scaling can detect when an apllication server or instances is unhealthy (not running) and therefore it would terminate the unhealthy instances and create another one to replace it. 

Amazon auto scaling can be configured in such a way that it can use multiple AZS so that if one availability zone is not fuinctioning, it can automatically launch another instances in another AZs to replace the failed AZs

Amazon s one of the company that can render auto-scaling services.

auto scalling is just about the tendencing of your infrastructure to expand automatically if there is an increase in user traffic and shrink if there is a decrease in user traffic.

Auto scaling saves us from provisioning or creating more instances or application server manually when there is an increase in user traffic and terminate instances when there a decrease in user traffic.

TWO KIND OF AUTO SCALING

1. Horrizontal scalling: This is the process of scaling where more target or targets are created automatically to be able to  manage user increased traffic.

For example: If the existing taregt"s memory is 4GB and another target with 4GB or less is created and another more target with memory is created making 3 targets. This is horrizotal autoscaling because it is expanding its size horrizotally

2. Vertical scalling: This is the process of scaling the existing target size or capacity is being increase in order to be able to handle user increased traffic. 

For example: If the target's Memory was initially 4GB, then automatically it increases to 16GB. This is vertical autoscaling because it is expanding its size vertically.

NOTE: If you are manually provisioning servers or targets or application server, there is the tendency that you would spend more money thank employing auto-scaling method.

AUTO SCALING PLAN OR POLICY

Using the creation of appServers as an example, you can set your auto- scaling policy as below

1. MINIMUM = 2   This means that you have set a policy to create 2 appservers as a start in your deployment infrastructure.


2. DESIRED = 2  This means that you have set a policy you desired just 2 appservers but if traffic incease, the 2 appservers can auto-scale vertically (increase in memory. in size) if need be because you preferred just 2 appservers

3. MAXIMUM = 100  This means that you have set a policy that your appservers can auto-scale horrizontally (creating more servers) with not more than 100 servers should incase there is too much increase in user traffic.

4. Creation of more servers or targets would be based on CPU or MEMORY usage of the initial servers:

5. The auto-scaler must starting with the creatiuon of a minimum number of instances and not the maximum 

For example:

Create instance or appServer if

i   CPU usagaE of the existen appServers exceeds 80%   or 
ii  Memory usage of the existen appServers exceeds 80%

LAUNCH TEMPLATE

1. Create a lauNch template or configuration

2. use your launch template to create your auto-scaling group.

3. once the auto-scaling group is created, the auto-scaling group would provision or create the desired/required number of servers. 

The auto-scaler would make use of the auto scaling plan or policy and the launch template to provision the required servers

NOTE: Launch Template is used to create your auto-scaling group.

Note: 

1. You to create a launch template on an existing instance or application server that is up and running instance

2. You can also create or configure launch template along with launching a server or instance and also install an application inside the instance.

The reason why it is neccessary to set or create or activate the launch template is because of that fact that auto scaling group will have the capacity or ability to be able to launch new server. Without the launch template and autoscaling plan, new instances can not be created b y autoscaling in time of crisis.

CREATION OF INSTANCE OR APPSERVER TEMPLATE AND AUTO SCALING BROUP

To achieve a very high availability of my appservser, an instance launch template and auto-scaling group has to be created

INSTANCE LAUNCH TEMPLATE CREATION

Let us create our appserver LAUNCH template by clicking on the instance or server that we want to use and that is already running, click on action, click on image and template, click create template from instance, name the template (like, appserver_Temp1), enter the version number that you want (you can type in 1 as your version number), select the AMI that was used to create the instance initially, click on create launch template.

CREATTION OF AUTO-SCALING GROUP

Go to the ec2 console, scroll down, click on auto scaling group, click on create auto scaling group, namae it (like, maven-web-app-ASG), , select the launch template that you just created,  select the VPC that you want it to be, select the AZs (You can select all the AZs here for more availablility), select instant type requirement (let select t2-medium), tick no balancer (you can deceide to attach it to a load balancer but for now, let us ignore load balancer), type in 2 as your minimum capacity and desired capacity and then type in 20 as your maximum capacity, type in app-asg under key name, under scaling polcy, click on target tracking scaling policy, select average CPU utilization, you can type in 75 under target value, click on create launch auto scaling group.

NOTE: Once you go back to your aws console and click ec2 and the click instances, you will see that 2 instances called app-asg has been created. This is because, we did enter 2 minimum instances to be created when determining the auto-scaling policy in the auto-scaling group and so upon the lauch of ASG, 2 instances will automatically created.

TO TEST RUN IF YOUR AUTO SCALING IS WORKING

NOTE: When you delete the instances that was provisioned by ASG, the instances would automatically be re-provisioned or created by auto-scalling group. 

This is because auto-scaling group auto-perform a health check of the server that was created by auto-scaling group and if any of the server is detected not running, the auto scaling group will automatically launch the exact number of instances that stopped running as a replacement from any available AZs.

The from any available AZs uses launch template to create or re-create instances

LET US CREATE ANOTHER LAUNCH TEMPLATE FOR TOMCAT SERVER

That will be used to create and start tomcat

go to isnatnce console and click on instance template, click on create template, name the template (let us call it tomcat-Temp), enter 1 as template version, select redhat as the  AMI or OS, select t2 micro as the instance type, select or create your private key, under network, select your public subnet you want it to be (subnet that is in your VPC before), select or create security group (you can select the existen one that you created before), click on advance details, copy the below scipts

#!/bin/bash
# Use this script to install tomcat in rehat servers
echo assign a hostname to your server
sudo hostnamectl set-hostname tomcat
sudo su - ec2-user
# install Java JDK 1.8+ as a pre-requisit for tomcat to run.
cd /opt
sudo yum install git wget -y
sudo yum install java-1.8.0-openjdk-devel -y
# Download tomcat software and extract it.
sudo yum install wget unzip -y
sudo wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.78/bin/apache-tomcat-9.0.78.zip   
sudo unzip apache-tomcat*
sudo rm apache-tomcat-9.0.78.zip 
sudo mv apache-tomcat-9.0.78 tomcat
sudo chmod 777 -R /opt/tomcat
sudo chown ec2-user -R /opt/tomcat
sh /opt/tomcat/bin/startup.sh
# create a soft link to start and stop tomcat
sudo ln -s /opt/tomcat/bin/startup.sh /usr/bin/starttomcat
sudo ln -s /opt/tomcat/bin/shutdown.sh /usr/bin/stoptomcat
sudo starttomcat

and paste the script under userData that will launch and startup tomcat, clikc on create launch template. This means that you have created a launch template for tomcat that you just lauch and startups via userData.

The above means that you have created a lauch template for an ec2-instnace of a tomcat server. When you create an auto scaling group wand select the above launch temoplate, an instance with tomcat application installed in it will be launched.

LET US CREATE ANOTHER AUTO SCALING GROUP FOR TOMCAT SERVER

Go to the ec2 console, scroll down, click on auto scaling group, click on create auto scaling group, name the group as tomcat-asg, select the launch template that you just created (tomcat-temp), select the version to be 1, select the vpc that was in your launch template, select all the AZs, tick attach to an existing load balancer, tick none under scaling policy, type in 1 as your Minimum, Desire and Maximum capacity, tick target tracking policy, select CPU UTILIZATION, select application load balancer request count per target, type in 5000 under target value (this implies that if the user requests get to 5000 requests, more instance or tomcat server would be created), under key name, type in tomcat as the name of the server that was created, click on create auto-scaling Group


NOTE: 

1. If you terminate an instance upon which have launch template and an auto scaling group, another instances will provisioned or set up automatically by the auto scaling group by the support of the launch template.

2. If you stop an instance or if an instance stop runnibg, if the instance that stopped ruuning have launch template and an auto scaling group, auto scaler will first of all terminate the server because it has the tendency to auto perform health check and then another instances will provisioned or set up automatically by the auto scaling group to replace the the one that was terminated

Note: It usually takes a bit of time to initailize or run the process for the termination and provisioning of the instance

NOTE: When companies are creating their auto scaling group, they make use of what is called Golden AMI  which is also called Golden
Amazon Machine Image.

WHAT IS A GOLDEN AMI?

A Golden AMI is an amazon machine image (like redhat, ubuntu, amazon linux 2) that is usually selected when creating an instance in a way that such instance would have a script on the userdata that would also install an application and other dependencies on the instance upon the launch of that instance. This is what is called Golden AMI because other application and dependencies is also be used along with the AMI to to lauch the instance.


GOLDEN AMI OR MY AMI (amazon machine)

This is the machine image that contains the default operating systmen (AMI) like ubuntu, redhat etc and other configurations or installations like have a script on the user data of an instance that would install an appllication like tomcat upon launch of that instance

For example, we just launched lauch an instance with AMI and script that would install and startup tomcat server. This is an example of a Golden AMI.

AWS SERVICES THAT WE INTENDED TO COVER IN THIS COURSE

1. ec2-instances
2. storage (s3, ebs, efs)
3. ASG
4. VPCs
5. IAM  =  watch the video for IAM

Watch terraform video 

QUESTIONS ASKED IN CLASS

1. WHAT DETERMINES SECURITY INSIDE A VPC

Firewall at the subnets level and firewall at the instance level determines the security inside a VPC

The firewall at the subnet level is called NACL (network access control list). It is stateless (meaning it is not intended for any specific instance)

The firewall at the instance level is called security Group. It is stateful (meaning it is intended for a specific instance)

2. WHAT DETERMINES FLOW OF TRAFFIC INSIDE THE VPC

a. The IGW  (internet gateway) manage the traffic or route internet or external traffic from or to VPC . this is attached to VPC

b. The route-table manage the traffic or route traffic from or to subnet. This is attached to subnets.  The route-table is usauuly atatched to a subnet

THe route table can be configured to be both public and local route table (this means it has a private ip address and also a private ip address), also route table can be configured to be a local route-table meaning it only has a private ip address. Can extend and receive traffic locally (insde the same VPC)

c. NAT gateway (NGW = network aaddress translation gateway). This is usually attached to a public subnet (the subnet that has route to the internet). With the help of this NGW, resources in the private subnet can have a one way internet access and should be terminated after use, for security reasons.

HOW DO YOU ESTABLISH A PRIVATE COMMUNICATION BETWEEN 2 VPCs?

A company can have more than 1 VPC. For a company to have a private communication between its VPCs, it will have to use the service of peering to establic the private communication.

HOW DO YOU ESTABLISH A SECURED OR A PRIVATE COMMUNICATION BETWEEN AN ON-PREM INFRASTRUCTURE AND VPC?

A VPN is used to establish a secure and private communication between and on-prem infrastrure with a VPC.

On-PREM INFRASTRUCTURE means in-house or on site infrasture where you have to be on site or where the insfrastructure is located or housed for you to utilize the server. You cant work from home here.

VPN = virtual private network

NOTE: auto-scaling is not supported by network load balancer. It is only supported by application load balancer.

VIDEO 110 BEGINS BELOW

introduction to IAM


Let us start with s3 for the intro. for a recap of what we have done before

S3 buckets: This is an aws ogjects storage like the below

   videos, audios, pdf, images  
   artfifacts, snapshots,   
   host static websites   


 How to secure data in s3: 

 s3 bucket has to be secured because of the vital objects that are stored in the bucket

 S3 can be secured using

    encryption
    nacl: network access control list
    policy: we can create bucket policy

Global resources/services : Eaxaples of aws Global resources/services are
  Route53  
  IAM 
Regional resources/services :  Examples of aws regional resources/services are
  efs 
  s3   
  vpc  
  asg  
  elb 
Availability zone resources : Eaxaples of aws AZs resources/services are
   ec2 instances  
   ebs volume 
   subnets     

 Let us create a bucket in S3

 click on create bucket, name it (note, a bucket name must be globally unique. it must be a name that has not been existence anywhere before), you can click on settings from existen bucket, you can tick enable ACL so that any other of your aws accounts can be able to access the bucket that you want to create if need be but  us tick ACL disable for now, tick block all public access for now, tick enable versioning, click create bucket.

NOTE
Assuming a compnay has 10 aws accounts
Tesla:
  10 aws accounts 
     userName: 
     password:

Question: Do you think is neccessary for a company to have several aws accounts?

Answer: It is neccessary for a company to have several aws accounts for 

- Dissaster recovery
- Security  backup  

For you to have multiple aws accounts, you need tyo create what is called organization  

AWS Organisation:  This enables us to have multiple aws accounts.  With aws organization, you can be able to use it to manage all of your accounts.

For example. We we have the below accounts

  userName= tesla1
  userName= tesla2 
  userName= tesla10

tesla1 have about 50m users. If a hacker hack the tesla1, we can recover all of our users if we have backed up whats is in tesla1 in tesla2. This what is called disaster recovery.

For example, If you have an s3 bucket in account tesla2, you can be able to access the s3 in tesla1 or tesla10 if you configure what is called NACL in your s3

nACL = network access control list

upload an object to your new created s3 bucket by clicking on upload, and the select the object or drag the object and click upload.

You can rename the object name by clicking on the object in your bucket, click on object action,  click on rename object, erase and enter new name, click save.

To access the object, click on the object name and copy the url and paste it in your borwser. if does not open, it means that you block external or public access of the bucket. 

You can always grant public access to your object by ticking block all public access disable of the bucket configuration by clicking on the bucket name, click permissions, click edit under block all public access, untick all the box and then save. 

Then try to access the object on the browser. you will see that it you still can access. This is because the object itself is not public.

For every object to be fully be make public is for you to once again click on the bucket name, click on permissions, click on edit under object ownership, tick ACL enabled, tick i acknwoledgaed ACL will be restore, click save.

tick object name, click object action, click make object public, click make public.

If you now click the bucket name and then click the bucket url, you will seee that it opened.


AWS SECURITY WITH IAM

IAM is a service in aws that is used to determine who can access our resources in aws and what they are allowed to do.

IAM =  Identity and Access Management

This has to do with 

AUTHENTICATION
What is usually needed for authenticatiuon isusername and password or token, 
multi-factor authentication (this may be your a code sent to your phone or email)

AUTHORIZATION; This determins what you can do with the resources in your environemnt. this cut accross

  Roles
  Policies

IAM CREATION

For you to manage access to your environment, you can create the following

Access management

    User groups like admin group,QA group, Engineers, managers, directors, developers group etc]
    Users
    Identity providers
    Account settings

 Above fall under authentication.

Let us go to our aws, click on IAM, click on user group under access management, click on create group, name the group (can call it admin), click on create policy, click choose a service, type on the search what you want this group to be able to do (you can type ec2, and click on ec2 = meaning you want IAM to manage ec2 instances), if you want to grant full access to members of this group then you can tick on all ec2 actions, scroll down and click resource, tick on all resources,  you can add additional PERMISSION by clicking add additonal permissions, click on choose a service, click inside the search and type eks, tick all eks action, scroll down, click on add additional permissions, type in IAM on the search and click it, tick on all IAM actions, click next, put admin as policy name, put ADMIN FULL ACCESS for description, click on create policy, while still on the prgress of created a group, tick the policyt name, and then click on create group

in IAM console, click on user group,  you will see the group that we just created.

Let us create users by clicking on the group name (admin), click on create user, click on add user, name it like class29,  tick on aws management programatic access (but in the new experirnce of aws, there is no programmatic access, it is just console access but you can also make it programmatic by ticking on generate access key instaed of ticking password. This means that you can now access it with password using GUI and also access key using CLI), tick i want to create an IAM user, click on attach existing policy directly, you may type in ec2 as te policy that you want to search to see the polciy in ec2, you can select ec2-readOnlyAccess or other permission as the preferred policy or role that the person can do, you may type in s3 on search and select s3-readOnlyAccess, click on review, click create user and then copy the access key and the secreat access key

upon creation, this type of user that has programmaticl access comes with access key and a secret access key

You will see that the user we just created has no console access (meanin, password created for the user). It was a programmatic access with access key and secret access key.

Let us create another user with a console access

click on the group name (admin), click on create user, name it like rafiu,  tick on both console access, tick i want to create an IAM user, tick custom password (create a password) and type in you new password (it is imperative that the user you are creating this console access to would also need to create a new password for himself), tick on attach existing policy directly, you may type in ec2 on the search, let us select ec2-readOnlyAccess, click on create user,  copy the url of this user sign login above so that rafui can be able to login with that user.  this is, something like below

https://066611333659.signin.aws.amazon.com/console

TO LOGIN IN TO THE USER YOU HAVE JUST CREATED

Console access means you can access your resource via gui

You can share the login user url of your aws IAM that you just created with your employee called rafui and give him the username and password.

https://066611333659.signin.aws.amazon.com/console

rafui will click the url or IAM aws user login or put it internet browser, enter your username and password.

After he has logged in, he maybe be required to change to his password if that permission was given to him to do by admin. He would see that he can only read the other ec2-instances you created outside of IAM because you ticked ec2-readAccess permission but if he try to create a new ec2-instance, it will fail to create. This is because he has not been given the permission to be able to create ec2 instance but when you tick the permission that talks about ec2 full access, then he can be able to launch an instance.

When he also try to access s3 bucket, he cannot see any bucket created or anything inside s3, no bucket. This is also because you did not choose or grant any permission to him on s3. he can only would be able see nd access the bucket inside s3 when the permisiion is granted or ticked or chosen to him.

NOTE: CONSOLE ACCCESS, gramting console access to a user means the admin is granting the user to be able to access permitted resources in a GUI manner (graphical user interface)

If you sign out from the console acces as a user, amd you want to sign in as an admin or root user, click on sign in as root user

PROGRAMMATIC ACCCESS, gramting programmatica access to a user means the admin is granting the user to be able to access permitted resources in an CLI manner (to access using a command line)

HOW TO GRANT MORE ACCESS OR PERMISSION TO USER IN IAM

go to IAM by typing IAM in your search, click users, click on the user name (rafui), click on add permission, click attach existing policy, type in ec2 on the search filter policy, tick ec2-FullAccess, you can also type in s3 on the search and tick s3-readOnlyAcess, and click save permission

If you go back to refresh the page of the user rafui and then type in ec2 on aws console, he will see that rafui can view ec2 instances and can perform any work on ec2 becauae he hyas been granted full access in ec2. If rafui type in s3 in aws console and he would be able view the bucket, read or view what is in the bucket but can not do any modification on the bucket or create any new bucket.

AS IAM-ADMIN, HOW TO CREATE PROGRAMMATIC ACCESS FOR A USER WITH A CONSOLE ACCESS.

Let us use class29 as an example.

click on class29, click on create access key, tick command line interface (CLI), tick on confirmation, click next, enter class29-ak as description, click on create access key, copy and save both the access key and the secret access key.

AKIAQ7ASYJIN4AKVXIDG    acess key
V6QasqNINSddQb5ec6ahZYOmt25w29OfGZp8UfuE   secret access key
ca-central-1      region

click done and click continue.

HOW TO ACCESS RESOURCES AS IAM USER THAT HAS ONLY A PROGRAMMATIC ACCESS

A user with only prgrammatic access would be able to access resources as IAM user via CLI

go to the CLI environment like linux or gitbash where aws is installed. If aws is installed in your gitbash, great.  For you to know if aws is installed in your gitbash is for your run the command

aws        

and press enter key and if it says argument are required, it means aws is installed here

If it says  command not found, it means that aws is not installed in your gitbash or your windows.

paste the below in your internet browser to download the setup

https://s3.amazonaws.com/aws-cli/AWSCLI64PY3.msi

after it has download, click on the setup and install this version of aws. after you have done that, go to your gitbash tpo verify if aws is running and run 

aws --version

and it will give you the version of aws that you just installed.


to list the buckets in your aws, run the command

aws s3 list

If it tells you access key is needed or . It means you were not permitted to list the bucket

For you to be able to list the bucket in your aws s3, you can run a configuration command by running

aws configure

Then it will ask you of your access key and soon as you put your access key, it will ask you of your secret access key and oce you put in the aws region, then put in your aws region and once you out in your aws region (ca-central-1), it will ask you default output format, also put the region ca-central-1 and then press enter key and then you now in aws IAM user class29

 if you now run

 aws s3 ls

 It will list all the buckets you have in your s3 and you would be able to read them because you activated readAccess for s3.  

 class29 IAM user can only access aws resources from the command line

 You can delete any of the permission that was given if you need to by going to permission, rick the exact permission and delete.


AS IAM-USER, HOW TO CREATE PROGRAMMATIC ACCESS FROM A CONSOLE ACCESS.

 IQ

 You maybe asked to found or create your access key and secret access key by an interview after they have created console access user for you  and you were told to access resources like s3 bucket using a programmatic access means via command line. They may want to know if you can create your access key, as an IAM user.

 ANSHER. For you to access resources via CLI, you will need to first of all create access key and secret access key (programmatic access). 

For the above to be successful, the admin need to grant the user IAM-Full-Access so that the user can be able to create access key and secret access key from console access

 Using RAFUI for an example, Rafui was created in a console access. To create access token and secrete access key for Rafui, all you need to do is for your to go the your user by clicking on the user logo at the up where you user logo is, click on security credentials, scroll down and click on create access key, click on command line interface, tick on confirmation, click next, enter kim-ak as description, click on create access key, copy and save both the access key and the secret access key.

 if it tells you that you need permsion to create access key, it means you do not have the permision. You can tell the interviewer that you have not be granted the permission to actually create an access key

 Access key creation permission can be granted by the IAM admin by going to the IAM admin user and click the users, tick on rafui user, click on add permision, type in IAM on the search, tick on IAM-FullAccess, click review, click add permission.

 When the rafui user get back to create access key, he would be able to do that by clicking on create access key,  it will create access key and secrete key

 copy the access key and secrete key

 TO ACCESS IT WITH CLI

 Go to your gitbash, and run

 aws configure

 put in your access key
 put in your secret access key
 press enter
 press enter  again
then run the command

aws s3 ls

it will list the buckets you have in s3

CREATION OF A BUCKET IN S3 USING COMMAND LINE

Let us create a bucket on the s3 by using the commamnd

in rafui gitbash environment, run

aws s3 mb s3://bucketName     

Note: Every bucket name using CLI must starts from s3://bucketName  

mb = make bucket

Let say the bucket would be called class29ccc

aws s3 mb s3://class29ccc 

If you run this command it says access denied, it then means that rafui do not have the permission to create bucket. Eventhough he has IAM-fullAccess but does not have Amazon-s3-FullAccess.

if it says, An error occurred (BucketAlreadyExists) when calling the CreateBucket operation: The requested bucket name is not available., it means that the bucket name you used has been taken by someone else. You know that names must be unique and must not have been used by someone else before. try another name until it get thorugh


For it to have full access, go back to IAM admin, tick rafui, click on add permission, click attach existing policy, type in aws on search and select Amazon-s3-FullAccess and then save the changes.

Go back to CLI and re-run the command

aws s3 mb s3://class29ccc

You will see that the bucket has been created.

Note: The bucket that has been created using aws s3 mb s3://bucketName come with BLOCKED PUBLICK ACCESS. This means that the buckent and the object that would be inside the object can not be visible accessed externally or via the internet browser when you paste it on the browser or click on the url
For you to make it public the object inside the bucket visible, you would first of all by click on the bucket, untick BLOCK ALL PUBLIC ACCESS, and secondly, go to OBJECT OWNERSHIP and enable the ACL 

LET US COPY FILES TO THE BUCKET class29ccc WE JUST CREATED

To list or see the buckets you have in your s3, run

aws s3 ls

-if clifford is part of the bucket, let us list what is in clifford by running

aws s3 ls s3://clifford

-if pom.xml is one of the file in clifford bucket, then let us copy the file to class29ccc by running

 aws s3 cp s3://clifford/pom.xml s3://class29cc

 To verify this run

 aws s3 ls s3://class29cc

 You will see that pom.xml file is seen.

You can also copy files or even artifacts from your download or anywhere the files is in your personal computer to the bucket that you just created

NOTE: The transfer of files, artifacts and other objects form one bucket or dictory to another bucket or directory is called data migration using the cp command.

Let us say you have a built artifacts in your music directory in your laptop with absolute path maven-webapps/targets/app.war   

to copy such artifacts to s3://class29cc

cd into music and run

aws s3 cp maven-webapps/targets/app.war s3://class29cc

It will copy the app.war file to s3://class29cc

To see the app.war in the s3://class29cc, you run the command

aws s3 ls s3://class29cc    you will see the app.war file there

FOR YOU TO COPY A FILE FROM A BUCKET TO THE ENVIRONMENT YOU ARE CURRENTLY WORKING

let say you want to copy a file from s3://class29cc to the music environemt that you are right now, you can run

aws s3 cp s3://class29cc/fileName .

if the fileName in s3://class29cc is kim.xml, you run 

aws s3 cp s3://class29cc/kim.xml .

You will see that file kim.xml is no longer there   if you run

aws s3 ls s3://class29cc

DELETE OF BUCKET USING CLI

If you want to delete an bucket, the bucket must be completely empty withoput any object before you can successfully delete a bucket. 

You can delete an object inside a bucket by running

aws s3 rm s3://bucketNAME/ObjectName             rm= remove

You can delete a bucket by running

aws s3 rb s3://bucketNAME                        rb= remove bucket


SOME aws COMMANDS using CLI are below:

   aws s3
   aws s3 cp   to copy
   aws s3 mb   to make directory in aws
   aws s3 mv   to move files
   aws s3 ls   to list directory or files
   aws s3 rb   means remove bucket in aws
   aws s3 rm    to remove a file/object   like     aws s3 rm s3://class29ccc/success.pdf
   aws s3 sync  to replicate data entry in one directory to also been see another directroy at the same time 
   aws ec2 describe-instances  if you run this coomand, it will tell you all the instances you have in your aws and the information about these instances as per thier AZs, ip addressess ect

   aws ec2 start-instances --instance-ids i-12345678c   

   if you run the above and it says unknowm output, dont worry, it still worked regardless.

   aws ec2 stop-instances --instance-ids i-12345678c 

   The above will stop and instance

   aws ec2 terminate-instances --instance-ids i-12345678c
   aws s3 ls s3://mybucket
   aws s3 rm s3://mybucket/folder --recursive
   aws s3 cp myfolder s3://mybucket/folder --recursive
   aws s3 sync myfolder s3://mybucket/folder --exclude *.tmp

For you to know most of aws commands in CLI, you can google aws CLI cheatsheet or paste the below url in your browser

https://devhints.io/awscli


NOTE THE FOLLOWING BELOW

-    IAM-Policies are attached to "USERS/Groups"  like the ones we created before   
-    IAM-ROLES are attached to Resources: like ec2-instances, s3 bucket, ecs, eks, ELB, VPC, DNS (domaine name system)

HOW TO INSTALL AWS CLI

Go to your aws console, click on install/update, click the copyon windows, or macOS or linux(to install it on linux environement), click on the copy simbol to copy zip file of the download and unzip file and the paste what you copy on your linux environment and press enter, run

aws --version  

that is

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version

You will see that aws is now install on linux

On ec2-linux environment, if you run

aws s3 ls

it will ask for us to configure credentials.
 you can configure credentials by running

aws configure

It will ask you for aws access key.  You use the acces token that we created for refui and press enter

it will the ask you of the aws secret access token. You can also use the secret acces token that we created for refui 

it will ask you for the region. Enter the region of the aws IAM user that you created and press enter , type in table and the press enter again

You are now authenticated

If you run

aws s3 ls

If it tells you access denied, it means that you have not been granted the permissions to  access resources like s3 in aws. or youR ec2-instance does not have the IAM-role.

LET US CREATE IAM ROLE

As an admin, you need to create role for all staff based on their roles in the company, before you create a role, you can first of all create staff policy or user policy before you cabn create role from the policy you have created

You can not create role out from the admin policy because admin policy is a full access to all the resources used in the company. admin role will be created from the admin policy.

Let us first let us create a user policy by search for IAM in your aws search and click it, click on policy under access managmenet, click on create a policy, go to search for ec2 and click it, tick on ALL EC2 ACTION, scroll down, tick ALL under resource, click on add more permissions, search for s3 bucket and click it, scroll down and tick ALL under resource, 

TO CREATE ROLE

click on role, click on create role, click on aws resource, click on ec2, tick user-policy1 under policy name, click next, enter the role name, enter description, click on create role.

HOW TO ASSIGN ROLE TO RESOURCES 

NOTE THE FOLLOWING BELOW

-    IAM-Policies are attached to "USERS/Groups"  like the ones we created before   
-    IAM-ROLES are attached to Resources: like ec2-instances, s3 bucket, ecs, eks, ELB, VPC, 

Let us assign role to EC2-INSTANCE

To assign IAM role to you ec2-instance, click on the instance, click on action, click on security, click on modify IAM role, click on create a new role (if you do not have an existing IAM role that you want to use), click on create role, tick ec2 under user case, type in ec2 on search and the select/tick AmazonEc2FullAccess, name the role as aws-role-IAM, click on create role.

under IAM console, click on role amd you will see the role that we just created 

Go to your IAM console, click on role and you will see the role we just ccreated

Let us add more policies to the roles 

Inside the aws-role-IAM role, click on add permisssion, tick the policy, click save.

NOT: You can attach more existing policies to the role. If you do not have anymore policy, you can create more.

if you run 

aws s3 ls

It would not be able to run because the role aws-role-IAM that was created and was not attached to the ec2

ATTACHING THE IAM-ROLE CREATED TO THE RESOURCE (like ec2-instance)

Before we attaching role to an instance, let us create an instance from class29 IAM user since we have already created an s3 bucket there. But you already have an instance there, we can make do of that.

After you have created the instance,

instance-role-IAM for ec2 and s3 full access role has been created and need to be attach to ec2-instance resources by clicking on the instance, click on action, click on security, click on modify IAM role, search for instance-role-IAM on the search box below, and then click on update IAM-role.

If you run once again

aws s3 ls

You will see that it will run when it list all the buckets in that s3.


LET US INSTALL TERRAFORM

make sure you go through your terraform video and materials slowly. Very important.

Teraform can be used to perform more task in aws

We can use the below script to install terraform by running the script or cpying and pasting below in your coomand line

sudo yum install unzip
sudo yum install wget 
wget https://releases.hashicorp.com/terraform/0.13.3/terraform_0.13.3_linux_amd64.zip
sudo unzip terraform_0.13.0_linux_amd64.zip -d /home/ec2-user/terraform_install

terraform_1.5.5

wget https://releases.hashicorp.com/terraform/1.5.5/terraform_1.5.5_linux_amd64.zip
sudo unzip terraform_1.5.5_linux_amd64.zip -d /home/ec2-user/terraform_install

then, run

vi ~/.bashrc

 copy and paste the below command into the ~/.bashrc file and save-quit

export PATH=$PATH:/home/ec2-user/terraform_install

the run the below command

source ~/.bashrc

then check if terraform has be installed by running the below: 

terraform -v

You will see that terraform has been installed.

CREATION OF AWS RESOURCE WITH TERRAFORM.

we have manage aws_resources using:
  1. console 
  2. commands = awscli 
  3. IaC with terraform code written in HCL format

  Let us create a simple VPC resource with cidr blocck via terraform code below

let us create a project directory by running

mkdir iac

cd into iac

vi into main.tf  as the terraform code/file name

copy and paste the below inside main.tf  and save-quit


# provider

provider "aws" {
  region = "ca-central-1"
}

# VPC1

resource "aws_vpc" "prod-vpc" {
  cidr_block = "10.0.0.0/26"
  tags = {
    Name = "prod29"
    Env  = "dev"
  }
}

# VPC2
resource "aws_vpc" "vpc" {
  cidr_block           = "172.1.0.0/24"
  enable_dns_hostnames = true
  tags = {
    Name = "demo_vpc"
  }
}

Note: Let us assume your ec2-instance name is tom-server, You need to have IAM-role assigned to tom-server that you want to use to provision this resource prod_vpc. That is, you must have granted ec2- full access when creating the IAM-role and then attach the role to the tom-server so tom-server can be able to successful create vpc resource in aws.

Above is called an infrastructure as a code.

Above terraform code means we want to use a programmatic approach to provision a resource in aws


We want to provision vpc1 and vpc2 using a single terraform code.

After you have saved main.tf in your iac directory.

1. You need to intialize the terraform code by running

terraform init

2. You need to validate the code to make sure it is free from suyntax error by running

terraform validate

3. You need terraform to re-organize the code to terraform format for terraform to understanding the language by running

terraform fmt

4. You need terraform to plan the resource he want to create by running

terraform plan

 4. terraform can now create the resources by running

terraform apply

 4. terraform can show you all your resources you created in aws by running

terraform show

 4. terraform can terminate all your resources you created in aws by running

terraform destroy


 4. terraform can import resources you did not create using terraform to be managed by terraform in aws by running

terraform import


 4. terraform can create workspaces in resources you created in aws by running

terraform workspaces

that is

terraform init|validate|fmt|plan|apply|show|destroy  
               import | 
               workspaces  


Let us create the two vpc inside main.tf by running 

terraform apply

and then type yes

You will see that it has created two VPCs which are prod29 and demo-vpc

If you go to your vpc console in aws and click on vpc, you will see that prod29 and demo-vpc is right there.

You can also want to delete or destroy the vpc that you just created by terraform by running

terraform destroy

If you go back to your vpc console and refresh, you would not find the vpc anynore.

Please go throuigh the terraform video.

advantages of terraform

1. Terraform can be used to provision or create resources any cloud provider platform like aws, gcp, microsoft azure

2. You can even used terraform to provisdion or create a source code manager like github account, to create a github repository, to create teams, to add members to team, create repository in dockerHub, 

VIDEO 112 BEGINS

DOCKER 1

iQ

You maybe asked, what are the services that amazon web services provides?

Answer: Amazon web services provides 

1. server resources/infrastructure like creating an ec2-instances. can also create a server and install an application and its dependencies in it and also can deploy the application (like tomcat application)
2  container resources liks eks, ecs, ecr
2. ELF  elastic load balancing
3. EFS/NFS  elAstic file system/network file system
EBS     ELASTIC BLOCK STORAGE
4. IAM   IDENTITY ACCESS MANAGEMENT
5. ROUTE-53 for domaine hosting
6. SSL/TLS certification for https
7. s3   SIMPLE STORAGE SYSTEM
8. AUTO-SCALING SERVICES

That is

aws = Amazon  WEB SERVICES:
  1. server resources / infrastructures 
      a. create an ec2-instances
      b.  install tomcat appliaction and it dependencies in the instance and deploy the application or artifacts to the webapps directory of tomcat.  That is ec2-instance/java-tomcat/webapps/app.war

     Note: VM = virtual machine is also called server or ec2-instance
  
  1b. containers resources / infrastructures 
       ECS / ECR / eks   

       ECS = elastic containers services
       ECR = elastic containers registry
       EKS = elastic kubernetes services

  2. serverless resources/ infrastructures

  amazon can also run serverless services like the below
       Lambda 
       fargate profile 

  You will need not to create or launch and manage any server. AWS does all the managing of the server. 

  It is called a serverless infrastructure to you as a client to aws or user. This is called IaS (INFRASTRUCTURE AS A SERVICE)

   3. storage recources
       EBS = ELASTIC BLOCK STORE  (ebs-jenkins-vol )
         We mount the root file system using EBS VOLUME
         /etc /bin /home /opt  /app 
         The Read/Write speed is very fast with EBS
         It is use equally to boot-up (start) servers  
          the OS of a server must load before it starts 

 IQ

 Where is the OS of your server stored or captured?

 Answer: The OS of your server is stored in the root file system

       EFS/NFS = ELASTIC FILE SYSTEM 

  This file system is highly scallable.

  It is a distributed file system. This means that if you have like 3 database server and you create a mount point on eanch of the server, data can be synchronize in the 3 servers at the same time.  its promote data synchronization

          dbServer1 
             /app
             touch names.txt 
          dbServer2 
             /app
          dbServer3 
            /app        
       S3  =  object storage
              log files / audio files / videos / 
              archives -- 2010logs.zip 
              artifacts -- app.war
              data migration 
              versioning   
              costing strategy: This depends on the storage class that you go for

         1. Standard class= =This is the Frequent access class

          where the visit to this object on the s3 bucket is frequent. Like a new movie in NWETFLIX, new book
                   new epizode of a movie in Netflix ---> $50k   5ms 

         2. IA - InFrequent access                  $25k   50ms
         3. Reduce redundancy                       $15k   900ms
                   Glacier                           $5k    5000ms             

     FOR EXAMPLE              
THE LAUNCH OF THE MOVIE BLACK PANTHER

In netflix, For the first 

1 -3 years                  5m request per month   this can be advised to be on standard    
3+ years                     100k request  per month  this can be advised to be on IA
5+ years                 10k request per month  this can be advised to be on reduce redundancy  
10+ year                   1k request per month  this can be advised to be on glacier

  4. security resources:
       IAM ---> 
       VPC and its components ---> 
         private subnets [db, apps] 
         public subnets [web, Jumpserver]
         Route Tables 
       Security Groups 
       private key pair
       NACL = network access control list
       Secret managers 
       Paramenter store 
       WAF = web application firewall
       ELB [users--->elb---applications]
  5. DATABASES :
       RDS [mySQL, mariaDB]
       nonRDS [dynamoDB,]
  6. Networking resources
        VPC / VPN 
        Route53

This are not the complete services of AWS:
  AWS SOLUTION ARCHITECT COURSE ----

WHAT IS DOCKER

Docker 
======
Docker is a containerisation software used to package or house an application and all its dependencies. 

With docker applications runs seamlessly in all Environments. This means that it can work in operating system (AMI = AMAZON MACHINE IMAGE) like windows OS, ubuntu OS, linux OS, centOS etc

VIRTUALIZATION OF A PHYSICAL COMPUTER

Your` laptop or computer has what is called a physical componet called hardware like

Hardrive = 32G, 
RAM = 500G  (SSD)

It has an internal compnent called the software like operating system

Let us assumne that the OS in your computer is windows OS

You can deploy your application on this OS. but your application is not very secure

You can install what is called a supervisor software.

Example of a HYpervisor software are   vmware, virtualBoX, xen

Hypervisor are virtualization softwares

WHAT DO0ES HYPERVISOR DO FOR YOU?

With the installation of a supervisor on your computer, multiple virtual machine (VM) or server can be installed in your computer. This is what AWS does on the backend. The VM is what the AWS call ec2-instance 

Once you have installed the supervisor in your computer, then you can go ahead to install other OS system in your computer.  Other operation system like 

ubuntu OS
linux OS
centOS
windows OS

The above other OS that would be installed later after the first OS installation is called GUESS operating system 

Any of the above other OS that maybe installed in your computer apart from the first OS that your computer is running with is called Guess-OS

The first OS installation that your computer was running with is called Host-OS (Host operating system)

With the above operation system, you can create 4 servers. This is what AWS does in the backend. They will install more operating system in different AZs or datacenter where customer go lauch servers.

VIRTUALIZATION 

The process of using a hypervisor software running in a computer or workstation to install multiple operating system that can launch multiple servers.

When servers are now launched on your computer using the above illustration, your computer now becomes a VIRTUAL COMPUTER.  This is how aws EC2 instance works.

NOTE: aws uses a hypervisor called XEN for vitualization

NOTE:  The storage resources of the GUESS operating systemS will be shared from the storage resources of the Host operating system. that is, 

if the Host-OS has 32 G of RAM and 500GB of HARDRIVE (SSD), it Host-OS may have utilized like 4G of RAM and 70GB of SSD, which is now left with 30G and 430G respectively, therefore, the 4 Guess-OS will have to share the balance 30G of RAM and 430G respectively.  You can assign the balance storage resources to them.

On these 4 virtual machines or servers running, you can now install an your application server and deploy application.

The whole setup above is called virtualization

VIRTUALIZATION

In a nutshell, the process of virtualization involves

1. installing a hypervisor in a computer or workstation with already existing operating like windows. This first OS is called Host-OS
2. installing additional operating systems or multiple OS in the computer with the help of the hypervisor. Each of these other multiple OS are called Guess-OS
3. Assigning storage resources to each of the guess operating system from the balance storage resources left on the host operation systeme
3. creating or launching multiple virtual machines on the multiple guess operating systems in the computer

It is the whole process of making a hardware computer to become a virtual computer or a virtual machine or a server.

NOTE: You can decide to containerized the virtual machine by installing a containerized engine or software called DOCKER on the server or VM. This is because docker can work in any environment.

CONTAINERIZATION OF A PHYSICAL COMPUTER

Just like the illustration above, your computer has a hardware like

Hardrive (SSD)
RAM

and an operating system like windows OS or linux OS.

A containerization engine or software called DOCKER can be installed on the computer to make the computer now becomes and A CONTAINERIZATION COMPUTER OF SYSTEM.

In containerization, you do not need any Guess Operating system to be installed. You only need just the Host Operating system.

With this, multiple containers with applications can be provisioned or created in the computer or system.

HOTE: Knowing that docker is a containerization software that is used to house application and its dependencies.

DIFFERENCES BETWEEN VIRTUALIZATION AND CONTAINERIZATION

- The major difference between Containerization and Virtualization 

is that Virtualization would need a Guess OS on the computer to become a virtual computer

Containerization does not need  Guess OS. 

- For virtualization to take place, a virtualization software called hypervisor (vmware), a Guess Operating system must be installed and binaries/libary files would be created

- For containerization to take place, a containerization software called Docker must be installed and binaries/libary files would be created.

- For containerization, each of the docker containers is very light in weigth compared to virtual machine or server. 

a docker container may require 0.2g of Ram for an application installed on the container to run

a virtual machine may require 4g of RAM for a application installed on the VM to run

WHY IS DOCKER CONTAINER LIGHTER THAN VIRTUAL MACHINE?

This is because container does not have a Guess OS.

For VM, when you start up your virtual machine, Guess OS has to start up first before the VM can start running. This whole start up process usually consume some resources. That is why VM is heavier than container.

VM size is large

Container is small

- it takes a longer time to start up a virtual machine
  it takes a short time and very fast to start up a contaner

- We can easily integrate containers with other software than interating VM with other software.

- Each VM contains an application, binary/libary files and a Guess OS

- Each container contains an application and binary/libary files and shared the Kernel with other containers. Kernel is the main or host operating system of the computer.

- containers can run in any computer, infrastructure and any cloud. This simply means that it can run in any environment and in any cloud provider services like AWS, GCP or Microsoft Azure.

- One of the major similarities is that they both need a Host Operating system.

It is important to note that you can also install a docker container in a virtual machine or server and application can run inside in the container. this is because Docker can be installed in any environment.

Docker is a software that is used to containerized or housed or packaged an application and its dependencies

Let us say an application code is

application code = app.war, td.war, boa.war 

Each application dependencies would be  = tomcat, java etc

Above is a web application.

If application code or application is = app.ear, td.ear, boa.ear 

Its dependencies     = jboss, java    

The above is an Enterprise application,  this application is deploy in JBOSS

if application code = app.jar, td.jar, boa.jar  
its dependencies     = java

Above are standalone application. It just need java for the application to be deployed. This is whay it is called standalone application

If application code = app.js    
Its dependencies  is   = npm  (Node-package manager)

Above is called NodeJS appplication

We maintain a minimum of 4 Environments: 
    dev,     QA,      UAT,          prod  
    working  working  not-working   not working 

In a situation abovem where an apllication worked in Dev and QA environment but failed to work in UAT and PROD environment would be a serious problem.

But with the use of DOCKER CONTAINER, if the application worked in Dev, it will work in all other environment. That is, if it worked in Dev., it will work in QA, UAT and PROD.

This what what it means by DOVKER can run seamlessly in all environment. if it runs in the first environ., it will run in others

In landmark, we manage 4 Environments 
   Dev 
   stage 
   uat/pre-prod 
   prod 

We maintain a minimun of 3 branches = 
   dev ---> 
   stage/testing  -->
   Master or production --> 

docker process:

When it comes to Docker, the first thingS we talk about are

IMAGE:

Docker image is a package containing an application code and its dependencies. 

It is contains set of intructions or code or template used to build a docker container

app.war + tomcat & java 


CONTAINERS

Docker containers are docker image that are now running in a server or instance

Docker image is used to create a container

Note: applications run as  containers in docker

HOW DO YOU CREATE A DOCKER IMAGE

For you to be able to create a Docker Image, you need a docker file. 

A docker file is used to create a Docker image

  Dockerfile----image---containers 


  DOCKER INSTALLATI0N

Docker Editions:

    Docker CE (Comunity Edition) --> Open Source (Free): 

This can  be installed in an ubuntu OS, windows OS and other OS except Redhat OS.
Docker CE cannot be installed on redhat OS

    Docker EE (Enterprise Edition) --> Commercial
This can be installed in any OS but it is not free.

Type: Containerization
Vendor: Docker INC


Docker EE --> Commerical :
   DTR --> Docker Trusted Registry(Private Repo to keep docker images)
   UCP --> Universal Control Plane --> It's GUI for managing Docker Machines

Containerzation Platforms/Softwares:

    Docker,     = 80% of most companies uses Docker for containerization
    CoreOS,     = 9%  of most companies uses CoreOS for containerization
    Rocket --   = 6%  of most companies uses Rocket for containerization
    Container-D = 5%  of most companies uses Container-D for containerization

Docker Container--> It's a runtime instance of a docker image
                    where our application is running.

     The standard unit where the application service is deployed or running.

 HOW DO WE KEEP OUR DOCKER IMAGES?

it is known that developers code are committed to and managed by SCM like GitHub 

Built packages or artifacts like app.war are uploaded and managed in Nexus/JFrog 

Docker images are stored and/or managed in dockerhub (DockerRegistries)

eXAMPLES OF dOCKER REGISTRIES ARE

1. docker-Hub
2. Nexus
3. Amazon ECR    = elastic container registry

Docker Repository

Inside the Docker Registries like DockerHub, a Repository need to be created where the Docker images can be stored and can be shared

PROCESSES

A containerization server called Docker will create a docker image and then push the image to a repository in docker registry like dockerHub.

As soon as the docker image is stored in the repository in DockerHub, any environment like Dev or QA or Prod can pull the image and deploy the image.

The image can easily be deployed by any environment by pulling the image and deploy.

The image can be deploy in any environement because the image contains both application codes and its dependencies and so it cannot fail in any environment.

MONOLYTIC ARCHITECTURE OF CODE  DEPLOYMENT

This is a process where all the the application codes are deployed as a single application to the application servers.

For example;

A developer can write different codes like 

login codes, registration codes, payment codes, order codes-------->tocat

If a customer now want to login, register, make payment, order an item, all these request can be deployed as a single application to just one application server

MICRO-SERVICE ARCHITECTURE OF CODES DEPLOYMENT

These codes witten by the developer (login codes, registration codes, payment codes, order codes) would be decoupled and be treated differently and deployed separately to different application server.

This is what Docker brings to the table. Every code and its dependencies is been packaged differently as a docker Image and would be deployed as containers separately.

login codes ---> login.war------>login-DockerImage--------->login-container
registration codes--->registration.war------>registration-DockerImage---->registration-container
payment codes ---> payment.war------>payment-DockerImage---->payment-container
order codes ---> order.war------>order-DockerImage----->order-container

, registration codes,  codes, order codes

Micro services works very well in a containerization environment.

DOCKER REGISTRIES

1. DockerHub:

DockerHub is generally a public repository. 

It contains all the open source softwares as  a docker images. it is a place or a market place where docker images are stored and can be seen publicly and utilized if you deeemd it fit

Using a dockerHub, We can think of docker hub as play store where docker images can be found publicly. like a play store in sasung where you can download or see many apps.

2.  (ECR, Nexus, JFrog, D.T.R(Docker Trusted Registory)) are all Private Repo

Using ECR or Nexus, We can store and share the docker images within our company network using private repos


 PaaS  = 
 IaaS  = EC2
 SaaS  = DockerHub, New Relic, GitHub, SonarCloud

Explain your experience in managing SaaS Platforms/solutions??

What is docker hub?
It's a public repository for docker images. You can think as play store for
docker images.

CREATION OF DockerHub Account

First Create Account in docker hub
    https://hub.docker.com

Create a repository and name it app

DOCKER DESKTOP INSTALLATION IN WONDOWS COMPUTER

Install Docker on your windows computer by typing docker desktop installation of google, then you can see where it says (docker desktop for windows), click it and it will download the setup to your downloads, click on the setup and it will run, and then go on to complete the installation

DOCKER INSTALLATION IN AN EC2-INSTANCE OR SERVER OR VIRTUAL MACHINE

Let us launch a new instance and install docker in it. 

let the name of the install be docker, let the AMI be ubuntu, let the instance type be t2-medium, let us use an existing security group if you like provided the neccessary ports are opened, let us increase the ebs volume from 8 to 15g, click on advance and then paste docker installation code containing other dependenices below in the userData to make the Ubuntu instance to become a Golden AMI. 

Note: You can type in docker installation in ubuntu in google to give you the steps for the installation.

#!/bin/bash
sudo hostname docker
sudo hostnamectl set hostname docker  
sudo apt update -y 
sudo apt install docker.io -y
#Add ubuntu as a username to the docker's user group by running the below line of code
sudo usermod -aG docker ubuntu 
sudo su - ubuntu


After you have pasted the above code in the user data, click on launch instance

Note: If you launch an ubuntu server, the username would be ubuntu by default. Just like when you launch an ec2-instance, the username would be ec2-user by default.

using an ssh client like mobaxterm to login to ubuntu server you just created

verify is docker sofatware or engine is running in the ubuntu server by running

docker

It it is running, it will bring out full page of information but if it not running, it will say command not found. 

You can further verity by running

docker ps

If it list out    container ID  Image  command  created   status

You will know that it is trully running.

If you run

whoiam

It will tell you who or the user that is accessing this docker engine. Ubuntu is the user that is accessing this docker engine right now

Let us add a user by creating a new user by running

sudo adduser kim

it will tell you to create password, enter admin123 as password and presss enter key, type in y and then press enter key

Let us switch to the new user, kim by runninbg

su - kim           becuae we have the password for kim

enter the password for kim

It will say, permission denied.

IQ

wHY is kim unable to access the docker engine. Why is kim denied to be one of the user that can run the docker engine?

Answer: This is because kim is not included in the docker group

Let us see users listed in the /etc/passwd/ file by

/etc/passwd file is used to keep track of every registered user that has access to a system

tail -7 /etc/passwd/           to see the last 7 added users

You will see the ubuntu user and kim user there.

To know if kim and ubuntu is part of the docker group, you can id ubuntu and kim by running

id ubuntu

You will see that ubuntu is part of the docker group and by default, also part of the sudoers group. This is because it was added to the docker installatiuon script and also ubuntu has a sudo privilege.

id kim

You will see that it is not part of the docker group and also not part of the sudoers group because it has not been added to them.

To add a user to a group, you run the below

sudo usermod -G GroupName Username

To add kim to docker group, run the below

sudo usermod -G docker kim

It will say you are not in the sudoers group and you will be reported.

This happened because we are executing the above command as kim user and kim does not have sudo privileges.

To be able to successfully add kim to docker group so that kim can execute any docker command, we have to first of all exit from kim to ubuntu because ubuntu has sudo privlieges by running

exit

This will take you to the main user by default

While in ubuntu user, you can add kim user in the docker group by running

sudo usermod -G docker kim

After you have done that, id kim and the you will see that kim is now in docker group. the run

switch user to kim by running

su - kim    and then run

docker ps

You will see that it will respond.

Let us start our docker desktop in our windows computer by click docker desktop icon and the click on start docker desktop

Since I have installed docker engine on my desktop computer, i can as well run docker on my gitbash terminal in my computer by running

docker ps

You will see that it shows    Container ID  Image  command  created   status

HOW TO RUN AN IMAGE IN DOCKER ENGINE

Let us copy a docker image from our dockerHub (copy the image from prof dockerHub)

mylandmarktech/maven-web-app

The above is a docker image

Image (mylandmarktech/maven-web-app) = app code + dependencies 

Container = running instance (where docker software has been installed) of docker image

Container = running instance  of docker image 

A Container is created when docker image is running in a server or instance where a docker software has been installed

HOW TO CREATE A CONTAINER

You can create a container by running the below command

docker run -d -p 80:8080 imageName

docker run -d -p 80:8080 mylandmarktech/maven-web-app

d = means to detach the docker image from dockerHub

p = means for the container and dockerHub to listen on a specific port number. 

Port 80 is the dockerHub port which is always http because it is a public repo.

Port 8080 is the container port because the application its self is running in a tomcat but this tomcat application and its depemdencies are packaged together in a docker image. 

docker run -d -p 80:8080 mylandmarktech/maven-web-app

If you run the above, it will pull the docker image from dockerHub, create a conatiner and start the container.

You can name the container along with the command that creates container. That is 

docker run -d -p 80:8080 --name NewContainerName imageName

You can know if a container has been created by running

docker ps

You can be able to acces the application by copy the below on your internet browser

DockerIpaddress/applicationName

DockerIpAddress/maven-web-app

Note:

When you install a docker sofetrware, the installation comes with

1. Docker engine : This is also called a docker daemon. It is the responsible for task to be executed. For example when you run the command

docker run

The above command triggers or activate the docker engine to create and start a container.

The Docker Engine is one that is responsible in creating and starting the container.

2. Docker CLI. The is responsible in running any command in docker environemnt and when you want to run any command in docker, it must start with docker. 

This mean that if you want to run any command in docker, it must start with docker

Foe example, if you want to check the version of docker that we have. YoU CAN RUN

Let us note some commands in docker

- docker --version----> this gives you the version of docker your system or server is running on
- docker build----> this create a docker image

Lets us go to Prof Github account and clone some repositories

https://github.com/LandmakTechnology/maven-web-application

https://github.com/LandmakTechnology/tesla-app

Let us clone the tesla-app repo by running

git clone https://github.com/LandmakTechnology/tesla-app

If says command not found. It means git is not instored in this docker server and so run

sudo apt install git -y

We are using apt instead of yum because apt is the package manager for ubuntu

yum is that package manager for ec2-instance

after git has been installed, run the below command again

git clone https://github.com/LandmakTechnology/tesla-app

the run ls

You will see that the tesla-app has been cloned here. cd into tesla-app by running

cd tesla-app   and 

ls

You will see the files inside the tesla-app

let us vi into dockerfile and you will see that it has 2 key work

FROM tomcat:8.0.20-jre8
COPY target/*war /usr/local/tomcat/webapps/     (Note: Prof delete other things after webapps/)

The 2 key words here are FROM and COPY

FROM is a Dockerfile keyword that defines the base image. This base image are  mostly dependencies 

COPY  is also a keyword that is used to copy files into the container

If you look at the content of the dockerffile closely, you will see that it made mention of copying target directory with a war package to tomcat webapps directory.

But in the docker server that we have launched, we do not have any target directory if you run 

ls

We need to do a build on a web application like .war file by a build server like maven so that a war package or artifacts can be created insie the target directory of the tesla-app project directory.  if you run

mvn clean package

You will see that it says mvn not found. This is because maven is not installed on this docker server. 

It will tell you the command to use to install maven. That command starts with sudo. that is

sudo apt install maven

Since we are in kim environment, kim does not have sudo privileges. we need to exit to ubuntu so that we can install the maven build software by running

exit                              and then run

sudo apt install maven

The above command is installing maven via yum but you can first of download maven via maven website and then unzip the file.

after maven has been installed, you can go back to kim accxount where the project repo or directory (tesla-app) is. cd into tesla-app and then run 

mvn clean package

The build is done inside the target/

ls target/
 You will see the artifacts that was built or created (tesla.war)


CREATION OF AN IMAGE FROM DOCKER FILE.

Image are always been created from a docker file or from a docker registry.

Let us first of all check if we have any image that was created before and if there is any conatining running already by running

docker images

The above command will dispay all docker images on this docker server

If you runn the command, you will see that an image is alredy been created before and that image was pulled or copied from prof dockerHub.

If you run the below command, it will display all container running in this docker server

docker ps

You will see that conatiner has been created and it is running on this server.

To create an image, you can run

docker build -t ImageNameChosen:VersionNumber DuctSign

Let us call the name of the image to be DockerImage and let us out 1 as number of my tag

docker build -t dockerimage:1 .

Make sure that you are inside the project directory where the file is located before you can run the above command

t = tagNumber

the duct sign . represent that you are building from the project directory (tesla-app)

If you run the above command, you will notice that it will do one thing

1. Docker engine docker daemon would be do one that would execute the task

2. it will pull tomcat version from tomcat libary in docker Registry

3. It will copy the file or artifacts from target directory to the webapps of tomcat

You will see that the build was done successfully.

If you run

docker images

You will see that dockerimage with tag number 1 is displayed there.

If you want to create a container and run the container from the docker image, you can run

docker run -d -p 8080:8080 --name NewContainerName imageName:tagNumber or Version Number

let us call the container app

docker run -d -p 8080:8080 --name app dockerimage:1

docker run -d -p hostPort:containerPort --name app dockerimage:1

d = detach the image from your project directory

p = port number of tomcat from the tomcat libary of docker registry (host Port) and port number of tomcat to tomcat webapps (container port). 

P is also called port forward, the host port receives the traffic or request and the forward the request to the container port.

NOTE: The HOST PORT can be changed to other 4 digits number and the conatiner name can also be changed to another name when you are creating more containers

Note: Tomcat is running inside the container for deployment to production

If you run the command, you will see that it took few second for the container to be created and started

If you run docker ps

You will see that actually the container was created.

You can access this on youe web broswer or on your docker server CL by copying and pasting the belwo

DockerEngineIpAddrea:PortOfDockerEngine/PathToTheApplication

DockerEngineIpAddrea:HostPort/PathToTheApplication

You can go to the project directory, cd into target/ and run ls

You will see that the path is tesla (in blue color) and the application itself with .war is red in color. sin ce the path to the application is tesla, you can run

dockerIpAddrea:8080/tesla

paste the above on your web browser and it will display the application.

You can deploy multiple application or multiple continer with the docker image.

To deploy multiple application, you can name the second container and so on,  and also change the host port to 4 digits numner. 

The host port is the port that is pulling deployment-application version from the libary of deployment-application (like tomcat). Which is always the first left port number

Let us change the host port to 8000 and the name the second container to be app2

You can run

docker run -d -p 8000:8080 --name app2 dockerimage:1

It will create another container called app2.

This means that the tesla.war application is now running on port 8000

You can run  docker ps  to confirm.

Note:

To create a container

docker run -d -p hostPort:ContainerPort --name containerName image:tag/version   
    where:
      -d = detachable mode  
      -p = port forwarding/mapping   

VIDEO 114

 PORT FORWARDING OR PORT MAPPING

As soon as host port which is the left hand port receives traffic , it will forward or route the traffic to the container port on the route to create and start a container

Note: Two containers cannot have the same host port but they can have the same container port

Always change the host port and name the container a new name when creating more containers

If you attempt to create another container called app3 with the same host port with app2. that is

docker run -d -p 8000:8080 --name app3 dockerimage:1

You will see that it failed. It will say port is already allocated. It will create the container but it will not be able to start the container

For you to know the containers that are created and running, and the container that are created and not started, run the command

docker ps -a

You will see the containers that are created and running, and also the ones that are created but not running

ANSWER TO QUESTION ASKED IN CLASS

1. Application can be deployed to production directly from tomcat

2. Apllication can also be deployed to production via docker container

Docker containers are what companies make use of today in deploying application to end users

The reason why most companies no longer use tomcat for deployment is simply because

1. tomcat vm is heavy weight. It consumes lots of storage resources but docker containers is light weigth

2. Docker containers can be deployed in any environement but tomcat cannot be deployed in any environment rather than prod environement.


Docker is a powerful containerizing tool used for deploying application in containers.

Diagram showing how docker container are built is below
            

              Build                  Run
DockerFile-------------->Image-------------->Container

Above explained that you use a dockerFile to build a docker-Image and using a Docker Image to launch and run a container.

The Image usually contains the application(built artifacts by a build technology like maven) and its dependencies like software (tomcat), operating like ubuntu

DOckerFile is a text file containing docker instructions used to build or assemble a docker Image

A dockerFile list the steps needed to build an image

Registry = this is an image repository

NOTE:

It is not recommended to have both docker build and docker run or deploy in one docker server

Why?

The reason why it is not recommended is because you need to avoid a single point of failure and disaster recovery in the sense that if we have both of them in one server and the server failed, 2 docker processes would be affected but if both of them are in separate docker server, only one will be affected. That is

DockerBuildServer                                            DockerDeploymentServer

DockerBuildServer =  is the docker server that is created just for the purpose of creating a docker image only  by running the command   

 docker build -t ImageName:VersionNumber

DockerDeploymentServer =  is the docker server that is provisioned just for the purpose of creating and deploying docker containers only, by running the command 

docker run -d -p HostPort:ContainerPort --name containerNameChosen ImageName:VersionName

IQ:

Where do docker gets images from?


Answer: 

Docker image are gotten from Registries like dockerHub, Amazon-ECR (Amazon elastic container registry. These are SAAS)

Docker image can also be gotten from IAAS like Nexus and Jfrog

HOW IT WORKS

1. First of all, you will create an image repository in docker registry like dockerHub. The repository will serve as a container that will house our image that would be built

2. secondly, you go to your docker Build server and run a docker build command 

docker build -t ImageName:VersionNumber

It will build a docker image 

You will push the docker image built or created to the image repo that you created in dockerHub or in Nexus by using docker push command.
The image repo can be a private Image repo or a public Image repo. 
Example of public image repo are image repo created in public registry like dockerHub. Example of private image repo are repo created in private registry like ECR, Nexus, JFrog

3. Thirdly, go to your docker deployment server and then pull the docker image from the image repo in dockerHub via docker pull.

Forthly, you will have to create and startup a container by running the command

docker run -d -p HostPort:ContainerPort --name containerNameChosen ImageName:VersionName

DOCKER IMAGES COMMAND

The following are commands that are related to docker images



Docker images commands:  
====================
- docker build   - create or build a docker image from a Dockerfile

CREATION OF A SIMPLE DOCKER FILE

Using the tesla project repo that we cloned from Prof github account as an illustration. That si

https://github.com/LandmakTechnology/tesla-app

In that project repo that we clone. There is a simple docker file that has been created. that is 

  FROM tomcat:8.0.20-jre8
  COPY target/*war /usr/local/tomcat/webapps/

  From the above dockerFile, The BASE Image here is TOMCAT.

  Base image represents the name and version of the appplication server from which the image is built(FROM), and the name of the application server deployed in the container

  The explanation to the dockerfile above is that the host is tomcat with tomcat version specified, meaning that the artifacts that would be deployed to tomcat is to be copied or forwarded to the webapps directory of tomcat home directory inside the container.

  Since the name of the application is tesla.war (*war), you can configure the dockerFile in a way that the tesla.war name will change to another name when it appears in tomcat webapps directory in the container by adding the name you want it to appear with after webapps/   in the form of

  FROM tomcat:8.0.20-jre8
  COPY target/*war /usr/local/tomcat/webapps/app.war

  You can use the above simple dockerFile to build or create a docker image

Another exapmle of a simple dockerFile that was copied from prof github repo is below

FROM jboss/wildfly
ADD target/*war /opt/jboss/wildfly/standalone/deployments/

Form the above dockerfile, the Base Image is JBOSS

looking at the dockr file above, you will agree with me that this is a web application because of the presence of .war

It is know that web applications can be deployed with a deployment server called tomcat and Jboss/wildfly

Wildfly is another name for Jboss

NOTE: 

Deployment in Jboss takes place in the deployments directory (/deployments)but deployment with tomcat takes place in the webapps directory (/webapps)

Let build an image using the dockerFile gotten from prof repo below

FROM jboss/wildfly
ADD target/*war /opt/jboss/wildfly/standalone/deployments/

Using the same docker server that has tesla project application cloned to that server to build docker image from the above dockerFile.

cd into kim user, cd into the project directory. if you run ls, you will see the other dockerfile that we used yesterday.

Let us created another dockerfile by runing

vi dc

copy and paste the above dockerfile with Jboss as base image, into dc and tghen save-quit

run the below comand to build image

docker build -t ImageName -f dc:2 .

We add pass f before the dockerfileName because of the fact that dc is a custom dockerfile, it is not the default name for dockerfile. we could not use the name dockerfile because we already have that name used and that is the default name.

If you are using another name other than dockerfile which is the default name, you need to pass letter f to the command so it can recognze that dc is a dockerfile.

the duct sign (.) means we are building from the same directory where dockerfile is located.

You can also build an image without assigning tag number or version number to it. That is

docker build -t ImageName -f dc .

Note:

If you run

docker build -t ImageName .

It will build docker image from the default dockerfile in your present working directory

  docker build -t myimage .   = building from the default Dockerfile in present working directory 
  docker build -t myimage -f dc . = building from a custom Dockerfile [dc] in pwd 

 Let us assume that your dockerFile is not in the working directory. Maybe the dockerFile is located in tmp directory (/tmp). you canb run the below command

  docker build -t myimage -f dc /tmp =  building from a custom Dockerfile [dc] in /tmp

 docker build -t myimage -f dc /home/kim/tesla-app

NOTE: You only run the command for docker build that end with a duct sign when your dockerfile inside the working directory where you want to execute the command

If you dockerfile is in another directory other than where you are present working, you will need to state the path to the directory to which the docker file is located and do away with the duct sign. That is 

If you are presently working in kim directory but the dockerfile is in tesla-app directory, you would first of all the command

pwd

To know the exact path to the working directory you are workin from. if it is 

/home/kim/tesla-app

docker build -t myimage -f dc /home/kim/tesla-app  is the same with

docker build -t myimage -f dc /home/kim/tesla-app

because the dockerfile is in the same working directory you are working with

Let say the docker file is in another directory in kim user like myfile directory which you created inside kim user,  You need to cd into myfile directory and run  

pwd

copy the path and the cd back into the tesla-app directory and run the command


docker build -t myimage -f dc /home/kim/myfile

Since we created the custom dockerfile in tesla-app. let us run the below command to created an startup a container by running

docker build -t myimage -f dc /home/kim/tesla-app or docker build -t myimage -f dc .

you will see that it says successful.

You can view if the image creation was suuccessful by running

docker images

You will see that it was created

Let us create a container from the docker image (myimage) by running

docker run -d -p 7000:8080 --name ContainerNameChosen myimage

We pass port 7000 as the host port and we know that that the conatiner port which is the Jboss port is also 8080 same as tomcat

let the name of the container be mycontainer

docker run -d -p 7000:8080 mycontainer myimage

if you run the command, it will say successful and you can verify by running the command

docker ps

You will see that the container was created.

You can access this container to show that it is up and running by the below commands

curl ifconfig.co   or  curl ifconfig.ca

to find out the public ip of the docker server and the run

curl dockerServerIpAddress:7000/tesla

You always use the host port that you assigned for this and not the container port

Tesla here is the path to the war application     

or paste   dockerServerIpAddress:7000/tesla   in your internet browser

it will disply the content of your the application

if you paste dockerServerIpAddress:7000/tesla   you will see that it show wildfly which is jboss

DOCKER PULL

How do we pull images from docker registry?

what kind of images can we pull from docke registry?

We can pull the following images from docker registry

open source images [these are images from the public or community]   
custom image   [these are images from your company. Its a private image] 

LETS US USE DOCKER TO DEPLOY SONARQUBE

Let us pull sonarqube images from dockerHub using our windows computer

if you go to your gitbash environment. make sure that you have already install docker in it or docker is up and running in your gitbash and then run

docker images and  docker ps

You will seee that there is no container and no image

Let us pull sonarqube image from dockerHub knowing fully well that dockerHub is a community registry (public register which is an open source registry)

Login to your dockerHub account and then type sonarqube on the upper search which is the dockHub search button and not your own search button, click on copy simbol of where it says (docker pull sonarqube)  and save what you copied. what you copied wil be

docker pull sonarqube

The name of the image here is sonarqube

Go to your gitbash and run the command

docker pull sonarqube

it will take a little while and it will successfully pull. You do not require any authentication to pull images from dockerhub but you will need authentication to pull from custom image like image from ECR, Nexus and other private registry.

If you run

docker images

You will seee that the image that you pull is now in your docker window computer

Let us run the image to create docker container by the command

docker run -d -p 4000:9000 --name ContainerNameChosen imageName

let us name the container to be sonarc

we know that the image name is sonarqube according to where we copied it from

4000 is the custom host port that i chose, 9000 is the default port number for sonarqube

docker run -d -p 4000:9000 --name sonarc sonarqube

after you have run the above command, it will say successful. You can run

docker ps      you will see that it was created.

You can access the container in your docker environment in gitbash by running

curl -v localhost:4000    

You can also access via browser by pasting the below in your internet browser

localhost:4000

It will tell you to login in credentials. use sonarqube admin login

username; admin
password: admin

when you press enter, it will ask you to create a new pasword. try and create a new password and then it will take you to your new sonarqube web page.

We have just succeeded in deploying sonarqube server using docker

LET US DEPLOY THE SAME SONARQUBE IMAGE USING OUR DOCKER SERVER in aws cloud in mobaxterm

Go to your docker server in mobaxterm and create a docker container from the same image called sonarqube above. That is 

image = sonarqube 

If you run the same command above

docker run -d -p 4000:9000 --name sonarc sonarqube

Since the image is not in our docker server in this aws cloud, docker server would first of all go to dockerHub which is a public registry to serach for the image and when found, it will pull or download the image and then run the image to create a docker container called sonarc as instructed.

You can verify this by running the below command

curl -v dockerServerIpadd:4000

and you can access it online using by pasting below on your browser

dockerServerIpadd:4000

and it will ask you to login. enter the defaul login credentials admin admin and then change the password.

You cna use this sonarqube for your codeQuality analysis to do whatever you want to do

using docker server/instance, you can pull images that are already available in private registry using the docker pull command  when you have the credentials for authentication or when you have authorization with 

SOME OTHER DOCKER I MAGES COMMANDS

  docker build   = create images  
  docker tag  = assigns repository name to images. That is

Let us assume that your repository name that you created in dockerHub is 
osas-devops/wf-app    

where wf-app is the repoName and osas-devops is the dockerHub account name.

osas-devops/wf-app   is the absolute path to the repo wf-app

if you click on your repoName in dockerHub, under docker command, it will tell you that to push a new tag or NewImage to this repository (docker push osas-devops/wf-app:tagname) the tagname can be just a number like 2). If you do want the tag, you can leave it. let us you put 23 has the tagname. the command wouyld be 

docker push osas-devops/wf-app:tagname    

this applies when you are pushing from another server via CLI

  TO PUSH IMAGE TO DOCKERHUB

You push the image that you built in your docker server to your repo (osas-devops/wf-app) in dockerHub by first of all creating the same repo name that you have in your dockerHub to your docker server and then add the image to the repo name you create (osas-devops/wf-app) using  just one command

docker tag imageName:tagNumber PathToYourRepo:TagName or Number 

docker tag imageName osas-devops/wf-app:23

or    docker tag imageName osas-devops/wf-app:23   with image tag assigned. it will show latest

docker tag imageName:2 osas-devops/wf-app:23

after you have run the above command, run

docker images

You will see that your image is now under a repo called osas-devops/wf-app with tag 23

LET US PUSH THE IMAGE TO DOCKERHUB

Note; Pushing image to your repository or someone else repo in dockerHub would require authentification but pulling images from dockerHub would not require authentication

You go can push the image to your repo in dockerHub osas-devops/wf-app:23 by running

docker push osas-devops/wf-app:23     thst ia

docker push pathToRepo:tagName or number

If you run the above command, it will tell you denied because it needs authentication

You need to be login to your dockerHub account via CLI in this your docker server by running

docker login -u osas-devops

it will ask you for password, enter your dockerhub password and  it will say login successful.

You can now run docker push again

docker push osas-devops/wf-app:23 

You will see that it was done successfully. when you go back to your dockerHub wf-app repo, you will see that the image was pushed there. 

  REMOVING OF DELETING OF DOCKER IMAGES IN YOUR DOCKER SERVER.

To remove docker images, you run the following comand

docker rmi imageID or name:tag

If the above did work, it will tell you to force remove, by using below command

docker rmi -f imageID or name:tag

If it did not work, it means that the image is running as a container. 

You will need to stop the running of the container before you can force the remove of that image.

TO STOP A CONTAINER

You can stop all containers in your server by running the below command

docker stop  $(docker images -q)

To verify if it has stopped, you can run

docker ps -q     it will tell you no container is running

docker ps -a    it will list the full status and tells you container exited. it is not running

You can stop a particular containers in your server by running the below command

docker stop imageID or name:tag

docker ps -a    it will list the full status and tells you container exited. it is not running

To verify if it has stopped, you can run

Since you have stopped the running containers, you can now remove or delete the image you want to delete by running again

docker rmi -f imageID or imageName:tag

Note, if there is no tagNumber, it always say latest. ensure to always include latest as tag if there is not number under the tag

If you want to delete all the images in your docker server, you run

docker rmi -f $(docker images -q)

It will delete all the images.

STARTING OF STOPPED CONTAINERS

You can start container/s that was stopped by running

docker start containerName   = it will start a particular container that was stopped.

docker start  $(docker images -q)   this will start all the conatainers that was stopped

DOCKER IMAGES COMMANDS

  docker build -t myimage 
   docker tag myimage:tagNumber PathToDockerRepo:tagNumber  
   docker push mPathToDockerRepo:tagNumber  
   docker login -u DockerHubUserName

docker build -t imageName:TagNumber .  it will build or create an image from dockerFile
docker run -d -p hostPort:ContainerPort --name containerName imageName:tagName
  docker images    = list images
  docker image ls  = also list images
  docker images -q = to list list only image ids 
  docker rmi imageID or name:tag     = delete or remove an image
  docker rmi -f imageID or name:tag  = force delete or remove an image
  docker rmi -f a848f84f4894 
  docker rmi -f  $(docker images -q) = deletes all images 
  docker run image2 
  docker stop  $(docker images -q)
  docker start containerName   = it will start container that was stopped.
docker start  $(docker images -q)   this will start all the conatainers that was stopped
docker history imageName OR image ID = gives you history of the image

Note. If you want to delete any image, make sure that no container is running on that image

DOCKER CONTAINER COMMANDS:

  containers are created from docker images  
  a container is a running process/instance of an image 

  docker pull dockerImage
  docker pull dockerImage:tagNumber    when it has a tag or version
  docker scan dockerImage

Let assume the image is mylandmarktech/java-web-app in dockerhUB

  docker pull mylandmarktech/java-web-app
  docker pull mylandmarktech/java-web-app:20    when it has a tag or version
  When you do not choose a verssion or tag, it will pull from the lastest versiuon
  docker scan mylandmarktech/java-web-app  TO CHECK VULNERABILITY, IF THERE IS OR NOT

ENABLING SCANNING IN YOUR INTERPRISE OR PRIVATE DOCKERHUB ACCOUNT

If you do not want to scan using CLI, you can enable SCAN in your enterprise dockerHub account by clicking on the repository, click on the repoName, scroll down, click enable scan where vulnerability is

It is very neccessary that we should always scan our images to check vulnerability and if it is vulnerable, our apllication would be vulnerable as well

Let us pull the images mylandmarktech/java-web-app from prof dockerHub repo and then scan the image

docker pull mylandmarktech/java-web-app

docker scan mylandmarktech/java-web-app

Note: java-web-app is the image here    mylandmarktech/java-web-app is a path to java-web-app

If there are vulnerable, the image would not be used until the vulnerability has been removed. Here the image was without vulnerablity

 TO CREAT A CONATAINER THAT IS NOT YET UP AND RUNNING

docker create -d -p hostport:conatinerPort --name CONTAINERnAME mylandmarktech/java-web-app

let myapp be the container name

docker create -d -p 3000:8080 --name myapp mylandmarktech/java-web-app

TO START THE CONTAINER

 docker start myapp  

To pull and start a container from dockerHub in just one command

docker run -d -p hostPort:ContainerPort --name containerName imageName:tagName

 It will pull, create and start the container  

to access the container in you CLI, run

curl -v dockerServerIPAdd:hostport/PathToApplication       that is

curl -v 44.211.63.229:3000/java-web-app  or curl localhost:3000/java-web-app

To access on the browser

44.211.63.229:3000/java-web-app

Note: containers also have their ip address and also have a port number

  CONTAINER MODES:
    -detachable mode  -d : to create a container that would be detached from it docker host or docker server, making the contrainer to run independently
    -interactive mode -it : this will create a container and then takes you directly inside the container. The container created will not work independently from the docker host. It interract with it

    To create an interractive container, you can run the below command

    docker run -it imageName bin/bash

curl -v 44.211.63.229:3000/java-web-app

docker run --name webapp -d -p 80:80 mylandmarktech/hello  

  docker ps -          = list running containers 
  docker container ls  = list running containers 
  docker ps -q = list running containers IDs 
  docker ps -a  = list running & stopped/excited containers
  docker ps -aq = list running & stopped/excited containers IDs 

List ONLY stopped Containers
=================================
docker ps -aq --filter  status="exited" 
docker ps -a --filter  status="exited" 

List Running Containers
=======================
docker ps -a --filter  status="running" 
docker ps 
docker container ls

List All Containers
==================
docker ps -a
docker container ls -a

List only running container ids
==============================
docker ps -q
docker container ls -q


List all container ids
==============================
docker ps -aq
docker container ls -aq

Delete excited/stopped Containers
================================
  docker rm containerId/Name
---
Delete excited/stopped Containers
================================
  docker rm containerId/Name

Delete running Containers
================================
  docker rm -f containerId or Name    to force remove a running container
  docker stop containerId or Name && docker rm containerId or Name  would stop and delete container

Delete all Containers
================================
  How to delete all containers
  docker rm -f $(docker ps -aq)

Delete all stopped Containers
================================
docker rm $(docker ps -aq --filter  status="exited")

docker rm --help 
   FROM CPA to a Sr. DevOps Engineer
   166k plus  = 185k plus 


Start the container
===================
docker start <containerId/Name>
docker start myapp

stop/kill the container
========================
  docker stop <containerId/Name>
    gives warning before stopping the container process 
  docker kill mylandmarktech/java-web-app
    gives no warning before stopping the container process  

pause/unpause 
============
    docker pause <containerId/Name>         it will puase the running of this container
    docker unpause <containerId/Name>        container will continue from where it was paused
You can pause a container to fix any problem on the container and the unpause it

STOPPING AND STARTING OF MULTIPLE CONTAINER

 To stop and start some containers amongst others is by COPYING the names or IDs and then run

docker stop containerId/Name containerId/Name containerId/Name

docker start containerId/Name containerId/Name containerId/Name 

If they are 50 Containers you want to stop and start, you run

docker stop $(docker ps -aq)          it will stop all the containers

docker start $(docker ps -aq)         it will start all the containers

HOW TO TROUBLESHOOT OR RESOLVE A PROBLEM OR DEBUG AN APPLICATION RUNNING AS DOCKER CONTAINERS?

The process is listed below

1. docker ps  = will check if the container is running  
2. docker ps -a = will check if the container is in "exited status or stopped status"
   docker ps -a --filter status="exited"

What do you think must have happened if a container was actually created but not running?

The causes may be as a result of below:

a. limited system resources like cpu/ram may not be enough to start the conatiner
b. port conflict/binding used in creating the container, 
c. address or name used in creating that container is already in use  

3. docker exec  = enable you to execute command inside the container when you used detachable mode to create the container. You would input docker exec containerName Command

For exmaple:

docker exec containerName ls = runs command inside the container by listing contents of the container

docker exec containerName ls logs = this will list the contenet of logs thats in the container

docker exec containerNmae  ls logs/catalina = this will list the content of catalina thats in the container

docker exec -it app29  /bin/bash  will take you inside the container and then you can begin to run the command that you want to run while inside the container. like, ls, cat, vi etc. if you are done, just type exit and it will take you back to the directory you were before.

4. docker top containerName  = will tell us amount resources the container is consuming. check resources 

5. docker logs containerName = check the logs so that can see the messages. whatever that is happening in the container would be captured in the logs 

  docker logs containerName | grep errors  = this will show you errors 

6. docker stats containerName = it will let you know how much resources is available and the one you have used
7. docker inspect containerName = gives info about the container
8. docker attach containerName = will attach the container to the docker host server/instance

NOTE: We will not be using tomcat in our CI/CD integration anymore because we are now using docker containers for deployment. 

Jenkins-docker integration is the one that are selling for now. It is what is in voke

QUESTION ASKED IN CLASS

How do you choose the hostport? is it an imaginary number?

Generally, sever port like   = Windows/Linux/MacOS has 0-65355 ports 

To determine you host-port, you would have to also chose a port from 0-65355 ports but make sure that the port is not in use.

Answer: Host port chosen should be a port that is not already in use. That is why it should not be the same as the conatiner port or host port.

HOW CAN I KNOW IF A PORT IS NOT IN USE IN MY SERVER?

 netstat -tulpn

yOU YOU TRY THE COMMAND AND IT SAYS  COMMAND not found. IT MEANS that netstat is not install in the sever. You can install it in am ubuntu server using

if the user like kim does not have sudo privileges. exit to ubuntu server that has sudo privilege and then run the coammand

sudo apt install net-tools

Go back to kim and then run the below command

netstat -tulpn

It will list all the ports that are in use like 80, 8080 etc. It is very important you know so that you do not don a conflict port. you need to choose a port that is free within 0-65355 for your host port. This also applies to addresses. The command will also tell you addresses that are in use. you cnanot use an address that is already in use.

To know a container port.

Maybe you have pulled the image from dockerHub and you are not sure what the container port may be. To check what the container port would be is by running

docker inspect PathToImageName

Once you run the command, scroll and look for where it says EXPOSED PORTS. Whatever number you see there would be the container number. The container port is always included on the dockerHug image because it is the image that you would be used to build a container but if you can access the dockerfile, you can know the container port from your dockerfile which depends on the application that would be running.

You can still verify or know the container port from your dockerHUB account when you click on the imageName, scroll down and you will see EXPOSED PORT number.

DockerFile
  FROM jboss/wildfly
  EXPOSE 8080 
  ADD target/*war /opt/jboss/wildfly/standalone/deployments/

You can tell from the above dockerfile that container port is 8080 because it is adding the built artifacts to be deployed in jboss and that jboss be one of the dependencies of the war artifacts that would be packaged on docker Image.

Note:  Your EXPOSE PORT is your container port
 docker run -d -p hostPort:ContainerPort  mylandmarktech/java-web-app 
 docker run -d -p 3000:8080 mylandmarktech/java-web-app 
 docker run -d -p 8000:8080 mylandmarktech/java-web-app


 How to copy files from container to host system or host system to container?

1. from container to host system or local system
  docker cp app:/usr/local/tomcat/logs . 

2. from host system to container
  docker cp java-web-app/ app:/usr/local/tomcat/

 WHAT IS A DOOCKER COMMIT?

 docker commit is a command that is used to create docker image from an existing docker container

Using docker commit we can create image from 

the container which will include changes effected.

NOTE: docker commit is used to create docker image from an existing  docker container

docker commit ontainerId/Name imageName

If the containerName is app, you can say let the image name be appimage and the version number can be 1. then

docker commit app appimage:1 

docker commit app mylandmarktech/java-web-app:3 
docker tag newimage:1 mylandmarktech/java-web-app:9
docker push mylandmarktech/java-web-app:9
docker login -u mylandmarktech 

what is difference b/w COPY and docker cp?

  cp = copy files and directories from docker-host to container and vice versa
  COPY is a keyword in the docker file use to copy files into the image at creation.
     
 VIDEO 115

 JENKINS-DOCKER INTEGRATION FINTECH PROJECT

 CI/CD JOBS

STAGES IN APPLICATION DEVEPLOYEMT (END TO END DEPLOYMENT)

After the developers have FINISHED written codes, they will commit the new version of the code or application to github

A. CONTINUOUS INTEGRATION (CI-JOB) OR BUILD AND RELEASE JOB

1. git clone = this will clone the latest code in github

2. clean package = this will clear the old build, compile the code to create java classics, do code valaidation to check for syntax error, run a unit testing and build or create artifacts/packages in the project target directory eg, .war, .ear and .jar packages

3. CodeQuality analysis by sonar:sonar = this will run CodeQuality analysis on the built artifacts

4. UploadArtifacts to artifactory like Nexus or Jfrog by deploy= this will uploads artifacts to Nexus/Jfrog to backup the artifacts

     THE ABOVE STEPS IS CALLED BUILD AND RELEASE OR CONTINUOUS INTEGRATION

5. Docker build = This will create the docker image

6. Docker push = This will push the image to docker Registery like Amazon ECR (elastic container registry), DockerHub, or in an artifactory like Nexus, Jfrog etc

7. Docker run = This will pull the image from docker registry and deploy the application as a container

8. Email Notification: If there are any issues, email notification will be sent

NOTE: Containerization is the process of housing or packaging an application and its dependencies and then run them in a server to form a light-weight executable called container that can run consistenmtly in any environment or infrastructure.

USING JENKINS DECLARATIVE PIPELINE TO RUN THE CI JOB ABOVE

Jenkins Declarative Pipeline

pipeline{

agent any

tools{maven "maven3.9.2" } 
 stages {
 stage('1GetCode'){
 steps {
 sh "echo 'cloning from the latest version of the application' "
 git "put the url of the public project Repo here from prof maven-web-application"
 }
 
 }

 stage('2Test+Build'){
steps{ 
 sh "echo 'run Junit testing'"
 sh "echo 'Junit testing must be passed before creating artifacts'"
 sh "mvn clean package"
 }

}

 stage('3codeQuality'){

steps{
	
	sh "echo'performing CodeQualityAnalysis'"
	sh "mvn sonar:sonar"
}
 }
 stage('4upload2Nexus'){

steps{
sh "echo 'uploading artifacts to Nexus or Registry'"
sh "mvn deploy"

}
 }
stage('build and push image to dockerhub')

{
	steps{
	sh"echo 'docker build and push image'"
      sh"docker build -t nameOfImage absolute path of the directory where the dockerfile is loctaed."
	 sh"docker push mylandmarktech/maven-web-app"
	 //make sure you login to dockerhub by docker login -u dockerhubUsername and the run below command
}

}
	
}
}


B. CONTINUOUS DEPLOYMENT JOB

9. build docker image by docker build = this will build docker image

10. push docker image to registry by docker push = this will build docker image

9. create container by docker run = this will run application as a container

The declarative Jenkins for CD JOB is below:

pipeline{
	agent any{

stages{

stage('deployment as docker container'){
	steps{
	sh"echo 'application ready for deployment'"
	 sh"docker run --name myapp -d -p 7000:8080 mylandmarktech/maven-web-app"
	 sh"sleep 20"
	 //sh"sleep 20" means you want the process to sleep for 20 seconds. In some case, it will allow the process to sleep for 30 minutes for the team to observe the process before deployment would commence. This is a sleep command in linux 
}
	

}
}
	
}


LET US INSTALL JENKINS ON OUR EXISTING DOCKER SERVER WITH JENKINS IMAGE GOTTEN FROM DOCKERHUB

The name of jenkins image in dockerhub that can successfully be installed is called jenkins/jenkins

You can first of all pull the image by running

docker pull jenkins/jenkins

You do not neeed to pull the image. You can just run or create jenkins-docker conatiner by running

docker run --name jenkins -d -p 800:8080 -p 50000:50000 -v /home/ubuntu:/var/jenkins_home jenkins/jenkins

This will pulll the jenkins image from dockerhub and create and start up jenkins container

After the container has been created, you can access jenkins on your web page, run

ServerIpAddrress:FirstHostPort

It will ask you for admin password. You can run the below command to get the admin password

docker logs containerIdNumber

Once you run the above command,

It will give you an encrypted password under some sentenses like

Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:

Password Wouild be here

Once you put in the password, you click on suggested plugins, register your credentails like user name and password, email address and then click save. You jenkins is fully running

NEXUS AND SONARQUBE INSTALLATION USING DOCKER

I have install nexus and sonarqube as a docker container on the docker server by running 

docker run --name sonarqube -d -p 1000:9000 sonarqube

You can access it in your web using

publicIpAddress:HostPort

It will ask you for username and password. The admin user name and password for sonarqube is 

admin   and  admin   respectively

TO INSTALL NEXUS

docker run --name nexus -d -p 3000:8081 sonatype/nexus3

You can access it in your web using

publicIpAddress:HostPort

click sign in and it will ask you for username and password

You username is admin

You password can be gotten from the below

docker exec containerIDNUMBER cat passwordLinkShownInThePage
           or
docker exec containerName cat /nexus-data/admin.password 

5f19cf7e-00db-47ce-b906-8d98765378bf


This server now has docker, jenkins container, nexus container and sonarqube container

NOTE: 

Anytime you stop a docker server, the containers running in the docker server stops also.

Anytime you start up your ec2-instance that has docker installed in it, the docker starts immediately also because docker is light weight. If j

You can verify this by running

docker   or docker images

but the containers running in side the docker server would not start.

You can very this by running

docker ps -a

You will see that the containers are stopped also

You can start up the container by running

docker start containerID

Once you run the above command, the containers would start within seconds

You can verify also by  docker ps -a

LET US CREATE JENKINS ON DOCKER HOST OR DOCKER EC2=INSTANCE OR DOCKER ENGINE OF DOCKER VM

For easy configuration, let us install jenkins on docker ec2-instances or vitual machine or docker host 

#JENKINS INSTALLTION SCRIPT

#Installing Prerequisites
sudo apt update -y
sudo apt install openjdk-17-jre -y
sudo apt install vim wget git -y
sudo apt install openjdk-8-jdk -y
sudo apt install openjdk-11-jdk -y

#Installing Jenkins and adding Jenkins
curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \
  /usr/share/keyrings/jenkins-keyring.asc > /dev/null
echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null
sudo apt-get update -y
sudo apt-get install jenkins -y
sudo systemctl enable jenkins
sudo systemctl start jenkins
sudo echo "jenkins ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/jenkins #Give jenkins sudo rights
sudo su- jenkins

If you want docker to be able to work with jenkins, you would register docker engine to be on jenkins group as an admin user by running

sudo usermod -G docker jenkins

If you install jenkins via virtual machine. That si, instally jenkins in a virtual machine like ec2-instance, you have install jenkins and docker along with instance created, try to access your jenkins server on your web browser.

jENKINS RUNS ON PORT 8080

publicIpAddress:8080

Once you are in jenkins GUI, click on new item and name it ebay-CI, click pipeline, click pipeline, copy the CI pipeline script above and paste it on the box, select pipeline script, click apply to check for syntax error, if there is none, click save, go to jenkins, click global tool configuration, click add maven, you will seee the latest version there and then type in the latest version on the name like maven3.8.4 click on add maven, select the latest version and click on save , click on discard old builds and put 2 days, put 2 as maximum builds, click save, click dashboard, click the item name ebay-CI, click build now.


You will see that it will start to builD the CI job and it will build artifacts but will fail to execute codeQuality reports and also fail to upload artifacts to Nexus or Registry because we have not configured or integrated sonarqube and nexus/ECR/DockerHub with Jenkins.

Let US CREATE A SONARQUBE CONTAINERS USING DOCKER

ssh into jenkins-docker ubuntu server via mobalxterm, switch user to a user called jenkins by running

sudo su - jenkins

You know that the docker imageName of sonarqube in dockerhub registry is sonarqube.

let us create or launch a sonarqube container by running 

docker run sonarqube -d -p 3000:9000 --name sonar

You will see that a sonar conatiner have been created and started. Please give it some minute to be fully initialized.

If you run  

docker container ls 

You will see that sonarqube is running inside the container. under status, it says UP and the tell you how long its been running

SIMILARLY LET US LAUNCH NEXUS CONTAINER USING DOCKER

Go to your dockerhub account, type sonartyper nexus on the dockerhub search, scroll down click on sonartype/nexus3, click on the copy sign for the command to pull the image sonartype/nexus3. That is   (docker pull sonartype/nexus3)

You can now create the nexus container on that same jenkins user by running

docker run sonartype/nexus3 -d -p 5000:8081 --name nexus 

and our nexus container would be created.

docker container ls   or docker ps -a

You will see that nexus and sonarqube is running inside the container. under status, it says UP and the tell you how long its been running.

This ec2 server is now docker-jenkins-nexus-sonaqube server sharing the same ip addresses.

The reasonS Why we are USING docker container to launch our resources here is because 

1. containers are light weight. it takes seconds to launch OUR RESOURCES via docker while it takes minutes to launch resources via VM
2. If you launch or install nexus, jenkins and sonarqube on the same VM or server it will take lots of your server memory resources because they are heavy weight compared to docker. You can use docker to launch the 3 resouces because docker containers are light weigt and can be deployed in any environment. docker can be install in any infrastructure and can use docker as container to install other resources.

NOTE: It is also possible to also launch your jenkins as a container with docker with same procedures.

docker run comman does 2 things. it create and it started container

So you can now access both nexus and sonarqube on your browser

login to sonarqube with the default username and password (admin  admin), change the password.

login to nexus by puting admin as the default username, it will tell you where you can find the password, cat the link on your jenkins-docker server. to do that, you have to cat the link in nexus container running jenkins-docker server. that is

docker exec containerName cat passwordLinkShownInThePage

docker exec nexus cat /nexus-data/admin.password 

6479007c-9fe7-45b1-9862-ae0aaabf1799

copy the encrypted password and the paste in the nexus login page, it will log you in to the nexus GUI, clcik next, create a new password, click disable anonymous access, click finish.

NEXUS CONFIGURATION

Go back to your docker-jenkins-nexus-sonarqube server and copy the public or private ip address but for security sake and the fact that it is nexus, copy the private ip address, 

Go back to your project in github (i have forked prof github application and i am using my github to do this by editing pom.xml in my repo called CLIFFOSA22/maven-web-application-1), click on pom.xml file, click on edit, inside pom.xml go to where it says properties tag, go to where sonarqube info are and replace the ipaddress in there with the private ip that you copied and also put in your new sonarqube port number, also change the username and password to what you have updated it to be.

To update the side of Nexus, let us first of all to the nexus GUI and then create a snapshot repository because snapshot repo is what is mostly used for deployment, by

clicking on repository, click on create repositorr, click on maven2 hosted, name the repository like ebay-snapshot, select snapshot for version policy, click create repository. copy the url, go to your pom.xml, scroll down and go to snapshot repository tag under distribution management tag and found where nexus info are and the change the info to the new nexus info, replace the url, the ipaddress and port number with the new ones. Remember you now have a new hostPort number, you can use the public ip address and then click commit the changes.

You also need to create a nexus servers repository tag in jenkins settings.xml via CLI by going to your jenkins user, and then ls, you will see tools, cd tools, ls, cd into hudson.tasks.maven directory, ls, cd into conf directory, ls, vi into settings.xml, scroll down, copy the whole servers tag just under where it says USED TOGETHER and paste it in-between ----> and </servers>, then this servers tag that you have pasted here, replace id there with nexus, replace the username to nexusUsername, replace the password with nexusPassword, and then save-quit

If you now go to your jenkins GUI, go to your project and run the build, you will see that both sonarqube and nexus has succeeded.

This means that our CI has successsful executed.

B. FOR OUR CD JOB

go to dashboard, click on new item, name it docker-deployment, select pipeline, click ok, under build trigger tick build after other projects are built, under project watch, you want them to watch like ebay-CI, click on discard old builds and put 2 days, put 2 as maximum builds, click save, copy the CD job declarative pipeline script above and paste it in the pipeline box below, click apply to check syntax error, if there is an error, the line where the error is will be marked red, correct the eroor and click save.

To build the CD job, you have to build the CI job in order to trigger the build for the CD job. that is how you configured it. Upon successful build for the CI job, the CD job would also build.

DISADVANTAGE OF USING DOCKER FOR CONTAINER DEPLOYMENT

If you to create a container with the samme name more than one time by clicking build now when you are inside your item on your jenkins GUI, it will fail. 

For it to be successful, you have to delete the old container or rename the container and then re-click the build now before it can be successful. It can not create more than one container with the same container name. This is why kubernetes is more preferrable to provision container for deployment.

This means that docker lack the capacity tyo scale

 REMEDY

For you to delete the existing container and create another container with the same name to be created anytime you run build now, you would have to go to your jenkins pipeline CD job by clicking configure, click on the CD job, click on pipeline and add the command below, click apply and then click save

docker rm -f containerName   that is 
docker rm -f myapp


pipeline{
	agent any {

stages{

stage(undeployment of the application){
	steps{
	sh"echo 'undeploying existing application'"
	 sh"docker run --name myapp -d -p 7000:8080 mylandmarktech/maven-web-app"
	 sh"docker rm -f myapp"
}
}

stage(deployment the application){
	steps{
	sh"echo 'application ready for deployment'"
	 sh"docker run --name myapp -d -p 7000:8080 mylandmarktech/maven-web-app"
}
}
stage('email notification'){
    steps{
	sh"echo'deployment sucessful'"
	}
	
	}
}
}


In the above configuration, you would create a container and delete the container and the re-create the same container because it is the same name. But if you click build now again to create the the same container it will fail. This is because docker does not have the same capacity to create a container with the same name in jenkins over and over again. except you delete the old container and recreate the container and this can be a very big problem especially customers are already accessing the container.

Docker container lack the ability to scale. This is what kubernetes is the best.


You can access the application on our web browser or externally ny 

publicIpAddress:HostPort/applicationName

publicIpAddress:HostPort/maven-web-app

Please make sure you dont used existing Host port number running in your server to deploy another application. if you do, it will result to what is called PORT BINDING. ALSO CALLED PORT CONFLICT 

Once you have done the above configuration of the declarative jenkinsfile, you CAN now be able to deploy or click build now on jenkins GUI multiple times while maintaining the same container NAME.

CLICK ON BUILD NOW IN CI JOBS TO TRIGGER THE BUILD OF CD JOBS

Since you have tested and confirm that your CD jobs is now up and running effectively the way you like it, you can now go and clck build now in CI job and you will see that as soon as the build of CI jobs is successful, it will automatically triggger the build of the CD jobs.

In a nutshell, all we have done here is that we have been a able to deploy our application as docker container, deploy sonarqube as docjer container and deploy sonatype/nexus as docker contaner as well.

Note: if you try to do this 3 deployment with a server or aws instance type with t2 medium, it will not work.
Let us check our server resources to know how much resources we have used and how many left by running

df -h

You will noticed the it is only about 20% resources has be used and 80 still avaialable because your instance type prof choose was t3 2x-large which has about 30G of size

You can still view the memory left and used of your server by ruuning

free -m


LET US AUTOMATE OUR CI JOBS SO THAT IT CAN TRIGGER OUR CD JOBS

Go to dashboard, click the project item (ebay-CI), click on configure, scroll down, click on poll scm under build trigger, type in * * * * *  or  H * * * *  or  D * * *  inside the schedule box below which means every minute or every hour respectively and the click apply and click save

After you have done this configuration, this means that anytime there is a new version of your application or a new commit as a rwsult by way of modification of your application in your SCM (SOURCE CODE MAGEMENT LIKE GITHUB), a build would be trigered in jenkins CI job, and it will also triger your CD job for the deployment of ther new version of the application.

Let us go to our project repo github where are are build this application from and modify our dockerfile of that project by adding below

# This is a great deal

and then commit the changes. 

Note. this modificatuion would not run in your dockerfile or mean nothing because it is commented. but it is seen as modofication in your project even though it means nothing.

after you have committed, you will see that once it gets to 1 minute, a build will be trigered in your CI jobs.

NOTE: If you install jenkins and docker in the same instance, you need to add jenkins to docker group. 

CONFIGURING THE CI JOBS AND CD JOBS TOGETHER IN OUR JENKINSFILE FOR DEPLOYMENT

You can also the app., with CI and CD jobs in one jenkinsfile. Let us add the jenkinsfile for CI and CD tother to make one file for the deploy. 

If you want to merge them, just copy from where it says stage in CD and paste it in the CI. That is

pipeline{

agent any

tools{maven "maven" } 
 stages {
 stage('1GetCode'){
 steps {
 sh "echo 'cloning from the latest version of the application' "
 git 'put the url of the public project Repo here from prof maven-web-application'
 }
 
 }

 stage('2Test+Build'){
steps{ 
 sh "echo 'run Junit testing'"
 sh "echo 'Junit testing must be passed before creating artifacts'"
 sh "mvn clean package"
 }

}

 stage('3codeQuality'){

steps{
	
	sh "echo 'performing CodeQualityAnalysis'"
	sh "mvn sonar:sonar"
}
 }
 stage('4upload2Nexus'g){

steps{
sh "echo 'uploading artifacts to Nexus or Registry'"
sh "mvn deploy"

}
 }
stage('predeployment to docker')

{
	steps{sh "echo 'docker build and push image'"
      sh "docker build -t mylandmarktech/maven-web-app ."
	//login to dockerhub by docker login -u dockerhubUsername and then run below command
	 sh"docker push mylandmarktech/maven-web-app"
}

}
	
stage(undeployment the application){
	steps{
	sh "echo 'undeploying existing application'"
	 sh "docker run --name myapp -d -p 7000:8080 mylandmarktech/maven-web-app"
	 sh "docker rm -f myapp"
}
}

stage(deployment the application){
	steps{
	sh "echo 'application ready for deployment'"
	 sh "docker run --name myapp -d -p 7000:8080 mylandmarktech/maven-web-app"
}
}
stage('email notification'){
    steps{
	sh "echo 'deployment sucessful'"
	}
	
	}
}
}


create a new project item naming it ebay-CI-CD and copy and paste the above jenkinfile in the jenkins script in jenkins GUI and click apply and click save.

You you click on build now, you will see that it all build from CI to CD.

if you run

docker images

You will see that latest image that was created few minutes ago.

Also save this jenkinsfile on the project repo in github as part of the source code.

ABOVE WAS PROF. BUT MY JENKINSFILE THAT I COMPOSED WAS BELOW

CI JOBS ONLY

pipeline{
agent any
tools{maven "maven"}
stages{
stage('1cloneclode'){

steps{
	sh "echo 'clone code from github'"
	git 'https://github.com/CLIFFOSA22/maven-web-application-1'
]
}

stage('2test and build'){
	steps{
    sh "echo 'creating artifacts in the target directory'"
    sh "echo 'make sure that the unit testing is passed before build'"
    sh "mvn clean package"
    }

    }
stage('3codequality analysis'){
    steps{
    sh "echo 'run code quality analysis on the application'"
	sh "mvn sonar:sonar"
    }
	}
stage('4deploy to Nexus'){
	steps{
	sh "echo 'deploy artifacts to be backup in nexus'"
	sh "mvn deploy"
	}
}
stage('build docker image'){
   steps{
   sh "echo 'build docker image from dockerfile in your project file'"
   sh "docker build -t app1 ."
   // Dockerfile is located inside the repo that was cloned from github in my Jenkins user. That is why i used the duct sign
   }
}

stage('deploy docker image'){
   steps{
   sh "echo 'deploy docker image to docker registry'"
   sh "docker push osasdevops/tes"
   // osasdeveops/tes is my url repo name in dockerhub. The repo name is tes
   }
}

}

}

CD JOBS ONLY

pipeline{
	agent any

tools{maven "maven"}

stages{
stage('undeploy as docker container'){
	steps{
	sh "echo 'pull image from docker registry and deploy application as a docker container'"
	sh "docker rm -f tesla-app1"
	}
}
stage('udeploy application'){
	steps{
	sh "echo 'undeploy the tesla-app1'"
		sh "docker run --name tesla-app1 -d -p 12:8080 osasdevops/tes"


	}
}
stage('email notification'){
	steps{
	sh "echo 'application deployed successfully'"
	
}

}
}
}

In the above configuration, we delete to configure remove container with the containerName so that anytime the container with the same container name is created, you can be able to click on build now multiple times to be able to create the same container with the same name. you would create a container and delete the container and the re-create the same container because it is the same name. But if you do not configure this like that, of you click build now again to create the the same container it will fail. This is because docker does not have the same capacity to create a container with the same name in jenkins over and over again. except you delete the old container and recreate the container and this can be a very big problem especially customers are already accessing the container.

Docker container lack the ability to scale. This is why kubernetes is the best for container orchestration or deploying applications as a container.

The deployment above is called a microservice deployment. This is because the deployment is decoupled to take place in two separate containers because the application was decoupled to be two application. 

We can also decide to deployment this application in one single way by joing the two deployment processes together. This becomes a monolythic service application deployment

for it to be monolithic service. we can do. That is, we combine both the CI-CD JOBS as below:

pipeline{
agent any
tools{maven "maven"}
stages{
stage('1cloneclode'){


steps{
	sh "echo 'clone code from github'"
	git 'https://github.com/CLIFFOSA22/maven-web-application-1'
}
}

stage('2test and build'){
	steps{
    sh "echo 'creating artifacts in the target directory'"
    sh "echo 'make sure that the unit testing is passed before build'"
    sh "mvn clean package"
    }

    }
stage('3codequality analysis'){
    steps{
    sh "echo 'run code quality analysis on the application'"
	sh "mvn sonar:sonar"
    }
	}
stage('4deploy to Nexus'){
	steps{
	sh "echo 'deploy artifacts to be backup in nexus'"
	sh "mvn deploy"
	}
}
stage('5build docker image'){
   steps{
   sh "echo 'build docker image from dockerfile in your project file'"
   sh "docker build -t app1 ."
   // cliffosa-app is the directory where my Dockerfile is located in my Jenkins GUI
   }
}

stage('6deploy docker image'){
   steps{
   sh "echo 'deploy docker image to docker registry'"
   sh "docker push osasdevops/tes"
   }
}
stage('7undeploy the application'){
	steps{
	sh "echo 'pull image from docker registry and deploy application as a docker container'"
	sh "docker rm -f tesla-app1"
	}
}
stage('8deploy application'){
	steps{
	sh "echo 'deploy the application as a docker application'"
		sh "docker run --name tesla-app1 -d -p 12:8080 osasdevops/tes"


	}
}
stage('email notification'){
	steps{
	sh "echo 'application deployed successfully'"
	
}

}
}
}


You can also site or attach your jenkinsfile in your repo in your jenkins script. Make sure you save your jenkinsfile staring wiyj jenkinsfile-anyOtherWord like jenkinsfile-docker because jenkins GUI always recognize jenkinsfile.

Note: anytime you click on build now in your jenkin GUI, a new application would be deployed as a container and from a new docker image that is created as you click build now. These processes or steps is very swift and fast.

NOTE: In realtime, you would have docker-Build engine and docker deployment engine.

This neams that DOCKER-BUILD ENGINE is just meant for docker to build image here from dockerfile by packaging both application and it dependency like tomcat as an immage and then push the image to docker registry like dockerhub, ECR, nexus or jfrog.

DOCKER-DEPLOYMENT-ENGINE will be responsible pulling image from the registry and deploy or run the image as a container. The deployment of the application can be carried out in any environment. It can carried out in dev or stage or uat or prod. env.

TWO MODES OF CREATING A CONTAINER

DETACHABLE MODE: 

This is a mode of creating in which the container created is detached from the docker host. Docker host is the docker server or docker instance or docker engine. that is what we have been using to create our containers 

INTERRACTIVE MODE: If you create a container in an interractive mode, it takes you into or inside the container you just created.

EXAMPLE OF CREATING A CONTAINER IN AN INTERRRACTIVE MODE

You can use the below command

docker run -it imageName bin/bash

In the above command, since you did not specify the name that would be given to the conatiner, the container created would bear the name with the imageName.

Note: When you create a container in interactive mode, if you come out of the container, the container we stop running and so when you run docker ps, you would not find the container

Let us create an ubuntu server conatainer by running the below command

docker run -it ubuntu /bin/bash

You will see that you are inside the container. If you run ls, you will see the content of your container.


NOTE: If you have an instance type of t2 medium and you want to change the type to a higher type like t2-2xlarge t2-xlarge so it the size can accomondate your softwares, you can simply go to your isnatnce in aws and stop it, tick the sinatnce and the modify the instance type and increate it to what you want

In an aws instance, you can not go for instant type that is lower that what you already have but allowed go higher 

VIDEO 117

DOCKERFILE END TO END

Docker is a containerization tool in our environmeent. Since our evironment is heavy on microoservices, we dwell so much with software that can permit us de-couple monolytic applications into microservices.

Dockerfile --> It's file which contains instructions to create an image quickly. 
               Docker Daemon or docker engine  will process these instruction from top to bottom.


	DOCKERFILE KEYSWORDS OR INSTRUCTIONS

What is the difference between CMD and ENTRY-POINT?

We can overwrite CMD but we cannot overwrite ENTRY-POINT

 NOTE BELOW

1. Maven Build server create artifacts e.g jar/war/ear

a. war application are called web applications. Web application or war applications are usually deployed to tomcat or Jboss also called wildfly
.

These above artifacts are stored securely in the artifactory like Nexus, Jfrog
b. Jar applications are also called stand-alone application. This type of application are deployed in boxes or servers or instances with java installed. 

Boxes here is refered to a VM or server or instances.

c. ear application are called enterprise application and they are deployed only in JBOSS/wildfly only


2. DockerBuildServer create images. This docker images are also called artifacts. for docker image to be built, a dcokerfile is required. The build image from dockerfile.

3. DockerDeploymentServer: This will pull images from registry and create a containing that is up and running.

A dockerfile is an instructiosn conatining keywords.

Docker images contain application codes + dependencies and softwares

FROM
MAINTAINER
COPY
ADD
RUN
CMD
ENTRY-POINT
WORK-DIR
ENV
EXPOSE
USER
VOLUME
LABEL
ARG

FROM           This is used to set our minimal base image. This minimal base image get to do with our dependencies. From: usually talk about the version of our deployment application that our application would be deploy to

MAINTAINER   This is used to define who is the author of the dockerfile
COPY         if you create an image and wants to copy some file into that image
ADD          if you dont want to use copy, use add to copy files from external sources
RUN          This is an instructions that are used to pass commands. Like running a command

             touch readme.md    to touch a create an empty file

CMD  This is also an instructions that are used to pass commands. CMD means command.Like

CMD sh startup.sh   

   when you want to start tomact. You can also CMD like below

CMD  sh catalina.sh start

this is starting a tomcat inform of a shell format This  is not recommanded.

             The recommanded one is to make the above command written in an executable form. that is 

CMD ["sh", "catalina.sh", "start"]

ENTRY-POINT   "sh", "catalina.sh", "run"]

When you run a command 

docker build

the instruction or keyword executed is the RUN,FROM  and everyother keyword except the CMD 

docker run

the instruction or keyword executed is either CMD or ENTRYPOINT.

WHAT IS THE DIFFERENCE BETWEEN SHELL FORM AND EXECUTABLE FORM?

Answer:  

executable form in docker:
When instruction is executed or carried out in exec form it calls executable directly, 
and shell processing does not happen. This means that executable (exec) form is the PARENT PROCESS. This can not be killed unnoticed

When instruction is executed or carried out in shell form it calls /bin/sh -c <command> 
under the hood and normal shell processing happens. The shell form is the CHILD PROCESS.
Example of a shell form is 

  sh catalina.sh run     sh catalina.sh start

For example;

In linux there is what is called process management command

whatever you do in linux, it is a process that is running. It can be a CHILD PROCESS or it can be PARENT PROCESS

CHILD PROCESS depends on the PARENT PROCESS to survive.

A child process can be killed unnoticed or without pre-information but a parent process can not be killed without pre-informing

ENTRY-POINT  This is also an instructions that are used to pass commands.

WHEN IS RUN, CMD AND ENTRY-POINT EXECUTED?

1. When you enter a command  {docker build}, RUN instructions is being executed.
2. When you enter a command  {docker run}, CMD instructions is being executed.
3. When you enter a command  {docker run}, ENTRY-POINT instructions is also being executed.

WORK-DIR
ENV      =  environmental variables. If you pass the variable, you may wamnt to deploy in different env., like dev, uat, prod
EXPOSE  =  This determines the port that the container is going to be listened on
USER
VOLUME
LABEL
ARG  = Argument


Our goal here is to use these maven built artifacts to create a deployable containerized light weight packages called docker images. 

For this to be achieved, we need some key words. Since we Know that docker images simply containes application codes and it dependencies.

COPY: If you want to copy artifact from the maven build server, assuming maven is installed on the same server with docker software, you can use the COPY keyword
     If you want to copy artifact from artifactory like jfrog/nexus. You should use the ADD key word. This means that you are copying artifacts from 3rd party locations or external sources. In https location. It will look like below

https://NexusIpAddress:portNumber/repository/repositoryNane/artifactName  in the form of

ADD: https://4.22.36.77:8081/repository/td-releases/app.war

This is the artifact url in nexus

The default port for nexus server is 8081  or nexus listen on port 8081

Meaning we want to download the above artifacts from Nexus. We should use the ADD keyword to copy the artifacts from artifactory to docker container

The location wherre to download this artificats to also matters. 

If we want to download it to the default working directory of the docker container, we use 

https://4.22.36.77:8081/repository/td-releases/app.war .

To get the default working directory of a container, we run the command

docker exec ContainerName pwd

it will give you sometine like this

/usr/local/tomcat

Knowing that development takes place in webapps directory, you also add /webapps

/usr/local/tomcat/webapps

Therefore, the downloads will take place in the url of application stored in Nexus and will be added to default working directory of container/placeOfDeployment. that is 

ADD  https://4.22.36.77:8081/repository/td-releases/app.war .
 
            or
 
ADD   https://4.22.36.77:8081/repository/td-releases/app.war /usr/local/tomcat/webapps

You can deploy more applications. let us deploy another application from maven target directory

ADD target/web-app.war /usr/local/tomcat/webapps

FROM  
ADD   https://4.22.36.77:8081/repository/td-releases/app.war /usr/local/tomcat/webapps
ADD   target/web-app.war /usr/local/tomcat/webapps

In the above url, our default container has tomcat in it as where the application would be deployed to. My BASE IMAGE would be tomcat. So FROM is tomcat.

FROM represents the base image. This base image contains dependencies and softwares. Always mention the version of tomcat or base image version. that is 

FROM  tomcat:8.0.20-jre8
ADD   https://4.22.36.77:8081/repository/td-releases/app.war /usr/local/tomcat/webapps
ADD target/web-app.war /usr/local/tomcat/webapps
RUN

It is not recommended for a single container to deploy multiple applications because we want a container to be light weight, we want it to run in isolation.

If you deploy multiple application using a single container, it may cause a scaling problem.

LET US FIND OUT SOME DOCKERFILE IN SOME DOCKER IMAGES

Let us search for tomcat docker image.

tomcat official image = go to dockerhug and search for tomcat and click it, click description

If you scroll down, you will see the url or link called README file under SUPPORTED TAGS AND RESPECTIVE. When you click the url, you will different versions or tags of tomcat Images. if you click on any of them, you will see the dockerfile that was built on to create the tomcat image. 

If you also want to find out the dockerfile for java image, type in java in your dockerhub search bottonclick description. If you scroll down, you will see the url or link called README file under SUPPORTED TAGS AND RESPECTIVE. When you click the url, you will different versions or tags of java Images. if you click on any of them, you will see the dockerfile that was built on to create such java image. 

EXPLAIN THE PROBLEM YOU FACED IN YOUR PROJECT RECENTLY

Answer: The problem we recently faced in your project is shell form vs executable form. What happened was that we have a new recruit that were hired, and we had assigned him to modify some of our dockerfile. In doing so, he went and change our CMD instruction from executable form to shell form. Based on this changed, our applications were killed without noticed. And this resulted to application outage. the application were now dependent on other processes. They were now running as a Child process and some of them were killed un-notice 

We quickly reconfigure our dockerfile properly by ensuring that our CMD instructions runs as a parent processes or in an executable format in order to be able to create our docker image.

NOTE: If you run your CMD instructions as a shell script, you are running it as a child process and so it becomes dependent on other processes for it to be substained.

EXAMPLE OF A DOCKER FILE

FROM openjdk:17-jdk-buster
ENV CATALINA_HOME /usr/local/tomcat
ENV PATH $CATALINA_HOME/bin:$PATH
RUN mkdir -p "$CATALINA_HOME"
RUN adduser tomcat 
USER tomcat
WORKDIR $CATALINA_HOME

EXPOSE 8080  
CMD ["catalina.sh", "run"]

CMD sh catalina.sh start

TAKE A LOOK AT THE DOCKER FILE BELOW. iT IS AN EXMPLE OF A BAD DOCKER FILE.

QUESTION: Tell me why it is a bad exmaple of a dockerfile?

FROM centos 
RUN yum install java -y
RUN yum install maven -y 
RUN  yum install git wget unzip zip -y
RUN git clone  github/maven-web-app 
WORKDIR maven-web-app
RUN mvn clean package
RUN wget tomcat-9.0.58.zip
RUN uzip tomcat-9.0.58.zip
COPY target/*war  /usr/local/tomcat/ 
CMD ['catalina.sh', 'run']


ANSWER:

In the above dockerfile. You will notice that we have multiple RUN instructions in the dockerfile. 

If you take a look at prof dockerfilr repo in github and see an exmaple of bad dockerfile that he illustrated in class.

It was also evident that there was so many RUN instructions on the dockerfile and

ADD instructions containing tomcat downloads url in that file is meant to downlaod and install tomcat in centos.

REASON WHY YOU SHOULD NOT WRITE A DOCKERFILE WITH TOO MUCH RUN INSTRUCTIONS

It is not recommend to write a dockerfile with multiple RUN instructions because for any RUN instructions in a dockerfile, it will form or create a layer in the docker image. Bear it in mind that dockerfile is used to create an image and the kind of image to be created is light-weight.

A Layer created by a RUN instructions would add to the image size after a build is done.

It therefore means that if we have multiple RUN instructions in the dockerfile, it will create multiple layers in the image created making the image to become heavier than expected. This is why it is not recommended to have multiple RUN instructions in your dockerfile.

Each run instructions increases the size of the docker image. To deliver a portable docker image that is light-weight, you need to reduce the number of layers that is created by a RUN command.

COMMAND TO KNOW THE LAYERS THAT IS IN AN IMAGE

To know the layers that is in an image, you run

docker history ImageName/ImageID

This will tell you how many layers you have in your image and when the image was created

NOTE: if you run the above command to check all the layers in an image, anywhere you see a size number attached to any of the line, that is a layer. There always a size of what the layer has consumed. All of these give a total number of what will be the size of the image.

most command with sizes or that has weight sizes started with

ADD
COPY   etc

Prof check the docker history of mylamdmarktech/maven-web-app:29 image of he created earlier and other images that was created in his docker server.

He also pull alpine image from dockerhub by running

docker pull alpine

and then run   docker images alpine   or docker images

Her discovered that alpine image was the one with the lowest light-weight image among every other images. Alpine is also one of the linux distributors and also an AMI in amazon 

For us to avoid multiple RUN instructions, we can introduced what is called and-and separator simbol (&&)

backward slash \ is a sign in linux to say that the command continues in the second line.

FROM centos 
RUN yum install java -y \
&& yum install maven -y \
&&   yum install git wget unzip zip -y \
&&  git clone  github/maven-web-app \
WORKDIR maven-web-app
&&  mvn clean package \
&&  wget tomcat-9.0.58.zip \
&&  uzip tomcat-9.0.58.zip 
COPY target/*war  /usr/local/tomcat/ 
CMD ['catalina.sh', 'run']
#CMD sh'catalina.sh', run      this is not recommeded. It is while it is commented. not to run

       or
without the forward slash the \. you can just type in the command in oone line like below

FROM centos 
RUN yum install java -y && yum install maven -y &&   yum install git wget unzip zip -y &&  git clone  github/maven-web-app WORKDIR maven-web-app &&  mvn clean package &&  wget tomcat-9.0.58.zip &&  uzip tomcat-9.0.58.zip 
COPY target/*war  /usr/local/tomcat/ 
CMD ['catalina.sh', 'run']
#CMD sh'catalina.sh', run      this is not recommeded. It is while it is commented. not to run

Always put this into a file by vi. You can name the file  df2

DANGLING IMAGE

NOTE: Any docker image that has no repository and tag or where it say NONE on the repo and tag but has an image ID and has SIZE is called a DANGLING IMAGE.

This dangling image are sometime created when you are trying to create a docker image with via a dockerfile and there was an error that made the image not to be successfully built.

When you run

docker images   to list all th images showing the image name

but rum

docker images -q       to list only all the imageID

If you see any image tag and repo name as NONE. it is a dangling image

TO DELETE DANGLING IMAGE

It is very imperative to delete dangling image for size management so that it does not take so much of your docker server reseources. by running 

docker system prune

and then Type in y  to be complete or confirm the removal.

 NOTE

docker system prune is the command used to remove or delete all dangling image, all stopped containers, all dangling built cache, all network not used by any container

docker image prune 

are also be used to delete all dangling image only. after the command, run

docker images                 to verify the removal.

To delete all the images in your docker server or docker daemon or docker engine is

docker rmi -f $(docker images -q)


MULTI-STAGE DOCKERFILE
WHAT IS A MULTI-STAGE DOCKERFILE

Multi-stage Docker builds, let you write Dockerfiles with multiple FROM statements. This means you can create images which derive from several base iamges, which can help cut the size of your final build.

Docker images are created by selecting a base image using the FROM statement. You then add layers to that image by adding commands to your Dockerfile.

With multi-stage builds, you can split your Dockerfile into multiple sections. Each stage has its own FROM statement so you can involve more than one image in your builds. Stages are built sequentially and can reference their predecessors,
 so you can copy the output of one layer into the next.

Example of multi-stage dockerfile is below


=1#git
FROM alpine/git as repo
WORKDIR /app
RUN git clone https://github.com/LandmakTechnology/maven-web-application.git

=2#Maven
FROM maven:3.5-jdk-8-alpine as build
WORKDIR /app
COPY --from=repo /app/maven-web-application  /app
RUN mvn install 

=3#Tomcat
FROM tomcat:8.0.20-jre8
#COPY  /app/target/*war /usr/local/tomcat/webapps/maven-web-app.war
COPY --from=build /app/target/*war /usr/local/tomcat/webapps/

In the above dockerfile, you will notice that different RUN and COPY command is been staged or in different stages from stage1 to stage3. 

1. clone code   was to clone the code
2. Build        was to build with maven
3. Deploy in tomcat     was to copy the built artifacts and deployed it in tomcat webapps/

By doing this, it will also reduce the layers of the dockerfile making the dockerfile to be light weight.

vi into a file and name it msdf, press i to be able to type on it, paste the above script and then save-quit by   typing  :wq and press enter

To Build An Image From The File Above

When you run

docker build -t NewImageName -f absolutePathToTheDockerFileName .

                    or

docker build -t NewImageName -f absolutePathToTheDockerFileName absolutePathOfFileDirectoryOnly

                                 or

docker build -t NewImageName -f DockerFileName absolutePathOfFileDirectoryOnly

                             or

docker build -t NewImageName -f DockerFileName .

The duct sign means that you are building from the same directory where the dockerfile is situated

To know the absolute path to the msdf file by running, 

pwd

copy whatever you get, add / and add the name of the file to it. in the form of

/home/ubuntu/lap   is the present working directory

/home/ubuntu/lap/msdf   the absolute path to the file msdf

msdf  is the relative path

if we name the new image to be myimage, therefore it becomes

docker build -t myimage -f /home/ubuntu/lap/msdf /home/ubuntu/lap

      or

docker build -t myimage -f msdf /home/ubuntu/lap 

or

docker build -t myimage -f msdf .               if your dockerfile is in the same pwd

NOTE: any dangling images that is created as a result of the above build should be deleteed by

docker system prune

Dangling image was created in the cause of stage1 and stage2. We do not need the dangling image. We only need the final image which is the myimage

==============
 for Best Practices In writing Dockerfiles:
==============    
1. Use Linux lightweight  alpine images were applicable and possible. If you want to launch a linux server, it better you used the lightest weight linux distributors like alpine.

Some of the linux distributions are: redhat / ubuntu / centos / alpine etc. 

Alpine is the lightest weight in the above AMI

2. Reduce the number of layers by minimizing the number
   of RUN instructions in your dockerfile. 
   Muliple run instructions affects the performance of containers because it create heavy-weight containers. Example

    RUN yum install java -y \
      && yum install maven -y \
      &&  yum install git wget unzip tree nano zip -y \
      && git clone  github/maven-web-app 

     docker history    

3. Use only official images in docker hub for docker image. This is because official image are well updated, managed and controlled by docker. 

Docker inc. guarantees the reliability of any images in their hub

The vendor of dockerhub or docker is docker incorporated (docker inc.)

4. Avoid downloading unnecessary softwares and packages because this will increase your image size and make it not to be portable as expected.  

5. Use multi-Stage Dockerfile were applicable. 

6. scan base images. Always scan images before deploying the application by creating a docker container.

7. Use executable form over shell form for CMD and EntryPoint instructions

WHAT IS A BASE IMAGE?

The base image is the starting point for most container-based development workflows. Most base images are basicALLY Linux distributions like: Debian, Ubuntu, Redhat, Centos, or Alpine. 

Developers usually consume these images directly from Docker Hub, or other sources.

HOW DO YOU SCAN BASE IMAGE?

You can scan base image in a GUI way by going to your docker hub and enable scanning.

You can also scan images by running the command

docker scan ImageName

You can scan images via CLI before pushing the image to your dockerhub repository

NOTE:

We can deploy jenkins, sonarqube, etc as docker containers.

To deploy Jenkins as a container.

There are two ways youn can do this.

1. You can just run the below command and jenkins will be deployed as container. That is

docker run -d -p hostPort:ContainerPort --name NewNameOfContainer ImageName

Container-Port is the default software port number that you want to run as a container.

Jenkins is EXPOSED on 8080

This is usually stated on the dockerfile of jenkins. This port cannot be changed often.

Host-Port is the port you can assume. It is the port that you can always change and make sure that no other container or application running in your docker daemon is using the port. Otherwise it will lead to what is called PORT BINDING or Port overlap. This will affect port forwarding.

The command to check if the port you want to use has not be used on your docker server is

netstat -nutlp

nutlp = network udp tcp listening port

If you run the above command and it says not found. You need to install netstat tools in your server by running

sudo apt install net-tools    for an ubuntu VM

sudo yum install net-tools    for an redhat VM

then run

netstat -nutlp

You will see ports that are already in use in your docker daemon. You can now choose your hostport to make sure sure it is not in use. 

The port range is about 0-65355

Go to your dockerhub account, type in jenkins, copy the imageName

Let us say the image name is jenkins/jenkins   and let us name the container jenkins

Using CLI, go to your docker server and run the command 

docker run -d -p 8000:8080 --name jenkins jenkins/jenkins

Once you run the above command, it will first search for the image name in your docker daemon or server and if not found, it will go ahead and search for the Image name in publich domaine like dockerHub and if found, it will pull the jenkins image from dockerhub and they deploy the jenkins as a docker container.

2. You can also create a jenkins docker container by going to your your dockerhub account, type in jenkins, copy the image name  (jenkins/jenkin), go to your docker server in CLI and pull the image by running

docker pull jenkins/jenkins

It will the image from dockerhub and then you can now deploy the jenkins image as container by running

docker run -d -p 8000:8080 --name jenkins jenkins/jenkins

The fatest way would be the option 1.

HOW TO ACCESS THE JENKINS CONTAINER YOU JUST DEPLOYED

You can access it in your browser by

PublicIpaddress:HostPort/ContainerName   or PublicIpaddress:HostPort

NOTE;

Ipadress WOULD be needed if you used a linux distribution server to launch your docker. 

If you use your local computer resource like gitbash with docker up and running in your gitbas, you can use the below to access your software in your web browser

localhost:hostPort

The default username would admin and you can get the password by following the instaruction it will give to you

You can do the same thing to any other softwares like sonarqube, nexus that you want to run as containers

IQ

Explain a project that you did that you are proud of?

ANSWER:

I migrated jenkins running in linux VM or linux ec2-instance to jenkins now running as a docker container using JOB IMPORT plugin

VIDEO 119 

DOCKERFILE are inputs or instructions of what is used to build a docker image

Docker images are inputs needed to create a containerized applications

If you were asked what project do you support in landmark?

You will say, in landmark, we support java based applications or projects. 

We sopport applications wriiten a java language. This means that developers are written thesde applications in java

We also support NodeJS and .net based appplications

NodeJS applications are applications written in java-script

If you join a company that supports python based project. It is will not be a problem to you if you familiar with java based project

APPLICATIONS

Applications that we supports are

1. Monolithic applications: This an applications has all the modules coupled together to form a large module

Monolithic applications are single-tiered, which means multiple components are combined into one large application. Consequently, they tend to have large codebases, which can be cumbersome to manage over time.

Monolithic applications are applications in which all the components that formed the appplication are combined together to form a large application. These can be cumbersome to manage over time

EXAMPLE

Monolithic applications  - ebay for online shopping   

   [registration / login / cart / payment  / order] = ebay.java 

The above are multiple components or modules that would be developed by developers and then fuse or couple all these components together to form a single application called ebay.java where customer can now register, login to the application, create a cart, make payment, then order the item.

The developers has written the application to be one module or fused multiple modules together to become just a single module

2. Micro-service applications: They will de-couple or break down the single module into multiple modules or components. In the form of 

ebay-registration.java
ebay-login.java
ebay-cart.java
ebay-payment.java
ebay-order.java

Developers will write applications in a way the modules or component in the applications are de-couple or broken into different components to create a multiple modules of applications. Like having different components like 

registration module, login module, cart module, payment module and order modules.

Microservices shines more in containerization

SPRING BOOT PROJECT

There is a project in Prof. Github account that we are managing. It is called spring boot project. (spring-boot-docker)

The project repo is https://github.com/LandmakTechnology/spring-boot-docker

Try to see if you can clone the project repo.

HOW DOES THE SPRING BOOT FRAMEWORK WORKS?

spring Boot framework --> It's a java based framework. It will eliminate the boilerplate code. 

Using this framework Developers build projects very quickly.

It eases the process of developing, building, testing and deploying java-based application 

This spring Boot framework has what is called spring-Boot-suite that comes with an IDE

spring-Boot-suite: ide

Example of IDE are scr, buildscript, dockerfile

What is needed to do a build or create a java based artifact is just 

scr  and  buildscript (pom.xml)

1. src = The source code usually comes with 2 directory or folder called 

main and test

Form prof project repo in github, the file that you will see inside these directories are

main/hello.java    and test/test.java

2. The BuildScript usually contain pom.xml

3. dockerfile: What was also included in the project repo above by the developers, is a dockerfile. If a dockerfile already exist in the project, you need to create another one. You can only modify if need be

If you check inside the src under resources, click on application.yml file. You will see the application code that has been written by developers. like below

backend database

code 1

spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin

      Looking at the above, you would know that this application is meant to be deployed using mongo database

  If you go to application template under templata and click on index.htmi, you will see that user informations are there in the index file which mongo database is meant to capture or store these info or input.

  Some of these users info are 

 email address
  first name
  last name

If the developer had written a code like below
BACK-END DATABASE

code 2 

  spring:
  data:
    mongodb:
      host: mongo 
      port: 27017
      username: admin29
      password: admin123
      database: users
      authentication-database: admin

This is not a good practise. It would result to code smell. This type of code above are called HARD-CODING. You would not be able to modify info in this code except you write another src

WHAT IS HARD CODING?

Hardcoding is writing code that is not easily modified or reused. A hard coded information cannot be easily changed without changing the source code of the PROJECT itself.

The best practise is writing a code that can easily be modified by users without changing the source code or writing another source code. like the example in code 1

TO CREATE CONTAINER FOR THE ABOVE APPLICATION

1. DEPLOYING STATELESS APPLICATION AS A CONTAINER

This is a process of deploying application as docker container using a dockerImage that was built privately by you

To deploy the application in our project repo in prof github account above, you can run

docker run -d -p hostpost:containerport --name contanerName --network networkName \ 
-e USERNAME=YourUserName -e PASSWORD=YourPassword -e HOST=YourHostName ImageName

From the below Backend server 

spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin

MONGO_DB_USERNAME=YourUserName (choose any username= cliff12)

MONGO_DB_USERNAME=cliff12
MONGO_DB_PASSWORD=admin123
MONGO_DB_HOSTNAME=mongodb

The imageName in prof url project repo is called landmarktech/spring-boot-mongo

Most application server like tomcat, jboss, dbserver listens on port 8080

Therefore, the application can be deployed as container and these containers would be able to talk to another container when it is created under a custom network. That is 

docker run -d -p 3000:8080 --name spring-app --network fintech -e MONGO_DB_USERNAME=cliff12 \
-e MONGO_DB_PASSWORD=admin123 -e MONGO_DB_HOSTNAME=mongodb landmarktech/spring-boot-mongo

The application we just deployed as a container is a mongo database in which Prof actually created the image by himself

2. DEPLOYMENT OF A STATEFUL APPLICATION.

You can also deploy the application. You can deploy mongo database as docker container directly from dockerhub. by running

docker run -d -p 3000:8080 --name mongodb --network fintech -e MONGO_DB_USERNAME=cliff12 \
-e MONGO_DB_PASSWORD=admin123 mongo

No hostname here because you are using dockerhub. The image name is usually mongo in dockerhub. 

You have just deployed a mongo database image from a dockerhub and name the container that is created mongodb. it must have the same container name with the stateless application.

Note: The stateless application would depend on the stateful application to store or save info that was collected from its users.

STATEFUL APPLICATIONS

A stateful application is an app. that is used to save users or clients data from the activities of one session and would be able to retrieve that data to be used in another session or every other day

EXAMPLES OF A STATEFUL APPLICATIONS ARE

Cassandra
MongoDB 
and mySQL database

1. Structure Querry Language (SQL) OR  Relational Databases (RDS)

Examples are

  oracle 
  mySQL 
  PostgreSQL 
  MariaDB 
  aurora

In SQL OR RDS, user or clients data are stored in rows and columns format as below

STORES DATA IN ROWS AND COLUMNS (TABLES) 

   ID      Name      AGE 
   ID24    SIMON     55
   ID27    Evet      66 


2. Non-RDS (noSQL)

Examples of Non-RDS or NoSQL are below

  mongo 
  casandra 
  dynamoDB 
  REDSHIFT

In Non-RDS or NoSQL, users or clients data are stored in json format like below

   {
    name: 'simon'
    id: 'id20145'
    age: 55
   }


In aws, we have what is called MANAGED DATABASED AND SELF MANAGED DATABASE

1. SELF MANAGED DB:  This is the database you can install and managed by yourself by creating an EC2-instance and then install a database software on the instance making the instance to become and database server.

Some of the database that you can install in your instance are
    mySQL / PostgreSQL / oracle         
            mongo / casandra 
You would be required to create DB username and password for your database at the end
           username = admin 
           password = admin123 
    login into the db using your credentials

MANAGED RDS : This is the database that aws equally provides for customers. The aws would be the one to manage the database

Remember that we are deploying 2 applications. Stateful and stateless application

DEPLOYMENT OF A STATEFUL APPLICATION.

These are the type of applications like databases. Database application is an example of a stateful application.

docker run -d -p 3000:8080 --name mongodb --network fintech -e MONGO_DB_USERNAME=cliff12 \
-e MONGO_DB_PASSWORD=admin123 mongo

DEPLOYMENT OF A STATELESS APPLICATION.

docker run -d -p 3000:8080 --name spring-app --network fintech -e MONGO_DB_USERNAME=cliff12 \
-e MONGO_DB_PASSWORD=admin123 -e MONGO_DB_HOSTNAME=mongodb landmarktech/spring-boot-mongo

IQ.

How can docker containers communicate with each other?

ANSWER: They can communicate with each other via DOCKER NETWORK


DOCKER NETWORK

Since we are dealing with a microservice project, using microservice example above. Thst is

ebay-registration.java
ebay-login.java
ebay-cart.java
ebay-payment.java
ebay-order.java
mongoDB.java       it is needed to be able to capture or store user info or inputs

Each of these application can be deployed as docker containers to become

ebay-registration.java container
ebay-login.java container
ebay-cart.java container
ebay-payment.java container
ebay-order.java container
mongoDB.java container 

Each of these container is expected to be able to talk or communicate with each other at every phase despite the fact that it is a microservice project. The tool that can make them to be able to communicate with each other is the use of DOCKER NETWORKING.

Note: Each of these container would have an IP address. 

We can have multiple containers deployed in one docker host or these multiple containers deployed in multiple docker host

HOW DO DOCKER NETWORK WORKS?

Docker containers can ONLY communicate with each other if they are in the same docker network 

When you install docker server or docker software in your ec2-instance, the docker software comes with 3 kinds of NETWORK by default. They are

BRIDGE
HOST
NONE

Any container you create would be created under the default bridge network by default

BRIDGE NETWORK

This is a networking device that forwards traffic between servers that are in the same network

TYPES OF BRIDGE NETWORK

Default bridge network and Custom bridge network

1. DEFAULT BRIDGE NETWORK.

This is the type of network in which contains created in this network communicate with each other only with their IPs.

	QUESTION

What is the reason why Default Bridge Network is not recommended for container creation?

Answwer: 

This is because IP address are DYNAMIC meaning that it can change anytime. For example, you login to your portal and you created a CART, maybe somehow your CART was deleted and you decided to create another CART, the ip adddress of the first CART will be different from the ip of the second CART. it will change and if that happens, when the new CART try to communicate with the PAY, it will not be successful because PAY only recognizes the ip of the first CART.

The communication between containers created under default bridge network has a short life span.  This is because If one of the container is down and another container is created as replacement, it will come with a differrent ip addresss, in-which other containers may not recognize and that can collapse the entire infrastructure. It will require you to begin to reconfigure the infrastructure in order to match up the ip with other container. This is really cumbersome. This is mostly applicable or happens when the multiple containers is deployed in multiple docker host. If think if these multiple containers are deployed in just one docker host. This miscommuincation would not arise but is is not recommended to host these multiple containers in just one docker host to avoid a single line of failure

This is why creating containers in DEFAULT BRIDGE NETWORK is not recommended

2. CUSTOM BRIDGE NETWORK.

This is the type of network in which containers can talk to each other using ip or container name.

The container name is the host name

	QUESTION

What is the reason why Custom Bridge Network is recommended for container creation?

Answwer: 

This is also because of the fact that if any of the conatainer is down and its ip changes, it will not break any communication with other containers because it still have the same container name/hostname. The ip address can change at anytime but the hostname remains.

NOTE: Docker containers can ONLY communicate with each other if they are in the same docker network

If you are in your docker server and run

docker network ls

You will see that it will display 3 type of networks

Bride
Host
None

The bridge that you see there is the default bridge. 

We have a project for a fintech clients. So will need to create a custom bridge network for reliablity and substanability purpose. 


Any container you create in your docker server is created on a default bridge network by default

You can create a custom networking br running

docker network create fintech  or docker network create fintech -d bridge

It is the same result that it will produce. It will create a custom bridge network called fintech  with bridge driver  -d

-d = DRIVER       you are passing a driver. whether you pass d or not, there will still be a DRIVER called bridge, as far as you run the command

docker network create fintech 

To confirm your network, run

docker network ps

LET US CLONE THE ABOVE PROJECT FROM PROF TO OUR DOCKER SERVER.

git clone https://github.com/LandmakTechnology/spring-boot-docker

Before you clone this repo, make sure that maven is running in your docker server and the same time git is up and running also.



If you have an existing docker container, you can delete and then clone the above spring-boot project in prof git repo by running

If you have stopped or exited container, if you run

docker system prune

It will remove all the exited or stopped containers, dangling containers, un-used networks etc

git clone UrlOfTheRepo

You have a spring-boot-docker directory. cd into the directory and ls

You will see the project scr, pom.xml, Dockerfile and others but no target directory. 

If you want to see the project file, cd into scr, cd into resources and cat application.yml  that is where the project file is


spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin

server:
  port: 8080

  In this applicatiuon, you will know that it will require mongodb to capture datas or info of the clients that would be stored here.

Come to the project repository and make sure you create artifact by running

You can create artifacts by running

mvn package

But if there is an existing target directory containing artifacts and you wish to delete the artifacts, you cam run

mvn clean package

It will delete and create new artifacts.

Since we have a docker file called Dockerfile, let us cat or vi into our file to see whats all about by running

cat Dockerfile

let us see if we have a docker image by running

docker images

It will tell you if yuou have any image in your docker server

Let us build the docker image by running

docker buil -t imageName:versionOrTagNumber .


If you want to build the image and push it to dockerhub repo that already has a docker image name of mylandmarktech/spring-boot-mongo. This means that the repository is spring-boot-mongo, we should then make the image name we are about to build to assume the same name with what we have in our dockerhub repo but with different tag. That is 

spring-boot-mongo and let the  tag be 29

docker build -t spring-boot-mongo:29 .

If you run the command, it has created another version of the docker image that is in dockerhub.

You can then register the spring-boot-mongo:29 under a repository called spring-boot-mongo by running

docker tag imageName RepositoryName

docker tag spring-boot-mongo:29 spring-boot-mongo

You can now push the image spring-boot-mongo:29 to your spring-boot-mongo repo by first of all login to your dockerhub account via CLI in your docker host or docker daemon (it will reuire you docker username and password) by running

docker login -u username

It will ask you for password and then put in your password

You can now run the below to push the image to your dockerhub by running

docker push repoName

docker push spring-boot-mongo

Note: You only push docker repo to dockerhub and not the imagename because the image is inside the repo.


DEPLOYMENT OF THE ABOVE STATELESS APPLICATIOPN WHICH REQUIRES A STATEFUL APPLICATION

Let us deploy a stateless application that requires a stateful application (which is databse).

The stateless application above is developed in such a way that it will need other appplication like a stateful application to make it to become a micro-service for deployment. This means that, there would be two applications that is to be deployed but would be able to communicate with each other.

NOTE: 

For the 2 appliucation to talk to each other, you must pass a network for communication between the 2 applications. Also based on how the application was built by the developer, where environmental variable were also built by the developer as show below,

Let us creat a fintech network. 

We can create network by running

docker network create fintech


This will create a custom bridge network

Looking at the code that the developers has built, we can see that there are env (environmental variables) variables, as shown below

spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin


e = environmental variables

The first environment variable -e on the above code is MONGO_DB_HOSTNAME, second is MONGO_DB_USERNAME and third is MONGO_DB_PASSWORD


To create a container we need to pass the above custom bridge network called fintech and also pass -e as an environemnt variable to the command and set or assign the neccessay values to the environmental viraiable.


Therefore, Stateless Application Deployment would be

docker run --name ContainerName -d -p hostport:containerPort ImageName --network \
networkName -e MONGO_DB_HOSTNAME= DatabaseContainerName -e MONGO_DB_USERNAME=AssingUseerName \
-e MONGO_DB_PASSWORD=AssignPassword

Let the containerName for the stateless application would be springapp and that of the stateless application (database application) is mongodb, Image name of the container is the image that we have just created which is mylandmarktech/spring-boot-mongo:29 and that of the stateful is mongo because we want to build from the mongo image that is found in the public dockerhub. let the username of the database be admindb and the password of the database be admin123

From my own school of thought, It would be better to even create mongo container called mongodb first before created the springapp container. Since you already know that it is mongo database that you would need as the stateful application from the code that the developers have written.

NOTE: What we stand to achieve here is that this stateless application is an application that would collect user datas or infos and then store these datas in a stateful application called mongo database

Every database must have a username and password. You must create user name and password for database.  Therefore, to deploy the above application via a container would be


docker run --name springapp -d -p 3000:8080 mylandmarktech/spring-boot-mongo:29 --network \ fintech -e MONGO_DB_HOSTNAME=mongodb -e MONGO_DB_USERNAME=admindb -e MONGO_DB_PASSWORD=admin123

Note: you use \ (forward slash) when running a command whose line is long, to show that the same command continued in the next line.

Before you run the application please confirm if there is anything like fintech network by running

docker network ps

If there is no fintech custom bridge network, run the below to craete fintech bridge network

docker network create tesla

Then run   docker network ps   you would see that it has been created

After you have created the network. Go to your working directory and run the command

docker run -d -p 3000:8080 --name springapp mylandmarktech/spring-boot-mongo --network \
tesla -e -e MONGO_DB_HOSTNAME=mongodb -e MONGO_DB_USERNAME=admindb \
-e MONGO_DB_PASSWORD=admin123

As soon as you run the above application, the stateless applicationb would be deployed via a container.

You can now access this container on your web browser by

dockerServerOrcontainerIPaddreess:hostport

You can know the public IP address by running

curl ifconfig.ca   or curl ifconfig.co     on your server 

If you paste that on the web, it will display your application in GUI. If you put in the details or try to register by putting in your details, you will see that after putting in your details, you cannot even submit. This is because the mongodb that is suppose to store the data that you have just entered is not yet deployed. That is why you are unable to submit the registration. We need to deploy the mongo database.

stateful application deployment

In stateful application, there would not be any port mapping here is will are deploying from an image in dockerhub and its a database image

docker run ContainerName ImageName

If you type mongo in your dockerhub public search, you will see that the official image name is just mongo. If you click on it, it will tell you how to pass or use the environment variable. Copy the environment, that is 

environment:
       MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example

they used root as a username here and they used example as the password

Let our username and password be admin and  admin123 respectively

This means that first -e (environmental variable) is MONGO_INITDB_ROOT_USERNAME, second -e is 
MONGO_INITDB_ROOT_PASSWORD

Also go to where it says WHERE TO STORE DATA by scrolling down and then you will see the command to execute when you want to store users data under where it says START YOUR MONGO CONTAINER LIKE THIS. It is in number 2.   copy the command also. in the form of

docker run --name some-mongo -v /my/own/datadir:/data/db -d mongo   

and you now pass your environmental variable or environment and the name of the custom bridge network in it. as represent below

That is 

docker run --name ContainerName -v \ 
storageDirectory/mountPointOfFileSystem:ContainerMountPoint -d imageName --network tesla \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

Let the storage directory be tmp directory  /tmp
Let the name of the moint point or name of file be mydata
The default container mount point for mongo is always /data/db    This container mountpoint of mongo is constant and cannot be changed. You must use the container mount point of what is specified by the vendor. The vendor here is mongo.

We already know that the container name is mongodb

docker run --name mongodb -v /tmp/mydata:/data/db -d mongo --netwrok fintech \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

NOTE: the -v represent volume. 

You can see that we did not actually create the mydata volume or file in tnp directory using

mkdir /tmp/mydata

/tmp/mydata volume is created automactically if you pass volume (-v) and the volumeName along with the command above when creating a database container.

Here, we decided to create the volume in tmp directory automatically. 

If it says no permission, use sudo and it ask for password, kill the process and exit to ubuntu user and run the command again before you can go back to your working directory.

NOTE: the hostname, username and password of the database must be reflected in any of the application before the application if such application wants to use the database to store its users info or datas.

You can also decide to use your own present home or directory on the above command in order to suit what you are doing. That is

Run pwd

if the pwd is /home/landmark/

Then

docker run --name mongodb -v /home/landmark/mydata:/data/db -d mongo  --netwrok fintech \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

That is, you want your storage directory and file to be in your present home directory.

Let us use the first command

docker run --name mongodb -v /tmp/mydata:/data/db -d mongo --netwrok fintech \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

v = VOLUME
d = detachable mode.

If you run the command in your server, it will pulll the mongo image from the mongo libary in dockerhub and then create the mongo database as a container.

Note: When creating the container, you need to pass -d to it so that it can be created in a detachabe mode so that it will not be attached to the docker engine alone. If you made a mistake by not pass -d when creating the container, you need to delete the contanier and then recreate it.

Since you have been able to deploy your mongodb container, if you now go back to access your springapp in your web browser  by

dockerServerIPaddreess:hostport

and enter the required info and click submit, you will see that the data you enter has been stored. You will see it right there.

Volumes:
=======

During our mongo database container deployment, we created mountpoint on our docker host or docker engine to be /tmp/mydata which was mapped with the default mountpoint of mongo database /data/db

This mapped point or path to our file is where datas or info that is entered into our springapp is stored or recorded.

Let us assume that our mongodb container is down by maybe stopping the container by running

docker stop mongodb    or docker pause mongodb

If you refresh the  dockerServerIPaddreess:hostport on your wbe browser, you will see that the datas there before will disappear. This is because the mongodb conatiner is down

If the start up the mongodb container again by

docker start mongodb

When you refresh your web browser, you will also see that the data are there again

If you also delete the mongodb by running

docker rm -f mongodb



When you refresh your web browser, you will also see that the data are no longer there.

But if you check mydata file in your tmp dir., of your docker host or engine, you will that the mydata file and user datas is still there even though the mongodb was deleted.

You can retrieve this info or datas back if you create another mongo database server with the same mount point hostname, username, password, or all the informations used in creating the first one. Once you have created the mongodb container. Refresh your web and you will see that the datas will appear. 

The datas in the docker host will synchronize into the mongodb container mount point because it has the same mount point

When you delete mongodb and also delete mydata file   that is

/tmpt/myapp  in our docker server by running

sudo rm -rf /tmpt/myapp

If it failed to remove, it means the current user kim does not have sudo access.  you have to exit to to ubuntu user and rerun the command

sudo rm -rf /tmpt/myapp

and then come recreate the mongodb container with the same information as when you started the deploymet, if you refresh, there would not be any datas deplaced because no data to synchronize.

LESSONS LEANT HERE IS THAT

The volume that you have passed or created in the above docker host that was mapped with the default mongdb volume is called BIND-MOUNT-VOLUME

BIND-MOUNT-VOLUME is not a persistent or reliable volume. This is because the volume is not managed by the docker process


Note: This 2 applications are running on 2 different containers communicating together because they are in the same network. These containers are also in the same docker host or docker egine. Note. This is not also recommended to be on the same docker engine but to save cost, we are just using it.

Bind Mounts:

Bind mounts may store data anywhere on the host system. 
They may even be important files or directories. 
Non-Docker processes on the Docker host or a Docker container can modify them at any time.

If you have any data that is not managed by docker is dangerous

Bind mount volume is not managed by docker

Remember that we created a directory or we create a file inside tmp directory 

/tmp/mydada
 
You can create it by running 

mkdir /tmp/mydata 

HOW DO WE MANAGE VOLUMES WITH DOCKER?

We can manage volume with docker using a PERSISTENT VOLUME.

DOCKER VOLUMES

Docker Volumes

PERSISTENT VOLUME

Volumes are stored in a part of the host filesystem which is managed by the 
Docker service 

(/var/lib/docker/volumes/ on Linux).

ILLUSTRATION OF THE ABOVE

When you install docker application in your ec2-instance, it comes with your docker home directory

/var/lib/docker

For BIND-MOUNT VOLUME, we create the volume by running

mkdir /tmp/mydata

But To achieve a PERSISTENT VOLUME, we must create our own or custom volume

Non-Docker processes should not modify this part of the filesystem. 

Volumes are the best way to persist data in Docker.

let us create a DOCKER VOLUME by running

docker volume create db-backup    

you can verify by running by running the belwo command

docker volume ls or docker volume inspect

You will see the db-backup you just created

This volume that you just created is your PERSISTENT VOLUME

If you run

ls    

you will not see any docker volume like db-backup because db-backup is being managed by docker processes and so if you want to access the volume, you must run

docker volume db-backup ls or docker volume inspect db-backup

The first volume or file system, /tmp/mydata was not created as a docker volume and so if you run

ls

You will see the file

TO KNOW THE MOUNT POINT OF THE PERSISTENT VOLUME 

To know the mount point of the volume you just created, you run

docker inspect VolumeName

docker inspect db-backup

You will see the details about the db-backup volume you have in your docker engine, you will see the absolute path to the mount point. Copy the absolute path to the mount point. This is like below

"Mountpoint": "/var/lib/docker/volumes/db-backup/_data",

LET US CREATE ANOTHER MONGO DATABASE CONTAINER USING THE SAME COMMAND BELOW

Let first of all remove the existing mongodb contaniner by running

docker rm -f mongodb

Let us redeploy our stateful application which is our database application below

docker run --name mongodb -v /tmp/mydata:/data/db -d mongo --netwrok tesla \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

Let us replace the docker mountpoint volume with db-backup   that is

docker run --name mongodb -v db-backup:/data/db -d mongo --netwrok tesla \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

If you run the application, you will see that a container was created. 

If you access the springapp on web and put in datas as required, you will see that your info would be stored.

NOTE:

Even if you did not create a mount point volume first on your docker sever, if you run the above command. that is 

docker run --name mongodb -v db-backup:/data/db -d mongo --netwrok tesla \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

The mountpoint volume db-backup will still be created because you pass a volume -v and volumeName.

example

Let us remove the present mongodb

docker rm -f mongodb

let us name our volume as myfile, you can create another mongo conatianer by running

docker run --name mongodb -v myfile:/data/db -d mongo --netwrok tesla \ 
-e MONGO_INITDB_ROOT_USERNAME=admindb -e MONGO_INITDB_ROOT_PASSWORD=admin123

If you run

docker volume ls

You will see that both volume are seen.

These voulme (db-backup and myfileis) are persistent volumes. It is been managed by docker processes or docker engine.

If you refresh the springapp in your web browser, you will see that the data or info entered into the app is stored as a result of the persistent volume class that was mounted with the mongo database base default volume during the creation of lauching of the mongodb container.

The bind mount volume that we created earlier in tmp directory is not manage by docker processes because it was created in tmp directory

NOTE: any files on tmp directory is removed or deleted when there is a system reboot or restart. This is because tmp directory is a directory in which temporal files are stored and would be remove when there is a system reboot.

QUESTION ASK?

How do we manage volume with docker?

Answer: We should manage volume with docker using  A PERSISTNET VOLUME and not a BLIND MOUNT VOLUME.

A persistern Volume class is a volume that is being managed my docker processes.

A Blind Mount Volume class is a volume that is not manage by deocker processes. such volume is not reliable for data storage.

NOTE: Once a PERSISTENT VOLUME is created, it is going to be stored in the default mount point below

var/lib/docker/volumes/volumeName/_data       that is

var/lib/docker/volumes/myfile/_data

So there, the data entered is stored in _data

The aboslute path to _data is var/lib/docker/volumes/volumeName/_data

If you ls var/lib/docker/volumes/volumeName/_data 

A persistent volume was created using the command

docker volume create volumeName

The mount point will now be var/lib/docker/volumes/myfile

NETWORK VOLUMES

Question:

What would happen if your docker host server or your docker engine that has the docker volume created that are in use is down?

Answer: We can solve that problem by created an EXTERNAL STORAGE SYSTEM MOUNT POINT that will be mapped with the default database mount point. This external storage mount point will be a storage backup of our volume containing user datas should incase the docker host server is down. The external storage mount point is created outside our docker server.

Some of the external storage resource that can be utilized to created storage mount point are

1. s3
2. ebs
3. efs/nfs
4. gpd   (google persistent disk)
5. mad   (microsoft azure disk)

With this external backup of our voulme, our data or usr datas can be accessed

LET US DEPLOY OUR APPLICATION USING ONE OF THE ABOVE OPTION LIKE ebs

TheRe is a software that can enable us to be able to deploy our application using one of the external storage mount point. This software is called NETWORK VOLUME

let us create a network volume using EBS

One of the network volume option in which we can install via EBS is called the REXRAY

DOCKER REXRAY

REXRAY is a docker plugin

If you want to deploy REXRAY   for ebs volume, you run the below command

docker plugin install rexray/ebs EBS_ACCESSKEY=$ACCESSKEY EBS_SECRETKEY=$SECRETKEY

For you to use this docker plugin in ebs volume, you need to authorize docker to manage ebs volume using access key and secret access key

1) Create IAM User with EC2 Full Access and user access key & Secret Key of the same.

Since your docker server does not have an IAM security, you can confirm by going to your ec2 instance in aws and click on security, you woill see that it is not attached to IAM.

All you need to do is to go to IAM in aws console and create a user and grant the user admin full access. If you already have a user and the user to not have admin full access, you can elevate the user role and permission to admin full access by clicking add permission, click on attach policy directly and then tick admin full access

once you have saved, click on the user again and then click on security credential and then click on create access key

copy the access key token =   AKIAVA7MDXUTVD24FMUQ

and then click show and copy the secrete access key = n8f9WAMiN1PW8JlOzuVrkyEPKxOF5Kfu5miV

  Replace your access key & secret on the command below.

   docker plugin install rexray/ebs EBS_ACCESSKEY=$ACCESSKEY EBS_SECRETKEY=$SECRETKEY

   docker plugin install rexray/ebs EBS_ACCESSKEY=AKIAVA7MDXUTVD24FMUQ EBS_SECRETKEY=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

 This access key will grant REXRAY full admin access to manage the ebs volume of our server

 NOTE: 

 As soon as you run the above command on your docker server and type in yes, the rexray software will be installed and your docker engine now have full access to your aws account

 to verify if successfully installed, run

 docker plugin ls

 You will see the rexray 

 If you also run

 docker volume ls

 You will see that it will list all the volumes you have in your docker engine and also list the ebs volumes that is attached to your ec2-instances or servers that you have in your aws account. 

This means that docker rexray is now the one managing all your ebs volume in your aws account

LET US CREATE AN EBS REXRAY VOLUME

You can create an ebs rexray volume with the command below

 docker volume create --driver rexray/ebs --name ebsvolumeName

                   or
 
 docker volume create -d rexray/ebs --name ebsvolumeName

 If you run the above command without passing the driver -d and the driverName, it will create a volume on the default driver or local driver. That is 

 docker volume create ebsvolumeName or docker volume create -d local ebsvolumeName

 the both are the same.

Let the ebvolumeName be ebsvol.

The run the below command in your docker engine

docker volume create -d rexray/ebs ebsvol

ebsvol would be created

If you run

docker volume ps

dh -f

You will mot see any ebsvol because it was not created locally or in default or local driver

You will see it in your docker engine

If you go to your aws account and seacr ec2, and then click instances, and then click volume, you will see the ebsvol that we just created.

Let us use this external volume mount point called ebsvol to map with the default mountponit of the mongo database to create our mongodb container.

Let us firsr of all delete the existing mongodb container to over overlap by running

docker rm -f mongodb

and then run the below command to create mongodb container

  docker run --name mongodb -d -v ebsvol:/data/db --network tesla  \
 -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=dev@123 mongo 

If you refresh your web browser and you will see that your mongodb will be capturing or storing user datas.

If our docker server goes down, we can still retrieve our data because the data is backup in ebsvol in our ec2 instance which is an external volume to the docker host but if that data is backup up in our persistent volume of the docker host, if our docker server is down or deleted, the data would be lost

EXAMPLE

If you stop or delete our docker engine1 in aws, our springapp container would stop accepting data input because our database is no longer able to store data. 

You can retrieve our volume if you created another docker host2 and then attach the ebdvol to the docker engine2 you just created. 

Mkae sure you install rexday in your new docker engine2 with the same info you used in installing rexray in first docker engine1

Docker Compose
==============

The number of microserveices that we have run so far is just 2 microserviceS. That is springapp application and mongodb application

Let us assume we want to run multiple microservices. This is where docker compose come in place.

DOCKER COMPOSE is a tool for defining and running multiple containerised applications.

In real time one application can have more than 7 micro-services:

For example:

An e-commerce java based web application for ebay can have the below microservices application

    Registration
    Login
    checkout
    Payment
    Order
    mongodb  

We want to ensure that these microservice applications talk to each other. This can be possible via docker networking.

To create docker network for ebay, run

  docker network create ebay  

To deploy each of these application as a container, it means that we will be running 6 commands to deploy each of the applications.

Note:   
     -v = volume mapping  
     -e = pass Environmental variables 
     -d = run the container in detachable modev   
     -p = portMapping/ 
     -it= run the container in interactive mode 
     --name=assign a name to the container  
     --network = define a network for the container 

     Each of these application has am image

1. Registration

if the docker image is mylandmarktech/registration  then to deploy would be

   docker run --name registration -d -p 1000:8080 --network tesla \
  -e MONGO_DB_HOSTNAME=mongodb -e MONGO_DB_USERNAME=devdb -e MONGO_DB_PASSWORD=dev@123   \
    mylandmarktech/registration

 2. login

if the docker image is mylandmarktech/LOGIN then to deploy would be

   docker run --name login -d -p 2000:8080 --network tesla \
  -e MONGO_DB_HOSTNAME=mongodb -e MONGO_DB_USERNAME=devdb -e MONGO_DB_PASSWORD=dev@123   \
    mylandmarktech/login

 until it gets to 6.  which is mongo database.

 6. mongodb

 docker run --name mongodb -d -v ebsvolume:/data/db --network tesla  \
 -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=dev@123 mongo 

These deployment of these multiple 6 microservices application would be cumbersome when deploying the individual application and you may be prone to error.

You can create what is called a docker compose file or script and paste all of these commands of these various appplications inside the file and then deploy as a single command.

This is what is called using IaaC to overcome multiple run commands.

That is, using docker compose file to overcome multiple run commands

docker compose file is an IaaC = infrastructure as a code

DOCKER COMPOSE FILE

For you to use docker compose file, you need to install what is called docker compose

With Docker Compose we deploy/create all of the above 6 microservices applications with just a single command using compose file. by the command belwo

 docker-compose up -d 

 TO install docker compose in our ubuntu docker server, we run the below

sudo apt install docker-compose

Note: make sure you exit to ubuntu user to be able to run dis sudo command because your kim or other user may not have sudo privileges except you have already configured sudo privileges in your custom user.

We will define all the serivces(cotainers) details in compose file using compose file we can deploy multi container applications.

Docker compose file start with version

Version 3.1 is what we are using now in our environment. You can check other version online by pasting docker-composer version in google. You will see different version. Each version depends on the docker engine releases.

Let us check our present docker engine or host or server release that we are using by running

docker -v

You will see the version release of your docker engine. You can determine which docker compser version you want to use depending on what comes out in google search but we are using version 3.1


LET US USE OUR 2 MICROSERVICES EXAMPLE THAT WE HAVE BEEN USING

That is

springapp stateless application and mongodb stateful application

NOTE:

A stateless application is the application in which the container or deployment server does not store data or other information of the user

A stateless application is an application in which the container or deployment server store data or other information of the user. A database application is a stateful application.

docker run -d -p 3000:8080 --name springapp mylandmarktech/spring-boot-mongo --network \
tesla -e -e MONGO_DB_HOSTNAME=mongodb -e MONGO_DB_USERNAME=admindb \
-e MONGO_DB_PASSWORD=admin123

 docker run --name mongodb -d -v ebsvol:/data/db --network tesla  \
 -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=dev@123 mongo 

Your docker compose file will have the below at high level. The below must be present in your docker compose file

version: '3.1'
services:
volumes: 
Networks:

version: '3.1'
services:
   springapp 
   mongodb
volumes: 
Networks:


version: '3.1'
services:
  springapp:
    image: mylandmarktech/spring-boot-mongo
    restart: always  
    ports:
      - 5000:8080
    networks:
      - ebay 
    environment:
      - MONGO_DB_HOSTNAME=mongod
      - MONGO_DB_USERNAME=devdb
      - MONGO_DB_PASSWORD=devdb123
  mongodb:
    image: mongo  
    restart: always 
    networks:
      - ebay 
    volumes:
      - data28:/data/db    
    environment:
      - MONGO_INITDB_ROOT_USERNAME=devdb 
      - MONGO_INITDB_ROOT_PASSWORD=dev@123
volumes: 
  data29:
    driver: local  
networks:
  ebay:
    driver: bridge


Note: You can list multiple volumes and network. The driver = local means that docker server will create volume for you called data28, driver = bridge means that docker engine will create bridge network for you called ebay

NOTE:

If the volumes and the networks is already existing

That is. If the existing volume is data and the network is aws-network, You can edit volumes and networks to the below as replacement

Let us assume volume and networks already exist. You can create it if it does not, and use the below compose file structire


version: '3.1'
services:
  springapp:
    image: mylandmarktech/spring-boot-mongo
    restart: always  
    ports:
      - 5000:8080
    networks:
      - ebay 
    environment:
      - MONGO_DB_HOSTNAME=mongod
      - MONGO_DB_USERNAME=devdb
      - MONGO_DB_PASSWORD=devdb123
  mongodb:
    image: mongo  
    restart: always 
    networks:
      - ebay 
    volumes:
      - data29:/data/db    
    environment:
      - MONGO_INITDB_ROOT_USERNAME=devdb 
      - MONGO_INITDB_ROOT_PASSWORD=dev@123
volumes:
  data29:
    external: true
networks:
  ebay:
    driver: true


Sometimes, some project can come with a docker compose file.

Above is how to construct a docker compose file. 

Let us copy the above configuration and paste in in a file name called 

dcoker-compose.yml

vi dcoker-compose.yml

paste, save and quite

First of all remove the mongodb and springapp container from your server by running

docker rm -f mongodb springapp

If it did not, you can stop the container before deleting by running

docker stop mongodb springapp

SYNTAX ERROR

Since we have install docker-compse of this server, let us first of all check the syntax of the docker compose file that we have created. if there is any syntax error with the content in our file by running

docker-compose config

If the file has a problem, it will tell you something is wrong with a particular line. And that you should fix it

You can now deploy the 2 microservice application on a docker-compose file by running

docker-compose up -d

-d = detachable mode


You will see that it will create the network ebay because it does not exist before.

if you run

docker ps

You will see that the 2 containers are running

If you access the application on web by pasting

DockerpublicIpaddress:hostport

iF you are unable proceed or save after writing on the springapp, it means that sometime is wrong with your database app. If you troubleshoot and found out the error, you can kill the entire docker-compose file and the container by running

docker-compose down

If you run docker ps, you will see that the container no longer exist and then you can redeploy when the problem has be solved.

OTHER USEFUL COMMANDS THAT CAN GO WITH A DOCKER COMPOSE

1. Deploying a docker compose file with custom name.  You run

docker-compose -f CustomComposeFileName.yml command

command may be   up -d   or dowm  or config

docker-compose -f CustomComposeFileName.yml up -d  = to create container

docker-compose -f CustomComposeFileName.yml down  = to kill container and docker-compose file

docker-compose -f CustomComposeFileName.yml config = to check for syntax error

Note: 

By default, the file must end with .yml  or .yaml   and the file must start with 
docker-compose or compose

That is while you must pass -f as filename just before the custom file name you want to name it to be. let the end of the file be .yml or .yaml

Let us rename of docker-compose.yml to docker-compose-cliff.yml by running

mv docker-compose.yml docker-compose-cliff.yml

first of all kill the existing container by running

docker-compose down

then redploy the apps by running

docker-compose -f docker-compose-cliff.yml up -d

if you refresh your browser, you will see that our user datas or info are still intact and saved.

If you want to bring down this applications, you must also pass the file name that was used to deploy the apps as containers by running

docker-compose -f docker-compose-cliff.yml down

Note: You can also list conatiners running with docker-compose command by running

docker-compose ps.

For a container created by a customer docker-compose file, we run

docker-compose  -f customerDockerComposeFileName  ps

If you run the below command

docker-compose

It will give you the list of the command you can run with docker command

Docker Compose Commands:

  config             Validate and view the Compose file
  create             Create services
  down               Stop and remove containers, networks, images, and volumes
  exec               Execute a command in a running container
  help               Get help on a command
  images             List images
  kill               Kill containers
  logs               View output from containers
  pause              Pause services
  port               Print the public port for a port binding

  ps                 List containers. That is docker-compose ps.

                    For a container created by a customer docker-compose file, we run

                    docker-compose  -f customerDockerComposeFileName  ps

  pull               Pull service images
  push               Push service images
  restart            Restart services
  rm                 Remove stopped containers
  run                Run a one-off command
  scale              Set number of containers for a service
  start              Start services
  stop               Stop services
  top                Display the running processes
  unpause            Unpause services
  up                 Create and start containers
  version            Show the Docker-Compose version information




# In Normal(Standalone) Docker Server We can use below command to create a containers.
docker-compose up

# In docker swarm we will use below command to deploy services using docker compose.
docker stack deploy --compose-file docker-compose.yml <stackName>


NOTE: 

QUESTION

WHY IS DOCKER DEPLOYMENT LIMITED?

Answer: It is limited because when you deploy your applicatios as docker containers, you can not scale and if there are problems in your applications when you have like millions of users accessing the applications and you are deploying with docker, you cannot increasing the number of endpoints, the number OF replica, the number of containers from the same host. With docker, You can not scale, like for example, based on increase in user traffic, let us increase the number of our container running mayby from 10 to 20 containers. Docker cannot not do that.

Another limitation of docker is that it cannot intergrate with load balancer. Load balancer is lacking in docker. That is why it is not recommended to deploy app as a docker container

Another limitation of docker is that docker does not support what is called multi-hosting. That is, deploy one application in multiple docker host (may like 5 different docker host or server)

You can group docker servers creating a cluster of docker server.  But you can do all these things in kubernetes host. Kubernetes supports multi-host

Based on these limitations, we would not use docker to run containers but to only use docker to build images and push it to docker registry. Dockerfile would be very instrumental and useful to create image. 

Therefore, we use docker to create images, and we use kubernetes for container orchestration and managements

VIDEO 124

KUBERNETES BEGINS

What is kubernetes?

Kubernetes is an open source orchestration platform or engine or server that are used in managing and deploy containerized applications.

Question asked in class.

If our applications are not containerized, what application will it be?

Most of the time, containerized applications are always micro-service application. If they are not containerized, they would be a monolithic application.

Responsibilities of Kubernetes are

1. deployment of containers also called pods in k8s
2. scaling and de-scaling of containers
3. container load balancing

Number 2 and 3 is what docker does not have. This is why docker containerization is not the a prefered platform for deploying application as a container.

Note:

Kubernetes is not a replacement for docker but kubernetes can be considered as a replacement for docker swarm.

Kubernetes is significantly more complex than docker swarm and requires more work to be deployed.

Docker swarm is also a container orchestrator like kubernetes but less complex than k8s.

How did Kubernetes came about?

Kubernetes was born or originated from GOOGLE and it is written in Go/Golang language. 

It started July 21st. 2015

KUBERNETES FEATURES

What does kubern. brings to the table?

1. Automated scheduling: Kubernetes provides advanced scheduler to launch container on cluster nodes or servers of instances based on their resource requirements without sacrificing availability.

EXPLANATION

How Do Kubernetes Run Auto-scheduling To Launch Containers on cluster Nodes?

K8 comes with a group of servers or group of nodes called NodeGroup

In K8 language, a server or instance is called a NODE.

When nodes are grouped together, it will form what is called a CLUSTER.

This cluster is a cluster of nodes

This brings us to what is called clustering technology.

CLUSTERING TECHNOLOGY

Note: Docker does not support clustering technology because it lacks the capacity to support multiple host or server or nodes or instances.

group of nodes or cluster containing nodes or clustering of servers falls under clustering technology

Examples of Clustering Technologies are

a. Docker Swarm
b. kubernetes.                                      I abbreviated kubernetes as k8

2. Self Healing Capacity: k8 has the capacity to lauch another container to replace or reschedule containers when nodes die.  It also kills containers that are not healthy. That is, containers that are not running or responding to user's traffic.
   EXPLANATION
Every kubernetes cluster has what is called a MASTER NODE also know as CONTROL PLAN

Inside the master node, we have what is called CONTROL PLANE.

This Control-plane or the master node is the one that manages or controls our nodes.

Master Node is the nodes manager in k8 cluster

Also, inside the cluster, every other nodes apart from the master node are called WORKER NODES.

NOTE:

K8S CLUSTER is made up of

Master Node -----> This is where you have the controller manage that manages the worker node
Worker node -----> This is where containers running in pods are deployed. It is where containerized applications in pods would be running

The worker Nodes are where the applications are going to be running

You can have worker-node-1, worker-node-2, worker-node-3, worker-node-4, worker-node-5 etc

In k8, containers are running inside what is called PODS.

In k8, container are deployed inside a pod .

In woker-node-1, you can have container running inside pod1
In woker-node-2, you can have container running inside pod2
In woker-node-3, you can have container running inside pod3
In woker-node-4, you can have container running inside pod4
In woker-node-5, you can have container running inside pod5

Note: each node can also have more than one pod. Each node supports multi-pod

That is

In woker-node-1 can house pod1. pod2, pod3 etc. But it is more stronger, better and secured when the pods are in different nodes.

If woker-node-1 is not responding or offline, k8 will auto-schedule and deploy a replica of that same pods that have containers running in them probabbly in worker-node-2 and then it will kill the worker-node-1 that is not responding

If there is no other worker-node left in the cluster when this incident happened, k8 would auto-launch another worker-node and then launch another replica  pod there.

This is what is called SELF HEALING CAPABILITY and killing of unhealthy node

3. Automated Roll-out And Roll-back: 

Kuber have the capacity to roll out changes or configurations that may have occured in your applications while monitoring the health check of your application instance so that the instances of your applications would not be killed as a result of this changes but if anything goes wrong, kuber also has the capcity to roll-back the changes that has occured.

EXPLANATION

In RealTime, developers would commit written codes, let say version-1 of tela (tesla-V1) to sourceCode Manager like github. This codes will involve

scr
buildscript (pom.xml)
Dockerfile 
etc

DeVops engineers will clone the code to their working environment that has docker, jenkins and other neccessary deployment applications and plugings running.

As soon as the cloning is successful, maven will run unit testing and do a build to create artifacts that would be stored inside the target directory of the project repo or inside the target directory in the workspace directory of the project item name in jenkinsby by mvn package, sonarqube will do a code quality analysis by mvn sonar:sonar, built artifacts or application will be deployed to Nexus for backup bu mvn deploy.

As soon as the application has been backed up in nexus, docker would build the dockerfile by containerizing the application and its dependencing to form what is called docker image by dockerBuild

As soon as the image is build, the image will be pushed to docker image registry like dockerHub via dockerPush

As soon as the image has been pushed to dockerHub, k8 will pull the image from dockerHub to create create a container and deploy the application as container inside a pod using one of the worker-node like worker-node-1 inside the k8 cluster as the node or server in which the pod is running upon.

Now, if another version of the application is to be deployed. Maybe tesla-V2. Developers will do the neccessay code commit of the new version and then commit the code to github, engineers will clone the code and deploy the application using k8 and maintain zero downtime. This is called ROOL-OUT 

ROOL-OUT  is the process of upgrading existing project to a newer version or launching a new project to replace existing project.

if something goes wrong with this new version of the application that has just been deployed, k8 can do what is called a ROLL-BACK of the old application tesla-V1 and deployed application tesla-V1 and maintain zero downtime, to prevent failure or service outage. This is called ROLL-BACK

ROLL-BACK: This is the process of reverting the system back to the older version from the new version or going back to the old version of the application if a problem is encountered in the new version.

4. Horrizontal scaling and load balancing: Kubernetes has the capacity to scale up by creating more pods when there are increase in demand or that the number of users assigned to a pod has exceeded what was assigned and so a new pod will be created. It also have the capcity to scale dowm by killing containers if there are decrease in user traffic and the number of users per containers is lesser that what was proposed. 
Scaling down is neccessary if there is a reasonable decrease in user traffic in order to save cost.

EXPLANATION

Horrizontal scaling is the process of creating more pods (scaling up) when there is an increase in user traffic and killing pods when there is a reasonable decrease in usr traffic with the help of the load balancer to help to route traffic to each of the pods.

Laod balancer ensure that each pods gets the required user that is assigned to them. If there is an increase, the auto-scaler would create more pods and the load balncer will route the needed traffic to such new containers.


Note: In Docker, Users access our application in docker via port forwarding. That is, the user traffic comeas to the hostport that you may have created or asssumed, and then the host port will now forward the traffic to the container port because hostport is mapped with the container port where our application is.

Auto-scaling can not happen in docker because when user traffic increased more than what was expected or subscribed for, there becomes a probleM because a replica of the container can not be created. Scaling here would have to create a replica container that is bearing the same name and storage size with the first container. This can not happen with docker.

The only way that another replica can be created would have been if the replica container is created in another docker host or server but docker do not support multi-hosting or cluster

Note: each node can also have more than one pod. Each node supports multi-pod

That is

In worker-node-1 can house pod1. pod2, pod3 etc. But it is more stronger, better and secured when the pods are in different nodes.

In k8 cluster, if you have the below nodes inside the cluster

In woker-node-1
In woker-node-2
In woker-node-3

and a container maybe myapp is running a pod is in  woker-node-1, if there is an increase in user traffic, k8 has the capacity to auto-scale by creating another replica myapp container runnning in another pod2 in woker-node-2 and if there is more increase in user traffic, another replica container myapp running in pod maybe created in woker-node-2 or it can be created in woker-node-3

In aws K8, there is what is called SERVICE DISCOVERY. This is the service in aws k8 supports load balancing. It is does what load balancing do.

This service-discovery is a service that you can create in your cluster

This service will be doing load balancing and would not allow any of the pod to be overloaded.

5. service discovery and load balancing: With kubernetes, you do not worry yourself about networking and communication because kubernetes will automatically assign IP addresses to each of the pods and you put these set of pods under a single DNS name that the load balancer can route traffic between these pods inside the cluster.

This means that each of the pod will have their own IP address and so you can put these pods with different IP address under a one DNS name for load balancing  

EXPLANATION

How do we ensure that traffic is routed to the different pods in k8 cluster?

Each of the pod in the cluster has its one Ip address

Since each of the pods has different ip address, it will now be very difficult for them to communicate with each other using ip address.

You can use the SERVICE DISCOVERY to create a DNS name where these pods can be housed since they are located in the same cluster even though these pods may be different nodes.

End-points represents the place where applications are running. The place where the applications are running are the pods. so the endpoints are the pods.

The Service Discovery can also server as your DNS name and also your Load balancer. Anytime you want to access the applications running in any of the pods, you will need to talk to the DNS name created from Service Discovery and then the same Service Discovery would be the one to route your traffic or request to any of the pods. These pods regulated by Service Discovery are dynamic. You can talk to pod1 few minutes ago and when you re-login, you may be talking to pod2. It is the Service Discovery that will route the traffic

6. With k8, you can mount the storage system of your choice. You can either use your local storage in your hostEngine/hostServer as your storage mountPoint or choose a public cloud storage system provider like GCP or AWS or Microsoft Azure

KUBERNETES ARCHETECTURE

kubernetes cluster comes with what is called Master Node also called ControlPlain and also come with worker Nodes.

Master Node

Master node is reponsible in managing the nodes inside the k8 cluster.

What is the Mater Node made up of?

COMPONENTS OF MASTER NODE

the master node are made up of the below

1. API Server :  This server act as the administrator. This perform all the administrative task. 

This is responsible running a command or running a task in k8s. 
Whatever we will be doing in k8, we will be making what is known as API call

    How To Make API call

 Let us say you want to deploy container in our k8 cluster

If you want to deploy container our cluster or in another word,  make an API call,

We either use the below  

a. CLI (which is kubectl)
b. kubernetes dashboard (UI)

Whenever you want to do anything in your k8 cluster, you can use one of the two above to make an API call

Note: If you execute any command in k8 environment or perform a task using UI in k8 dashboard, it means that you are making an API call.

Note: if you run the command below

kubectl create              is create a resource in k8
kubectl run                 is create, start and deploy container in k8

above command is an example of an API call

If you make an API call, it will go to the k8 cluster and an API server which is the administrator would receive the call and act on it.

For API server to receive the API call, API server will make sure that whoever is making the call must be authorized.

API server make use of .kube/config file for authorization. 

If you do not have .kube/config file, you would not be able to make an API call or execute kubectl using CLI.

Once the API server receive the API call, it will store the info or data or request in ectd

etcd is the database that store jobs, task and others that would be done in k8

As soon as the info or task are stored in etcd, API server will also talk to the scheduler to schedule for possible new pod provisioning or pod replication. 

The scheduler will move through the worker nodes and look for the worker nodes that have sufficient resources where the pods can best be provisioned

The main agent that is working in our worker nodes is the Kubelet agent

 WORKER NODE COMPONENTS

a. This kubelet agent is the one that establish communication between the master node and the worker nodes.

Note: before a pod is created, images would be pulled from image registry BY K8S and then create a kubernetes container and deploy the container via a pod.

b. kubeProxy: This will help user to access the application running in the pod by creating a DNS name through SERVICE DISCOVERY. This DNS name now stand as a proxy because that is what end users can see.

This same service would be used to route traffic to the endPoints or pods

Let us name the service as appSrv

appSrv is now the DNS name for the service discovery that would stand as a proxy to the pods and also do load balancing that would route traffic as required to the pods or endpoints


CLASS QUESTION

How do kubernetes deploy containers or containerized applications?

answers:

1. Containers are deployed in pods

The scheduler will schedule 

2. Scheduler: This is responsible in assigning jobs or task to worker node.
              That is replicating node or creating new nodes

3. etcd : this is used as database or key-value-store to store datas or info

4. Controler managers: This manages the worker nodes. 

In k8, the control manager can increase or expand our cluster and can also shrink or decrease our cluster. 

it can create a new node if more node are needed that we pilot or host our container that is running inside the pods 

controler manager can also create more nodes if need be. It can also replace any node that is down by creating new one as replacement. We can actually automate the creation of this node by control manager

COMPONENTS OF THE MASTER NODE

1. apiServer (called kube-apiserver) ---> is the administrator for api calls. Once api calls or any command is runned in k8s cluster, it goes to apiServer who processes the call to make sure that you are authorized to make such call and the store the datas in ectd.

3. ectd  (called kube-ectd) --> This stores the data collected from the apiServer


2. scheduler (kube-scheduler) ----> The scheduler gets informed by the apiServer and then the scheduler assign the tasks  to a workerNode

4. conrollerManager ( kube-controllerManager) : This is the one that manages what happened inside the workerNode. 

Worker Nodes

Worker node is the node or server where pods are launched.

What is the Worker Nodes made up of?

It is made of  kubelet, containerRunTime, and kubeProxy

COMPONENTS OF WORKER NODES

The principal components of worker nodes are

1. Container RunTime: A software that can be used to create and start a container is an example of container RunTime.

Docker engine and container-D are example of software that uses container RunTime.

Kubernetes used to deploy container using docker engine or docker as their container Runtime before but recently, Kubernetes now use what is called container-D as our container RunTime


2. Kubelet : This established a communication relatioship between the worker node and the apiServer of the master node.

This agent is also responsible for setting up pod requirements, such as mounting volumes, starting containers and reporting status.

3. KubeProxy: Kube-Proxy is a Kubernetes agent installed on every node in the cluster. It monitors the changes that happen to Service objects and their endpoints. If changes occur, it translates them into actual network rules inside the node. Kube-Proxy usually runs in your cluster in the form of a DaemonSet

Note: Worker Nodes. This are the servers also called nodes in k8 which is always inside k8 cluster. 

Worker nodes are the nodes or servers where deployed containerized applications as pods are hosted inside k8 cluster.

KUBERNETES INSTALLATION

For Containerization --> We can use Docker, Rocket(Rkt), etc

For Container Orchestration Tools --> We can use Docker Swarm, Kubernetes, OpenShift. These are softwares that can be used to manage your containers.

For Installation. We have 
============

Self Managed K8s Cluster: 

This is a cluster that is bening managed or owned by an individual. A person having k8 installed in his local computer maybe for practise.

Example of These are below

 1. minikube --> This is made up of single Node K8's Cluster. A cluster that comes with just one node.

 2. Docker desktop --> This is made up of single Node K8's Cluster. A cluster that comes with just one node. This can be installed in your dekstop where docker is installed. Make sure docker is installed in your desktop before you can install this.

 The disadvantage of this that you do not have the full benefit of what kubernetes clustering technology bring to the table because this has only one node which is one server and so you can only have applications running in just one node. There is no form of multiple hosting benefit here.

 The only advanyage is that you can actually use it to practise as a student.

 3. kubeadm --> This is also a self managed k8s that you can actually create multi-nodes. We can setup multi node k8s cluster using kubeadm.

Cloud Managed k8 Cluster:

This offer what is called PaaS (platform as a service). Example of cloud managed k8 are below

   EKS --> Elastic Kubernetes Service.  This is provided by (AWS)
   AKS --> Azure Kubernetes Service.  This is provided by (Azure)
   GKE --> Google Kubernetes Engine.  This is provided by (GCP)

KOPS --> Is an open source Kubernetes Operations software use to create production ready
highly available kubenetes services in Cloud like AWS.

CREATION OF K8S CLUSTER

A Kubernetes (K8s) cluster is a grouping of nodes that run containerized apps in an efficient, automated, distributed, and scalable manner.

kubernetes is represented as k8s

kubernetes = k8s  

EXAMPLE

An email may be sent to your jira email with ticker number Ticket0045
Ticket0045 that you should do the below

  Create a self manged multi nodes [3 nodes] k8s cluster using kubeadm.

  kubeadm = kube-adm

 Kubernetes Setup Using Kubeadm In AWS EC2 Ubuntu Servers.


Prerequisite

    AWS Acccount.
    Create 3 - Ubuntu Servers -- 18.04.
    1 Master (4GB RAM , 2 Core) t2.medium
    2 Workers (1 GB, 1 Core) t2.micro
    Create Security Group and open required ports for kubernetes.

    Open required ports for this illustration. Open all traffic because master nodes components has different ports, worker nodes components all have different ports. This is why we should open all ports for now
Attach Security Group to EC2 Instance/nodes.

INSTALLATIUON PROCEDURES

Go to aws to launch k8s cluster using ubuntu server, mame it k8s-master (meaning master node), instance type will be t2-medium, enter your key, create a security group and open all ports, all traffice and  open all protocol should be opened, and the click launch instance. Copy the ip address

Let us consider this instance you just launched as you master node

master node = paster the ip address here. We will use this server to install k8s cluster and then it will now be called k8s cluster or master node.

Let us change the hostname to k8s by running

sudo hostnamectl set-hostname k8s

switch user to root user by running

sudo su -   or   sudo -i

It will take you to root user.

Create a shell script and call it k8s.sh by running

sudo vi k8s.sh

and past the below long script

#!/bin/bash
sudo hostnamectl set-hostname k8s-master
sudo -i
# copy this script and run in all master and worker nodes
#i1) Switch to root user [ sudo -i]
sudo hostnamectl set-hostname  node1
#2) Disable swap & add kernel settings
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
#3) Add  kernel settings & Enable IP tables(CNI Prerequisites)
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sysctl --system
#4) Install containerd run time
#To install containerd, first install its dependencies.
apt-get update -y
apt-get install ca-certificates curl gnupg lsb-release -y
#Note: We are not installing Docker Here.Since containerd.io package is part of docker apt repositories hence we added docker repository & it's key to download and install containerd.
# Add Docker’s official GPG key:
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
#Use follwing command to set up the repository:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
# Install containerd
apt-get update -y
apt-get install containerd.io -y
# Generate default configuration file for containerd
#Note: Containerd uses a configuration file located in /etc/containerd/config.toml for specifying daemon level options.
#The default configuration can be generated via below command.
containerd config default > /etc/containerd/config.toml
# Run following command to update configure cgroup as systemd for contianerd.
sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
# Restart and enable containerd service
systemctl restart containerd
systemctl enable containerd
#5) Installing kubeadm, kubelet and kubectl
# Update the apt package index and install packages needed to use the Kubernetes apt repository:
apt-get update
apt-get install -y apt-transport-https ca-certificates curl
# Download the Google Cloud public signing key:
curl -fsSL https://dl.k8s.io/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg
# Add the Kubernetes apt repository:
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
# Update apt package index, install kubelet, kubeadm and kubectl, and pin their version:
apt-get update
apt-get install -y kubelet kubeadm kubectl
# apt-mark hold will prevent the package from being automatically upgraded or removed.
apt-mark hold kubelet kubeadm kubectl
# Enable and start kubelet service
systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet.service
#Initialised the control plane in the master node as the root user.
# Initialize Kubernetes control plane by running the below commond as root user.
sudo kubeadm init
#exit as root user  by running
sudo su - ubuntu
#execute the below commands as a normal ubuntu user by running
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#To verify, if kubectl is working or not, run the following command.
kubectl get pods -A
#deploy the network plugin - weave network and verify
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
kubectl get pods -A
kubectl get node

PROf encountered some anormalies that some of the pods were not running. It was malfunctioning. He decided to switch to using local moba-xterm that has docker desktop and kubernetes installed in it, to illustrate what he wants to illustrate. The use of local computer for kubernetes only comes with one node in the cluster by default.

Note: what happened above is what usually happens in company using self-managed k8s cluster. You will be the one to make sure that all the pods and the nodes are up and running.

After he has moved to mobal-xterm that has docker and k8s installed, he ran 

kubectl get all

Above is used to list cluster

Note. if you install docker desktop and k8s in your local computer, you can access it via gitbash or moba xterm.

if you run

kubectl get all

You will see that it only display cluster info because we are dealing with docker desktop of a self managed k8s cluster which is made up of single Node K8's Cluster.

if you run

kubectl get pods -A

You will see that it shows master node components Like kube-proxy pod, kube controller manager docker desktop pod, kube scheduler docker desktop pod etc. These are pods from our master node or control plane. 

THese are pods or containers that has been deployed by default upon creation of the cluster

Kubernetes NameSpaces: 

This is an isolated environment which you can create in your k8s cluster

It can also be defined as a cluster within a cluster

Namespace is an isolated k8s environment where applications are deployed.

USES OF NAMESPACES

1. We can use namespaces for isolation 
2. We can use namespaces for security and RBAC (role based access control)
3. We can use namespaces to assign resources - 1GB RAM. That is assigning resources to application. That is how much storage space each application can take.

Example Of Isolation in Namespaces

Let us assume that you are managing a customer support application

For you to deploy the application, you can deploy it in [ Dev uat  prod ]

[ Dev uat  prod ] are namespaces because they are in isolation or separated from each other. 

Namespace is synanymous to git branches.

Dev namespace is isolated or different from Uat namespace, so also prod. It is a different environment

In deploying, we created this isloated namespaces to help deploy our applications 

When you launch your k8s cluster, your cluster comes with some default namespaces. these are below

default  namespace

kube-note-lease namespace

kube-system  namespace: This is the most important namespace in your cluster. It contains most of the pods that are running in our controlplane also called master node.

kube-public  namespace

Note:

It is important to create a custom namespaces while deploying your containerized application in pods.

If you want to find out the namespaces in your cluster, run

kubectl get ns kubectl get namespace

You will see that it will list the namespaces above

How To list pods that are running in namespace

kubectl get pod -n default   ----> This will list pods running in default namespace
kubectl get pod -n kube-note-lease  --->list pods running in kube-note-lease namespace
kubectl get pod -n kube-system  ---> This will list pods running in master Node by default
kubectl get pod -n kube-public  ---> This will list pods running in kube-public namespace

If you run the above, only kubectl kube-system  will list all the pods that are running in the master node or control plane which usually comes with k8s cluster upon k8s installation. Others will show nothing because no pod has been created there.

Namespace need to be protected and not everyone should have access to namespace.

We use namespace to isolate the environment

POD:
==== 
In k8s, Containers run in pods 

Remember that to check if containers are running in docker, we use docker ps or docker container ls

In k8s, to check if containers or pods are running, we run the below

kubectl get pods     

It will list all the pods that are running and all the pods that are not running. This is because containers are deployed in pods in k8s

QUESTIONS

How can Containers or containerized applications be deployed in kubernetes? or How do you effect deployments in your environment using kubernetes?  

Answer:

Workloads/jobs are effected or deployed or runned in kubernetes using kubernetes objects.

Kubernetes Objects:

These are as follows

POD

Let us deploy an application using k8s pod

   Deployment of a  Sample Application
==========================

Sample Application shows an easy-to-use interface for viewing, updating, and searching order and customer information for electronic and computer products. Users can navigate among the pages using the Home, Customers, Products, Orders, and Charts tabs.

Let us deploy a sample application called hello and the image name in prof dockerhub account is called mylandmark/hello   and the application port is 80

This hello application was in prof dockerhub account

Method that can be used to deployed a Sample Application are as follows:

1.

IMPERATIVE METHOD = 

This means using only direct commands to deploy the application as below:

Let us use imperative method to deploy this application.

   kubectl run NewpodName --image=NameOfImage --port=80 

   That is 

   kubectl run hello --image=mylandmarktech/hello --port=portName 


To expose the pod created, you run


   kubectl expose podName serviceName --podPortNumber=80 --ServiceType=LoadBalancer

If you paste the above in your k8s host, a pod will be created called pod/hello would be created

If your run

kubectl get pods

You will see that a pod is running.

To get more info if any pod is running. we run

kubectl get pod -0 wide

This will show more info about the pod and the type of node. it will even show the pod IP address.

If you curl the ip address of the pod, thats is 

curl -v ipAddress

You will see that no response. This is because the application has not been exposed.

EXPOSING THE APPLICATION

After you have deployed the application, now you want to expose the application so that the application can be discovered externally. To be in public. You run the below command
   
  kubectl expose pod hello --port=80 --type=NodePort

 That is 

    kubectl expose pod podName --port=portName --type=serviceType

To see if the container has been exposed, you run

kubectl get svc -0 wide

It will list all the pods, ports (host port mapped with node port), cluster ip, pod ip.

svc = service

What is a service?

In Kubernetes, a Service is a method for exposing a network application that is running as one or more Pods in your cluster

We created a pod called hello in the above command. This pod is also called a service. We run pod as a service in k8s.

Since we are using You can curl the localhost:nodePort   to see if the pod are running by

let say the NodePort was 124452

curl -v localhost:124452

Above will let us know if the application or container are running inside the pod.

You can access it on your browser by pasting

localhost:124452

You will see that application will open on the browser.

We are using localhost because we are using dockerdesktop in our local computer and so we should use local host to access it externally.

Above is how you can created a pod or deploy container using pod in k8s. Jesus is lord

Hello was the pod service we created.

Let us get the endpoint of this application by running

kubectl get ep

You we see that we have just one end point. The endpoint has the ip address of the pod and the port number of the pod. The endpoint is coming from the pod service.

Therefore, service pod route traffic to the endpoint

   Get Node Port details . To get node ports details, we run the following below:
   =====================

   kubectl get services or  kubectl get service


We have used imperative method to deploy containerized application via pod in the sample application above.

2. Declarative method = This is using commands & yaml files to deploy containerized application in k8s = This what is called IaaC: 

examaple below

Let us assume you are going to deploy a pod using command.

When you are writing a k8s command in yaml file, we have 4 elements to take note of or to consider. These elements are

KAMS= These are below

1. Kind of k8s object you want to adopt to deploy your containers in pods. Here, the kind of object is POD. Let us use Pod to deploy for now though not recommended and not best practices.
2. API-version of the object: API-version for pod is V1
3. Meta-data
4. spec

Most companies uses declarative method of deployment because it is re-usable.

yml files come with the below

1. key: value pairs  

Example of key value pairs are below
    name: simon              the name is the key, simon is the value
    job: evangelist  

2.  list:

Example of list is below
     List
 List of students:  
  - simon 
  - john 
  - erica  
3. dictionary:   this can be a combination of key value pair. it give details about somthing

Example of dictionary are below

  student information:
    name: dominion 
    age: 45 
    wt: 200
    ht: 1.8 

The declarative method that we want to use is written in yml language

yml is simplier than shell script.

Let us see how we can use KAMS to create pods

kams: 
kind: Pod        = kind here is an exmaple of a Key value pair 
apiVersion: v1   = apiVersion here is an exmaple of Key value pair 

To know the API-version of the pod or other API resources, we can run either of the below commands

kubectl api-resources   

kubectl api-resources | grep pod    if you want to know the API version of the pod

kubectl api-resources | grep objectName --> To know API version of a particular ks object

metadata:   = meta-data her is an example of a dictionary
  name: webapp  # podName
  lables:
    app: webapp
spec:   = This is an exmaple of a list  
- containers:
    - name: wp  # containerName you want to deploy
      image: mylandmarktech/maven-web-app   # containerImage
      ports:
      - containerPort: 8080   #first containerName port number
    - name: db     #another containerName
      image: mysql     

- volumes 
- nodeSelector 
- imagePullSecrets: This is when you are trying to pull image from private registry like ECR

ECR = elastic container register. It is been powered and owned amazon
---

In the above, we have more than one container. Which is wp and db. The containers here is a list.

A pod can have multiple containers.

BELOW IS THE EXAMPLE OF A DECLARATIVE METHOD OF DEPLOYMENT USING YML FILE FORMAT

Example: yaml or yml format

apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    app:  myapp
spec:
  containers:
  - name: myappcontainer
    image: mylandmarktech/java-web-app
    ports:   this is also a list
    - containerPort: 8080


NOTE: Kubernetes is basically what we used to manage our containerized application.

We use docker to build on dockerfile to create image and then push to image registry. Then we use kubernetes to pull those image from registry and then deploy image as a container that would been running in pods in the working nodes of a kubernetes cluster.

What is kubernetes?

In our environment, we use kubernetes as our container orchestration engine that is used to manage and deploy containers in pods on worker nodes inside a k8s cluster.

VIDEO 125 KUBERNETES 2

Kubernetes Objects

K8s objects are persistent entities that is used to represent the status of your cluster.

Once you create a k8s object, k8s system will constantly work to ensure that the object exists.

To work with k8s object, whether to create an object or to modify the object or to delete the obkect, you will need to use k8s API.

Whent you use the CLI in k8s system like

kubectl

The above command will make an API call for you. 

the basic k8s objects are below:

Pod
Replication Controler
ReplicaSet
DaemonSet
Deployment
StatefulSet
Service
Namespace
ConfigMaps
Ingress
Persistent volume
secrets

DEPLOYING KUBERNETES USING KOPS

How do we deployed a k8s production grade cluster

We will be deploying a cluster using KOPS

What is KOPS

1. KOPS --> Is an open source Kubernetes Operations software used to create a production ready kubenetes cluster.

2. Kops is a software that can be used to create resources in aws like VPC, ASG, ELB, launch configuration, launch template, and including a k8s cluster, ec2-instances of worker nodes and ec2-instance of masterNode. 

3. kops supports multiple cloud providers

4. kops compete with other managed kubernetes services like EKS (elastic k8s service, by aws), AKS (Azure k8s service, by microsoft azure), GKE (google k8s engine).

5. kops is cheaper than others

6. kops create production ready k8s cluster

7. kops is IaaC = Infrastructure as a code.

Just like when we installed rexray as a docker plugin that can be used to create a volume in aws, so also kops can be used to create resources in k8s cluster.

rexray, kops are third party softwares.

Remember that when we were trying to create an ebs volume in aws via docker rexray, for docker rexray to be able to manage ebs volume in awas, we created an IAM which we used for authenticating and authorization by creating an access token and a secret access token.

IAM authorized rexray docker plugin to manage ebs volume in aws

When you run the command

kops create cluster

1. kops software will fully provision or create a controlPlain or masterNode
2. kops software will fully provision or create workerNodes or nodeGroups
3. kops software will fully create and manage other aws resources whick includes
   ASG (auto scaling group)
   LC (Launch configuration)
   LT (launch template)
   ELB (elastic load balancer)

For all of these to happen, we need to authorize kops to manage other aws services via IAM

INSTALLATION OF KOPS in ubuntu where kubeadm is running.

Let us install kops in kubeadm k8s cluster

Make sure you get the kubeadm k8s installation and the kops installation from prof github account.

KOPS CLUSTER INSTALLATION

https://github.com/LandmakTechnology/kops-k8s

I have forked this installation on my github account

like the above link or paste it in your browser and it will take you to prof github that we show you how to install KOPS cluster.

After you have install kops fully via CLI

You can go to your kops aws instance and create on modify IAM role for kops to be able to manage aws.

You can create IAM role by ticking the kops ec2, click on action, click on security, click modify, select the role that you want to modify but if you do not have any exsting IAM role, click on create IAM role, select aws services, select ec2, type in ec2-administratorAccess on the search, so that it will grant full access to all your aws resources. This involves, ec2-fullAccess, s3-fullAccess, VPC-fullAccess, IAM-FullAccess and more.

You also select the type of access you want by unticking the admin and then type in the resources and they type of access of the resources that you want.

Let us call the role kops-admin, click on create role

Go to your kops instance by selecting it, click on action, click on security, click on attach, select kops-admin and then click save.

If you to to your kops server via CLI and run the command

Make sure you change to your other user. Apart from ubuntu, let say your user is kops.

aws s3 ls

You will see that it will respond if you already have any existing bucket created before on your s3 aws resources.

Let say we do not have any bucket before, we can create new bucket by running

aws s3 mb bucketName      or        aws s3 mb s3://bucketName

let us call bucket name as myclass1984

create a bucket by running

aws s3 mb s3://myclass1984

This bucket that we just created will act as our key value store

let us create environmental variable 

the environmental variable would be 

export NAME=class.k8s.local
export KOPS_STATE_STORE=S3://bucketName

export NAME=class.k8s.local
export KOPS_STATE_STORE=S3://clas290kops-1980

vi .bashrc

paste the above environment variable below where it says FOR EXAMPLE   which is in line 3
save and quit

You need to refresh the file by running

source .bashrc

If you run 

echo $NAME

It will show you the name of the k8s cluster that we want to create. which is

class.k8s.local

Above is the name of the cluster that we want to create, as indicated on the environmental variable above.

If you also run

echo $KOPS_STATE_STORE   it will list what is in KOPS_STATE_STORE which is 

S3://myclass1984


Note: NAME and KOPS_STATE_STORE where in our env. variable and when you add dollar sign to then and run, it will list what they were equal to.

Let us check if there is contnet in this bucket by running

aws s3 s3://myclass1984

You will see that there no content or object in the bucket

TO CREATE K8S CLUSTER

We want to create our k8s cluster using the name that we have in our env. variable. That is

class.k8s.local


It will create master node and the worker nodes of k8s cluster

In order to connect to that node, we must first of all create an ssh key in the kop server

To CREATE SSH KEY

To create shh key, you run

ssh-keygen


It will ask you where to save the key, ignore and type enter enter enter until it i save and it will tell you that it has be saved in /home/ubuntu/.ssh/id_rsa.pub

this will create ssh key public and try to save the key

If you want to see or copy the ssh-keygen, run the command below

/home/ubuntu/.ssh/id_rsa.pub



We can now run the command

To create the cluster with the name class.k8s.local, we run

kops create cluster --zones us-east-2a --networking weave --master-size t2.medium --master-count=1 --node-size t2.medium --node-count=2 ${NAME} -dns=private


Note; NAME was equal to class.k8s.local on our env. variable. If you add $ sign with curl bracket and run it in your server, it will list it as class.k8s.local

this means that it is creating k8s cluster with name ${NAME} will result to class.k8s.local which the the name of the cluster.

his will create k8s cluster with name class.k8s.local in us-east-2a zone with masterNode size of t2-medium(4G), workerNode size of t2-medium(4G), masterNode to create is 1, workerNode to create is 2, under the network called weave. It will create a whole lots of things including vpc, subnets, security group etc.

LET US COPY OUR SHH KEY WE CREATED INTO OUR KOPS by running

kops create secrete --name ${NAME} sshpublickey admin -i ~/.ssh/id_rsa.pub

Above will permit us to be able to access our kop server via ssh

If you run

kops get cluster

It will bring the name of our cluster which is class.k8s.local, the zone which is us-east-z2 and the cloud provider which is aws

Finally, for this resource to be fully created in aws, we run the below command

kops update cluster ${NAME} --yes

kops will create an ec2-instance with name class.k8s.local in aws

It will take some minutes like 5-15 minutes for it to be fully up and running.

always go to ec2 instance in us-east-2c zone to check, also go to VPC network IN us-east-2c zone to see if THE VPC has been created. 

After a while, you will notice that it has created 3 ec2-instances in us-east-2c zone.

2 of the ec2-instance are woker-nodes and one of the isnatnces is masterNode as we have stated in our command

It also created vpc called class.k8s.local in us-east-2c zone

HOW TO CONNECT TO OUR MASTER NODE

The master has private ip and public ip.

Note that the kops server/instance or host that we have before was on default VPC and this is new cluster with 3 instances we now have now is in another VPC different from the kops.

For connection to be possible via private ip, it has to be on the same VPC. 

We can only will be able to connect to master in the kops server or host that we have before via a public ip

To connect to masterNode on the kops server and run

ssh  admin@<IPOrDNS>

it above command  is not working then execute

ssh -i ~/.ssh/id_rsa admin@<IPOrDNS>

ssh -i ~/.ssh/id_rsa admin@kop-serverPublicIPaddress

Note:

~/.ssh/id_rsa  here is the privtae key and not the publich key. If you cat it, you will it

If the kops server already have so pods running in any of the nodes, makes sure you delete the pods and the service and any other resource you may have created by yourself before executing the command for delete.

It will take your kops masterNode (it is going to show ubuntu user because that is the ami)

That is the kops master now.

You can exit and it will take you to kops server earlier.

 Note, if you exit, you will no longer be inside the kops masterNode. You can also reconnect by running the above command.

 With the help of that private key, we were able to ssh into our kops server.

 NOTE :

 To created a bucket you the required region, you can run

 aws mb s3://bucketName --region=regionName    

  aws mb s3://bucketName --region=us-east-2


 I have created a shell file for kops installation in my ec2 module in vs code.


 while connected to the kops master, if you run

 kubectl get node

 It will list all the nodes that you have, including the master


 kubectl get node -o wide

 It will give you detailed info about all the nodes. It will let you know which of the container runtime that they are using. which is containerd

  kubectl get nodes   or  kubectl get node -o wide

 It will let you know if there are any pod created and running. iF NO POD IS RUNNING, IT WILL SAY NO RESOURCES FOUND

 We have succeeded in deploying a kops cluster which has the masterNode  and 2 workderNodes which has their respective ip address

 Let is found out if our pods are healthy by running

 kubectl get node -o wide -A   or kubectl get pod -A

 You will see that all the pods are running

 What we have just done is deploying a PRODUCTION READY K8S CLUSTER using kops

 The reason why it si called a production ready k8s cluster is simply because k3s cluster with all the required agents needed for deploy will be available upon the installation of kops. Some of this agents are ASG, LoadBalancer, vpc, LT, etc 

SINCE WE HAVE A CLUSTER ALREADY RUNNING, HOW DO WE DEPLOY AN APPLICATION IN THAT CLUSTER?


You can deploy containerized application as pods in two was

1. imperative method ---> use of command e.g deploying nginx image from dockerhub registry

kubectl apply deployment ngynixpod --image=nginx --replicas=1 --port=80

2. declarative method---> method used of command and yaml file

Let deploy an application using declarative method in the cluster that we have created by creating a single pod container in our masterNode.

NOTE: you can READ THIS BELOW LINK FOR MORE EXPLANATIONS ABOUT KOPS

https://blog.kubecost.com/blog/kubernetes-kops/

In declarative method, we make use of commands and yaml or yml file. Below is the yaml file


kind: Pod
apiVersion: v1
metadata:
  name: myapp
  namespace: dev
  labels:
   app:  myapp
spec:
  containers:
  - name: myappcontainer
    image: mylandmarktech/java-web-app
    ports:
     - containerPort: 8080

while connected to masterNode, let us first of all create namspace called dev that is indicated above because we have not created dev namspace yet

You can verify this by running

kubectl get ns -A

You can create namespace by running

kubectl create ns aut

It will say namespace created   

You have made an API call using the above command        

of kubectl create ns dev --v=7

This means that you have created ns in verbose mode. The last command will create the namaspace with more details of what was created.

When you make an API call, you have to be authentictaed. The kube/config file would authenticate you

Since the namespace dev has been created, let us make directory  and create a file inside the directory

make a dir called manifest and create a file called webapp.yml

mkdir manifest

vi manifest/webapp.yml     to create webapp.yml inside manifest 

and then paste the above yml file inside it and quite-save.

You can now deploy the application by running

kubectl apply -f directoryName/

kubectl apply -f manifest/   or kubectl apply -f webapp.yml

Note, you use the directorr one in the cmd above, it will check all the files in that directory and process the one or the yml files that is needed.

You will see that a pod called myapp  has be created

To found out if the pod has been created in dev namespace, we run

kubectl get pod -n dev

You will see that the pod was actually created in dev namespace

MULTI-CONTAINER IN A POD

To create a multi-container pod, That is, more than one container in a pod, you can follow the format below

kind: Pod
apiVersion: v1
metadata:
  name: PodName
  namespace: namespaceName
  labels:
   KeyName:  ValueName
spec:
  containers:
  - name: ContainerName
    image: ImageName
     - containerPort: containerPortNumber
  containers:
  - name: ContainerName
    image: ImageName
     - containerPort: containerPortNumber

99% of the time we will go with SingleContainerPods


Other command for pod includes

kubectl apply -f filename.yml
kubectl apply -f directory/

-f means file

HOW TO SET YOUR NAMESPACE TO A CUSTOM NAMESPACE THAT YOU CREATE


If you run 

kubectl get pod

It will only bring out pods that are in default namespace. It will not show you the pods that are in the custom namespace that you created unless you run

kubectl get pod -n namespaceName

If you want it to show pods that are in your default namespace anytime you run

kubectl get pod          you have to set the context to the custom namespace

To change your default namespace to a custom namespace, you run the below

kubectl config set-context --current --namespace=namespaceName

If you now run

kubectl get pod 

It will show the pod in your custom namespace but if does not, always spacify namespace. That is 

kubectl get pod -n namespaceName

kubectl get pod
if you created the pod in a custom namespace, you run
kubectl get pod -n namspaceName
kubectl get pod -o yaml
kubectl get pod -o wide 
kubectl describe pod podName 
if it is in an namespace, you can run
kubectl describe pod app -n namespaceName
kubectl delete pod podName  
if it is in an namespace, you can run
kubectl delete pod podName -n namespaceName
kubectl get pod -n dev
kubectl get pod   
kubectl get all 
kubectl get pods 
kubectl get pods --show-labels           just to show the label of our pod
kubectl get pods -o wide
kubectl get pods -o wide --show-labels
kubectl api-resources  
kubectl api-resources | grep -i pod 
kubectl config set-context --current --namespace=<namespace>

Note: If we don't mention -n <namespace>, it will refer to the default namespace. If you created a custom namespace, always specify -n namespaceName

in the .kube/config the context is set the default namespace. 	It is always import that anytime you run a command, you should pass a custom namespace that you created that is 

-n namespaceName 

kubectl get pod -o wide -n namespaceName

NOTE: 
 To describe or inspect or know details about a pod, you run

kubectl describe pod podName

In docker, it is called inspect while in kubernetes, it is called describe

if it is in an namespace, you can run

kubectl describe pod app -n namespaceName

Whenever you run

kubectl apply,  

it means that an API call has been made, an API server will receive the call, authentication and validation would take place in the .kube/config file also called the and then create the pod, and also store or persist the data in etcd and once that is done, the scheduler is notified and it will schedule the pod that was created by assigning the pod to a node that has enough resources, once the pod is assigned to the node, the kubelet running in the node goes into action and pull the image from image registry like dockerHub, and then the kubelet will also notify the container runtime which in this case is containberd, the containerd will now create and start up the container inside the pod.

A scheduler watches for newly created pods and finds the best node to assign or attach the pod to.

Kubernetes scheduling is simply the process of assigning pods to the matched nodes in a cluster. 

So kubectl describe tells you everything that happen.

API SERVER  is that one resposible for the creation of a new pod without assigning it to the node, updates the information in the ETCD server, and updates to the user that pod is created.

How do you access a pod?

For you to access a pod, you need what is called a service.

SERVICE

A vscode can be used to write a manifest file. Let us use vscode to write a manifest file.

When using a vscode, you have to created a folder in your window or select an axisting folda that is is your computer. like when you select desktop and then select the folda that is in your desktop. I selected the folda that i created in my desktop 

if you want to create file, you can click on the create icon sign which is the first box immediately after your folderName and the enter the name of your file like pod.yml 

and the enter

kind:pod

It will you auto complete and show guildlines on how you would input details what you want to do.

You can remove the lines that you dont need asnd edit the once that you need

If it does not auto complete, it means that you have install kubernetes inside your vscode.

To install any k8s in your vscode, click on extension icon which is at the left hand side which is the 5th icon, and then type in k8s on the search and then click on install k8s. It will install k8s in your vscode. And then, also click on install k8s dependencies that will reflect on the down right side of the screen.

From there, anything you do here in K8S, it will auto-complete.

vscode is what developers are using to write code

If you want to create a SERTVICE in k8s, 

if you erase what auto-completed inside your pod.yml file and type in service, it will also auto complete. if you erase and type in pod, it will also autocomplete. if you type in deployment, it will auto complete also.

the vscode make it easier for you to create file with required procedures on how to input the data that it will container

LET US CREATE A SERVICE

if i type in service inside an empty file that is on a folda in my vscode , it will autocomplete in the form below

apiVersion: v1
kind: Service
metadata:
  name: myapp 
spec:
  selector:
    app: myapp
  ports:
  - port: <Port>
    targetPort: <Target Port>

To illustrate the steps above that i copied from my VScode is below

apiVersion: v1
kind: Service
metadata:
  name: serviceName
spec:
  selector:
    keyNameOnTheLabelOfTheApplication: valueNameOnTheLabelOfTheApplication
  ports:
  - port: <Port>
    targetPort: ContainerPortOfTheApplication

NOTE: The essence of the service is to help you access or discover the application. And so, the selector of the service must reflect the same info that is in the label of the application above.

This means that label of the pod to be created must match or have the same name with the selector of the service

This is called service discovery

Let vi into pod2.yml

and paste the below inside the file

kind: Pod
apiVersion: v1
metadata:
  name: myapp-pod
  namespace: dev
  labels:
   app:  myapp
spec:
  containers:
  - name: myappcontainer
    image: mylandmarktech/java-web-app
    ports:
     - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080

copy the above and paste it in a yml file like pod.yml


Note: The targetPort of the service must be the same with containerPort of the pod. It will enable the service to be able to route traffic to the right pod or a pod that exist

port here is the servicePort which is 80 by default   but targetport is what containerport is

If the targetport does not tally or equal to container port, a problem will occur

A service make use of label and selector.

labels and selectors are  key:value pair

label and selectors must have the same info

LET US CREATE and DEPLOY THE POD AND SERVICE in one command

If you now run

kubectl apply -f manifest/

It will create the pod called myapp-pod and also create a service called myapp-svc

To know if the pod is created, you run

kubectl get pod   or  kubectl get pod -n dev  

To know if the service is created, you run

kubectl get svc

Let us see if our service has any endpoint by running

kubectl get ep

You will notice that there is no endpoint. 

The reason is that the application was deployed in dev namespace but the service was deployed in the default namespace because we did not specify the same custom namespace of the application for service.

For endpoit to be established, we need to have both in the same namespace by editing either by editing the namespace in the application to be same with service or edit that of service to be same as with application

kind: Pod
apiVersion: v1
metadata:
  name: myapp-pod
  namespace: dev
  labels:
   app:  myapp
spec:
  containers:
  - name: myappcontainer
    image: mylandmarktech/java-web-app
    ports:
     - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc
  namespace: dev
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080

 If you now run re-deploy the application and service by running

 kubectl apply -f manifest/ -n dev

 You will see that both will be created

 You can also deploy both on default namespace when you have the file without no namespace as shown below

kind: Pod
apiVersion: v1
metadata:
  name: myapp-pod
  labels:
   app:  myapp
spec:
  containers:
  - name: myappcontainer
    image: mylandmarktech/java-web-app
    ports:
     - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080

If you deploy by running

kubectl apply -f manifest/

That will create the pod and the service in default namespace

Namespace is what is used to isolation

The same application can be deployed in dev, uat, prod namespace in k8s without issue. The namespace isolate the application for over-riding each other since they have the same label.

You can deploy the same application multiple times in the same namespace if they have different labels.

Let us delete the pod and service running in the dev namespace and leave the one of default

You can also delete the pod by running 

kubectl delete pod podName -n namespaceName
kubectl delete svc serviceName -n namespaceName

HOW DO YOU NOW ACCESS THE APPLICATION INSIDE THE POD VIA A SERVICE?

K8S SERVICE OR SERVICE DISCOVERY

  Types of services

The following are the services we have in k8s.

1. ClusterIp  service : This is used to communicate within the cluster. This is the most secured serviceType. With clusterIP, it cannot accept external traffic INTO THE CLUSTER

2. NodePort  service: This is used to receive external traffic or communication inTO the cluster

3. loadBalancer  service: This is used to also receive external traffic or communication in the cluster. Service act as a load balancer and it route traffic to the pod.

nodePort service and loadbalancing service do similar task in a way of receiving and processing traffic.

4. ExternalName: The service is used to route traffic from the app that is inside a cluster to another app that is outside the cluster (external app). Example is  mySql database. If the app wants to store or back up their data in an external database service that is outside the cluster where the app is located.

NOTE; All serviceType in kubernetes acts as a load balanceror performs load balancing

This is because serviceType perform or does a dns resolution for pods

DNS resolution is the process of making an ip address to become a domain name

IQ

You want to deploy a web application to end users to access, should you deploy the application using a clusterIP serviceType?

Answer:

It is absolutely NO, because clusterIP service is used to deploy application that can be accessed with the cluster and it does not support an extrernal traffic comming into the cluster. endusers will not be able to access the application running in the cluster

You can only deploy the application using a nodePort serviceType or a loadbalancer serviceType because they both can access or support external traffic into the cluster and so endusers can be able to access the application running in the cluster


Labels are key value pair that we have attached to any k8s project.

In the service file we created, we do not have anything like serviceType, did have what is called namespace. This is why using vscode sometimes will make you miss some steps that out to have been on the file.

The default serviceType is the same as the clusterIP address

Since we did not specify a serviceType in the above file, it there means that the serviceType will be a default swerviceType which will now assume the ip address of the cluster. There is no custum serviceType.

To verify the serviceType, you can run a describe commane by running

kubectl describe svc serviceName

kubectl describe svc myapp-svc     or     kubectl get svc myapp-svc -o wide 

You will see that the serviceType represented by the word type is = clusterIP

also, when you run the command

kubectl describe svc myapp-svc

You will see that the type which is the serviceType is mapped with clusterIP. This means that serviceType route traffic to the clusterIP.

The custom serviceType or type always have an ip address and this ipaddress will also have the ipadress of the cluster that the pod is assigned to.

NOTE:

There is different type of clusterIP

1. Kubernetes CLUSTER IP : this one is the clusterIP of the masterNode by default
2. Custom CLUSTER IP : This is the cluster ip address of an assigned woker-node

Your clusterIP is also your serviceType

the custom servicetype ip address = assigned clusterIP or workerNodeIp address

the default servicetype ip address = default clusterIP or masterNode ip address

Therefore, you can now access your pod that is assigned to a cluster or workerNode via your serviceType by running

curl -v PodClusterIPaddress

It will redirect you or connect you to the pod

We can access the pod via the service ip address.

The service now serve as the load balancer and the proxy to the application inside the pod

curl -v serviceTypeIPadress   or curl -v ClusterIPaddressOfPod

It will connect you to the pod

The default pod is called kubernetes which has it own clusterIP address or masterNode ipadress

The custom pod is the pod that you have created and it will have its assigned clusterIP address or workernode ipaddress

If you want to connect directly to the application or container that is running inside the pod, you run


curl -v serviceTypeIPadress/nameOfContainer

curl -v serviceTypeIPadress/java-web-app   or curl -v ClusterIPaddressOfPod/java-web-app

We have just created a CLUSTER IP services.

Note: Service in kubernestes is what is used to expose application running in pods to either externally or internally depending on the servicetype that you used in exposing the app.

Note: if there is no service type in your yml file where serrvice is, it means you are exposing the application via a ClusterIP serviceType. that is the default servicetype. The application can be accessed only within the cluster.

PROF DIAGRAMATIC ILLUSTRATION

k8s cluster with just one node. This node has two pods which are pod1 and pod2

pod1 has only one container called web running in it 

pod2 has 2 containers called app and dbp running in it 

If container in pod1 wants to communicate with a container in pod2, they can talk to each other via a service. and the name of the service type is ClusterIP service since they are in the same cluster node

For this to be actuallized, servicetype ip is mapped with clusterNodeip address assigned to the pod. therefore, the servicetype IP = clusterip assigned to the pod

container in pod1 can access a container in pod2 by running

curl -v  PodClusterNodeipAddress/nameOfContainer

if an end user who is outside the cluster wants to talk to a container in pod1, it pass through a service called NODEPORT service

NOTE:  pod is expected to have just one ip address

For a node port service, for external user to access the container in pod1, he can do that by running below in his environemt

curl -v NodeipAddress:nodeportNumber

to know the node ip address, you can also run

kubectl get node -o wide

node or woker-node ip addresses usually comes when you launch k8s

NOTE; NodePortNumber has a range between 30000 to about 32500 ports. 

You can choose with the above range of ports above.

when user enters curl -v NodeipAddress:nodeportNumber in their env, the node will receive the call via it ip and then notify the service and then the service will route the traffic to the container in pod1.

note:


container in pod1 cannot talk to container in pod2 without passining through clusterIP service.

let say web in pod 1 want to talk to app in pod2, it will pass through a clusterIP service called cluster-appsvc if we created a cluster service called cluster-appsvc.

the web container will notify the cluster-appsvc and the cluster-appsvc will route traffic to the app cotainer via the app ipaddress and containerPort Number

NOTE: each of these containers has their own service.

The containers and their respective service was deployed together in a pod

multipl container pod is not very often or common.

The single container pod is always bening used.

Note; In a multi container pod, each container has their own container port number but have the same ip address which is the ip address of the pod

for different containers in the same pod to talk to each.

let us say, app wants to talk to db

in app evn, it will enter below

curl -v localhost:dbPortNumber

if db wants to takl to app

in the db env, run 

curl -v localhost:appPortNumber

They can communicate with each other via local host

When containers are in the same pod, they share the same resources like file system (volume etc)

WHAT HAPPEND WHEN YOU HAVE MORE THAN ONE CONTAINER IN A POD?

1. There is going to be a scaling problem because you are going to be scaling eventually everything.
2. The resource management is going to be poor because they would be sharing th same file system which can consume lots of resource and this can make it very expensive to maintain

The percentage of having multiple container in one pod is less that 5 percentage.  A single container pod is always being utilized (percentage of the time).


ServiceType and their file format in yml is below

When writing a yml file of service, we use the following

apiVersion
kind
metadata
spec


1. ClusterIP service

apiVersion: v1
kind: Service
metadata:
  name: cluster-svc
spec:
  type: 
  selector:
    app: myapp
  ports:
  - port: servicePort (maybe it is 80. it is 80 by default)
  - targetPort: containerPort (maybe it is 8080)

If there is nothing on the type, it means it is a clusterIP service by default. You can decide to put ClusterIp on the type if you wish


Note: selector must match with label that you have in the application-pod format

2. NodePort service

apiVersion: v1
kind: Service
metadata:
  name: cluster-svc
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
  - port: servicePort (maybe it is 80)
  - targetPort: containerPort (maybe it is 8080)
  - nodePort: (this ranges from 30000 - 32000. you can choose inbetween)

3. LoadBalancer service

apiVersion: v1
kind: Service
metadata:
  name: cluster-svc
spec:
  type: LoadBalancer
  selector:
    app: myapp
  ports:
  - port: servicePort (maybe it is 80)
  - targetPort: containerPort (maybe it is 8080)
  - nodePort: (this ranges from 30000 - 32000. you can choose inbetween)

  CREATION OF A NODEPORT SERVICE

  Let us create a NodePort service from the service that we already have.

Since we now have a service called clusterIP in our environemnt. Let us edit the service to become a nodePort service.

The command to edit a service in k8s is below

kubectl edit svc serviceName

kubectl edit svc cluster-svc

it will bring out the clusterIP service we have, you can just press i and then, do just two things.

1. type: remove clusterIP and input NodePort
2. under metadata, under name, remove the cluster-svc and input nodeport-svc
3. add   nodePort: (between 3o000 - 32000) under ports just after targetPort

You can even edit the namespace. there alot you can do there.

and then save-quite

If you run

kubectl get svc

You will see that the service has changed to NodePort service.

Let us access our application using this NodePort service

Note: initially, our applicxation can be exposed internally by clusterIP servicfe but now it has been changed to nodeport service.

With nodePort service, we can be able to access our application both internally and externlly

Internal access

curl -v clusterIpAssignedToThePod

This can be gotten by running

kubectl get pod    or  kubectl get pod -o wide

External access

curl -v NodeIpOfThePod:nodePortNumber

curl -v NodeIpOfThePod:nodePortNumber/AbsolutePartToTheApplication

NodePortIp address that the pod is using can be gotten by running

kubectl get pod -o wide

It is usually the ip address of the node that the pod is using as server.

If you run these commands, you will see that both can be accessed 

but if you run health check and see that it can not access it externally, it means that the firewall is not responding. Go to your securityGroup in your cloud provider ec2 instance in aws and then add a rule by allowing all protocol, all traffic, and from anywhere.

If you another health check using a coomand line above, you will see that it is now running.

As a devops, it is more professional to be performing health check using command line than using internet web borwoser.

if you want to use internet browser, copy and paste

NodeIpOfThePod:nodePortNumber  or  NodeIpOfThePod:nodePortNumber/AbsolutePartToTheApplication

NOTE THE BELOW FACTS

Let us assume that there is a spike in our application. Maybe there is an increase in user traffic and there a need to create more replica of the pod. If you deploy your application using pod directly, you would not be able to scale. This means that you cannot create more replica of the same pod.

This is one of the limitation of deploying your application via POD. This lack the benefit of scaling which k8s brings to the table.

The only way scalinmg can occur or the creation of more replica is the use of what is called CONTROLER MANAGER in k8s

If you want to scale, you have to make use of CONTROLER MANAGERS for deploying your application and not via POD

KUBERNETES OBJECTS OF APPLICATION DEPLOYMENT

The k8s objects are as follows:

1. POD: You will not be able to scale when you deploy application via pod as we have been doing above. If you deploy application via pod directly, if the pod goes down, it cannot be able to re-create itself. 

For example. Delete the pod if created before by running

kubectl delete pod podName

If you run 

kubectl get pod

You will see that the pod has been deleted. and it did not recreate

2. Controller managers: If you use controller managers as a k8s object. That is, if you use controller manager for deploying an application as pod (static pod), if the pod goes down, controller managers would be able to re-create the pod.

NOTE: Static pod is the pod that was created by controller managers. If it get destroyed or deleted, it will automatically rec-create itself. It is different from the pod that was created directed when you make an API call for pod creation.

For example. Let say a junior engineering deleted a pod by mistake, the pod would be created immediately by controller managers. That is what is called sef-healing

The controller managers are as follows

a. Replication Controller  --- rc
b. Replica Set - rs        
c. DaemonSet   - ds
d. Deployment  - deploy 
e. StatefulSet - ss 

REPLICATION CONTROLLER: 

This is one that ensures that a pod is always availabale in the cluster. 
It is the one that create a pod immediately as replacement if any pod crashes or dies.
This is the one that is repsondible in managing the pod lifecycle 


QUESTION ASKED IN CLASS

How can we backup our data in ETCD

Answer

If you are using a manged cluster like the one we have been using, to backup data would be very easy.

You can backup data by the use of external storage system like s3 bucket, ebs volume, and it can also be backup automatically in ETCD anytime you make an API call. 

The ECTD can be found in your project Directory like manifest/ that you created where your yml file is housed.

Anytime you launch or have a fulle installation of k8s tool via aws, a volume will also be created. That volume is ETCD volume that back up all your API calls by the help of API server.

If you look at the s3 bucket that we created along with when we were launching k8s. if you ls the s3 bucket, you will discover that it has some content of the k8s cluster that we have deployed. You will see the manifest/ and other files there.

aws s3 ls s3://bucketName

NOTE: The entire life cycle of a pod are stored in ETCD.  The API server save the info or data entered for the pod in ETCD

A pod is a collection of one or more containers running in a node.

What we have been able to do so far is deploying via KOPS cluster

The installation we did was KOPS installation.

Kops, EKS,  AKS  are the ones companies are using today because they have more well managed k8s cluster services.

KOPS = k8s operations
EKS = elastic k8s service
AKS = amazon k8s service

kubeadm was what companies where using about 5 years ago

When you use KOPS cluster as you k8s tool, it also comes with other managed aws services like elastic load balancer and others.

KOPS is very expensive to maintain

When a worker node is terminated in a KOPS server, the terminated node is re-created because of the fact that KOPS server upon installation comes with ASG, LOAD BALANCER, LAUNCH CONFIGURATION, CONTROLPLAIN (MASTER-NODE).

It is the auto-scaler that re-create the terminated note or well the node goes down with the help of launch template because it is already configured by default that some numbers of worker node should be created along with the master-node and so if any of the worker node is deleted, it will be re-created. The launch template has all the configuration of any ec2-instance that is running.

You can also decide to reconfigure the ASG by going to the ASG and change the desired capacity and minimum capacity to 3 and maximum capacity to 10. 

Go to your kops ec2-instance in aws, click on ASG, you will see that master auto scaler and the nodes auto scaler is created by default upon launch.

click on the nodes, click edit, increase the both the desired and minimum cap to 3 and maximum to say 6 and then save

if you go back to your kops ec2-instance and refresh, after a while will notice that another node is initializing. The ASG make sure that in anytime, 3 nodes must be running in the KPOS server.

If you also delete any of the node, it will rec-create itself after a while.

You can verify this by running

kubectl get node

You will see that a new node has be launched

kubectl get node -w 

The above command will give me a floating file. meaning, it display all successful activities that is coming newly.

HOW TO SHUT DOWN YOUR KOPS SERVER TO REDUCE COST

If you do not want to terminate your kops server completely, you can scale down the resources 

This implies scaling down the nodes and the master node (controlPlain).

You can do that by going to your ASG click on ControlPlain node and reduce the desired, minimum and maximum to 0. also click on the nodes and reduce the desired, minimum and maximum to 0. This will automatically short down all the nodes.

If you shutdown all the nodes, the volume will not be terminated because it is a backup of data or info by API server. the data is backup in the ETCD.

Note: If you shutdown a node, it you reinstate or restart the node, its IP address will change. If you want to ssh into your mab-xterm in your windows OS, you need to copy the new IP address to do that.


VIDE0 126

K8S cluster is made up of 

1. ControlPlain (maternode). This is the one that manages the enter cluster. Its components include
a. API server
b. controller managers
c. scheduler
d. etcd

a. API server. This is the most important components that is responsible isn making sure that
anything you do in kubernetes is achieved. It is the one that go to action whenever you run any command in k8s. The command that you run is called API call.
Examples of API calls are

kubectl apply pod/node/svc or kubectl create pod/node/svc

kubectl delete pod/node/svc  podNAme/nodeName/serviceName

kubectl get pod/node/svc -o wide

kubectl describe pod/node/svc -o wide

kubectl edit pod/node/svc

kubectl start pod/node/svc 

kubectl run

kubectl log

kubectl scale

kubectl top

kubectl expose

How are API calls made in k8s

API calls are made using

kubectl via CLI  or via UI

Anyone cannot just make an API call in a server. That person needs to be authenticated before he can be able to make an API call

How do you know if you are authenticated?

For you to be authenticated, you need what is called kube/config file. This is the file that authenticate users. You use this file to access the cluster

After the kube/config file, we have what ius called RBAC (role based access control)

RBAC determines what you can do or what you are authorized to do or permitted to do in the cluster. that some users that you authorized or permitted to just view or read but can not write. just read permission.

RBAC

Engineers may be authorized to make an API calls as below

kubectl create
kubectl apply
kubectl expose
kubectl delete
kubectl scale
kubectl exec

These are more serious calls/ We may decide only managers or senior engineers can do all of those

What is difference b/w kubectl create and kubectl apply ?

kubectl create will create or provision a pod or node that has not been existing

kubectl apply will create or provision a pod or node that has not been existing and also update a pod or node that has been existing

Developers may be authorized to make an API calls as below

kubectl get
kubectl logs
kubectl describe
kubectl top

The above are not serious calls.  This can be permitted by developers to have access to

The roles are placed on members of team depending on their possition. We use RBAC to place this roles on team members. Every team members must be authenticated and authorized to perform a certain task or to make an API CALL.

NOTE: 

For you to be authenticated, you need .kube/config file. 

.kube/config file is like a password. You use the .kube/config file to access the cluster.

The senior DevOps engineers may have more permissions to do more in the cluster. 

For you to be authorized, you need RBAC

The developers may have restriction access to do more things in the cluster. This can be controlled by RBAC.

What you can do in the cluster is determined by RBAC

The admins should be authorized and authenticated.

It is not good to use a single node cluster like Minikube and dockerDesktop to run your production because it would lack the advantage for scaling for the replication of another node if there is a sudden spike in user traffic. 

It does not support a multi-node cluster

You can used these single node k8s cluster just for practising


With kubeadm software, we can set up a multi-mode k8s cluster

Note:

if you want to delete all of your pods in your node, run

kube ctl pod --all

It will delete all the pods you have in that worker node

NOTE: If you use a command to deploy application as pod in k8s environment, it is called IMPERATIVE approach but when you use a file like yml file to deploy application as pod, it is called DECLARATIVE approach.

Note using a file to declare the application is what is more sophisticated

KUBERNETES OBJECTS OF APPLICATION DEPLOYMENT

The k8s objects are as follows:

1. POD: You will not be able to scale when you deploy application via pod as we have been doing above. If you deploy application via pod directly, if the pod goes down, it cannot be able to re-create itself. 

The life cycle of pod is very short. if you deploying application using pod directly

For example. Delete the pod if created before by running

kubectl delete pod podName

If you run 

kubectl get pod

You will see that the pod has been deleted. and it did not recreate

2. Controller managers: If you use controller managers as a k8s object. That is, if you use controller manager for deploying an application as pod (static pod), if the pod goes down, controller managers would be able to re-create the pod.

NOTE: Static pod is the pod that was created by controller managers. If it get destroyed or deleted, it will automatically rec-create itself. It is different from the pod that was created directed when you make an API call for pod creation.

For example. Let say a junior engineering deleted a pod by mistake, the pod would be created immediately by controller managers. That is what is called sef-healing

The controller managers are as follows

a. Replication Controller  --- rc
b. Replica Set - rs        
c. DaemonSet   - ds
d. Deployment  - deploy 
e. StatefulSet - ss 

REPLICATION CONTROLLER: 

This is one that ensures that a pod is always availabale in the cluster. 
It is the one that create a pod immediately as replacement if any pod crashes or dies.
This is the one that is repsondible in managing the pod lifecycle 

CREATION OF A STATIC POD USING REPLICATION CONTROLLER

Let us create a replication controller yml file rc.yml as below

Replication Controller  --- rc = rc.yml 
kams = rc.yml 
# ReplicationController

apiVersion: v1
kind: ReplicationController
metadata:
  name: RCPodName
  labels: LabelsOfMasterPod
    key: value
spec:
  replicas: NoOfPodReplicas. let say 3
  selector: # ReplicationController will fine pod based on the below key and value
    key: value
  template:
    metadata: PodMetadata
      name: PodName
    labels: Podlabels
      key: value
    spec:
      containers:
      - name: containerName
        image: <imagaName>
        ports:
        - containerPort: containerPort
---
apiVersion: v1 
kind: Service
metadata:
  name: serviceName
spec:
  type: serviceType 
  selector:  must match with labels of the pod. That is below
    app: webapp
  ports:
  - targetPort: match with container port above
    port: servicePort (80  maybe)


ReplicationController 
========================  rc.yml  
apiVersion: v1   
kind: ReplicationController    
metadata:
  name: apprc  
spec:
  selector: 
    app: webapp
  replicas: 3      
  template:
    metadata:
      name: app   
      labels:
        app: webapp
    spec:
      containers:
      - name: appc 
        image: mylandmarktech/hello
        ports:
        - containerPort: 80  

---
apiVersion: v1 
kind: Service
metadata:
  name: webappsvc
spec:
  type: NodePort 
  selector:
    app: webapp
  ports:
  - targetPort: 80
    port: 80


If you check above file, under service, we did not pass any nodeport and nodeportNumber to it which we are supposed to do sinve the service type is NodePort. The system can assign all of that for us. We you want to pass it, you can go ahead. the nodeportNumber ranges between about 30000 to 32500

We did not pass a namespace here because we have changed our default context name space to dev.

You can found out your current namespace that you are working with by running

kubectl get ns  or kubectl get ns -o wide

If you have not changed it from defauly namspace to dev or something else, 

You can create the namespace by running

kubectl apply ns dev 

and then you can change your context namespace from default to dev that you just created by running

kubectl config set-context --current --namespace=dev

It will make dev to become you context namespace space that you will be working with henceforth.

Note: Since you 3 replica pods to be created, you need to have not less than 3 worker node and one master node running in your k8s cluster because each of the pod would be assigned to one node and so the 3 pods would be assigned to 3 nodes individually.

I think even if you have less than 3 worker node running, once you deploy the above that has 3 replca pod to be created, k8s would automaticlly provision another node in which the last pod would be deployed from, with the help of the replicationController if the maxiumum capacity of the node in ASG is above 3.

vi into apprc.yml and the paste the above inside it and the save-quit

to create the pod via replicationController and also create service at the same time on the same yml file, we run

kubectl apply -f FileName

kubectl apply -f apprc.yml

Both the service and the pod would be created.

Note: the pod names that would appear or that will be created is the masterPodName.

to verify if your service is created, you run

kubectl get svc

You will see that 3 services was created for each pods. This nodePort service has their own ip address.

Since we have 3 pods that is created on 3 nodes individaully, it means that 3 nodePortservice were created.

To verify this, run

kubectl get ep

You will see that for serviceName webappsvc, we have 3 services created which has service IP mapped with port.

Each of the service ip addresses are different.

The pods ip addresses are also different because of course they are assigned to different node by csheduler. The work of a scheduler is to assign the pods created to different nodes.

Note: Endpoints are usually pods, and each service is connected to the pods to make the application running in the pods securely to accessible

If you run

kubectl get po -o wide    or     kubectl get po -o wide

You will see that the 3 pods that was created, each of the pods is attached to a node. it means a pod is attached to one node, the other is attached to another node and the last one is attached to the another one.

Since the NodePort service would expose the application internally and externally

if you run

kubectl get svc -o wide

You will see that on the same line with the serviceName, the servicePort is mapped with the NodePort

Since we did not pass a nodePort, the system will choose the nodeport
 The servicePort that we chose here is 80. 

 It will be in order of

 servicePort:nodeport     that is 80:nodeport

 The right digit numbers is the nodeport.

 The nodeport reflects the type of service that we are using. It is under service.

 You can access the application EXTERNALLY in CLI by running

  Since we have 3 pods, you can just spot any of the pod and see which node is the pod assigned to and the copy the ipadddress of the node of that pod, then also spot which service is assigned to that same pod and also copy the nodeport of that service

 curl -v nodeIpaddressOfThePod:serviceNodeport

 serviceNodeport ranges between 30000 to 32700

 You can get the ip address of the node by running

 kubectl get node -o wide   or   kubectl get node  or  kubectl get node nodeName

 This will show you all the node you have here and also you will see the ipaddress of each node

 You can also paste the above on your google browser. 

 You will see that is is accessible

 If you refresh the page, it will change the serviceName and ipaddress. This means that in every visit, different type of node, pod and service would be running. The service is the one that is routing the traffic to different pod, node on every visit or on every refresh mood.

 NodePort service is the one doing the load balancing here.

NodePort service also do internal access by running

curl -v serviceClusterIpaddress

NOTE;

Pods are managed by replication Controller.

If you run

kubectl get rc -o wide

You will see the MasterpodName that we have created via RC and 3 replicas in the form of desired and the number of pod currently running, the name of the container running inside the pod. You will see the imageName in which the container was created and started and lastly you will see the selected which usually reflects the label of the pod which is the key and value.

LET US DESCRIBE THIS RC

We are describe or inspect this rc BY RUNNING

kubectl describe rc rcName

kubectl describe rc apprc

It will give you insight about the replicationController pod that was created

SCALING

If there is a spike in user traffic, the rc has the tendency to create more rcPods by ruuning

kubectl scale rc rcName --replicas=NumberOfScale

let us scale to 5, by ruuning

kubectl scale rc apprc --replicas=5

If you run

kubectl get pod

You will see that it is now 5 rcpods running. addition 2 pods was created.

Note: in scaling, the addition pods that was created was attached to another new 2 created worker nodes. The scaling created more 2 pods and also 2 nodes respectively

We scaled from 3 to 5 pods. This is called SCALING UP

Let us SCALE DOWN   by running

kubectl scale rc apprc --replicas=2

We have scaled down. if you run

kubectl get po -o wide

You will see that we now have just 2 pods

If you also run

kubectl get node -o wide

You will see that we now have 2 workerNodes running.

REPLICA-SET

ReplicaSet RS is the next generation ReplicaController RC

It is like a newer version of RC

Both RC and RS manages pod replicas and state

The only different betweem RC and RS is the selector support system

The ReplicaSet supports the new set-based selector requirements

ReplicationController supports the equality-based selector requirements

EXPLANATIONS

EQUALITY-BASED SELECTOR

RC -->  only Supports equality based selectors.

Equality Based Selector: This is a selector that only has one key and value. That is key:value

labels:
app:webapp

selectors:    Example of equality based selector
app:webapp

The above can work with RC and RS

SET-BASED SELECTOR

Set Based Selector: This is a selector that can have multiple key and value and also can have just one key but multiple value .

Example of set-based selector is below

selector:
 matchExpressions: =# is Set Based
  - key: app
     operator: IN
    values:
   - webapp
   - webapplication
   - web

This set based selector above have one key and has multiple values
RS --> Supports eqaulity based selectors and set based selectors at the same time.

selector:
   matchLabels: =# is Equality Based
     key: value
     app: webapp

selector:
   matchExpressions: =# is Set Based
   - key: app
     operator: IN
   values:
   - webapp
   - webapplication
   - web


Mainfest File for ReplicaSet (RS) is in the form below:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: <RSPodName>
spec:
  replicas: <numberOfPODReplicas>
  selector:  # To Match POD Labels.
    matchLabels:  # Equality-Based Selector
    <key>: <value>
    matchExpressions:  # Set-Based Selector 
  - key: <key>
    operator: <in/not in>
    values:
    - <value1>
    - <value2>
  template:
    metadata:
    name: <PODName>
    labels:
      <key>: <value>
  spec:
  - containers:
    - name: <nameOfTheContainer>
      image: <imageName>
    ports:
    - containerPort: <containerPort>


EXAMPLE

example:
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mavenapprs
spec:
  replicas: 2
  selector:
    matchLabels:   
      app: mavenapp
  template:
    metadata: 
      name: apps  
      labels:
        app: mavenapp
    spec:
      containers:
      - name: appsc
        image: mylandmarktech/java-web-app
        ports:
        - containerPort: 8080 
---
apiVersion: v1
kind: Service 
metadata:
  name: app-svc 
spec: 
  selector: 
    app: mavenapp
  type: NodePort
  ports:
  - targetPort: 8080   
    port: 80
    nodePort: 32000  


vi into rs.yml and paste the above and then save-quit

run

kubectl apply -f rs.yml


If it refused to run, it may let you know what line number of the file that is broken.

if you want your file view to reflect number of line while you are inside the file. That is, if you vi into the file, you can type in

:se nu

and the file will show line numbers.

There was an error when prof did it earlier because there was no space but it was corrected. So, no more error henceforth.

If you run

kubectl get rs -o wide

It will list the rs and the rsPodName that you have created.

kubectl describe rs rsNAME

It will give more details of the rsName that you have created

You can also verify the service by running

kubectl get svc -o wide

You can also verify the pod by running

kubectl get pod -o wide

You can also verify the endpoint by running

kubectl get ep -o wide

If there is a spike in your application, you can also increase the number of your application by scaling up by running

kubectl scale rs rsPodName--replicas=NumberOfReplicasNeeded

let us scale to 4

kubectl scale rs --replicas=4

let us scale down to 2

kubectl scale rs --replicas=2

SOME COMMAND THAT YOU CAN RUN IN REPLICA-SET

kubectl get rs 
kubectl get rs  -o wide 
kubectl get rs -o yaml  
kubectl get rs -n namespaceName   = when the namespace is not set as the context namespace.
kubectl get all
kubectl scale rs rsPodName --replicas NumberOfReplicasNeeded
kubectl describe rs rsPodName
kubectl delete rs rsPodName


IQ: What is the difference b/w docker serivce and k8s service?

Answer: In Docker Swarm, docker service manages the containers. we can access containers using serviceName in Docker. 

docker-compose.yml 
version: '3.1'
services:
  springapp 
  mongdb  


In the above, you will see that springapp container and the mongodb container is under services which therefore means that containers are managed and controlled by the docker service.

If you go through dockerSwarm video, you would understand better

in k8s service does not creating/managing the pod. 
controllers manages the pods state   
Service in k8 is only used to route traffic to the pods. It does the DNS resolution. It is serves as a load balancer to the pod. It also exposed the application running in the pods.


We have deployed quite some applications.

If you want to see what you have deployed so far, you can run

kubectl get all

It will show you the things that you have deploy.  You will see that we have deployed ReplicationController pods and also ReplicaSet pods.

DAEMON-SET

DaemonSet: This is the same thing like deploying application in global mode using dockerSwarm

DaemonSet = Global mode in Docker-Swarm deployment 

When you deploy application using dockerSwarm, you can deploy a ReplicaMode and in GlobalMode

If you deploy in ReplicaMode, it means that you are deploying in default mode which involve deploying in ReplicaSet or in ReplicaController

DaemonSet is used to deploy application in Global mode 

DaemonSet can be use to monitor all in the nodes in our kubernestes cluster to see if the nodes are healthy or not, it can perform regular health checks on all the nodes.

DaemonSet can be also be used to ensure that scheduler schedule a pod creation in each of the node that we have in the cluster if need be. 

IQ

When you have 15 worker nodes in our Kubernetes cluster and you want to deploy a monitoring agent to monitor the nodes, Which kubernetes object should we used?? 

Answer:

The kubernetes object that we will use is DaemonSet

Example

If we have 12 nodes in the cluster 

  Then Workloads deploy via DaemonSet will assign a pod in each node or the nodeGroup

 For NodeGroup

 If we have the below nodeGroup

 nodeGroup 1 has 4 db-nodes  
 nodeGroup 2 has 4 app-nodes  
 nodeGroup 3 has 4 web-nodes  

DaemonSet can use Node affinity or node selector to deploy application in pod all the nodes in each individaul nodeGroup selected. If we want the deployment to only be on nodeGroup1, DaemonSet will achieve that with the help of the node selector

If you do not used the node selector to select the nodeGroup, DaemonSet will create pods in each of the nodes

Daemon deployment is Good for deploying:

a. monitoring tools 
b. log management tools 

In dockerSwarm, it is called GlobalMode

In dockerSwarm, it is called DaemonSet

Eaxample

If you want to deploy something like EFK (ElasticSearch,  FileBeat and Kibana). Prof said we will do that soon.

1. FileBeat is an agent, it gathers the logs from the nodes. For this to occur, all nodes should have a Filebeat running in them

WHAT IS A LOGS

Logs files is a data file that stores events, processes, messages, and other data from applications, operating systems, or devices.

We use DaemonSet to deploy fileBeat in a node. FileBeat is a logs agent.

We do not need DaemontSet to deploy Kibana

2. In Prometheus and Grafana, 

Prometheus comes with the following

 a. NodeExporters

 NodeExporters is the logs agent that is used to gather the logs in all the nodes. 

 NodeExporters can be deployed as DaemonSet

  b. Prometheus server 
  c. kube-state-metrics 
  d. alertManager 
  e. Prometheus-UI 


LET US DEPLOY A LOG CONTAINER USING DAEMO-SET

Deploying a log container in a pod using a DaemonSet

The manifest file is in the form of below

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: logds
spec:
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      name: log
      labels:
        tier: frontend
    spec:
      containers:
      -  name: logc
         image: mylandmarktech/hello
         ports:
         - containerPort: 80


let us vi into ds.yml and paste the above and save-quit and the run

kubectl appy -f ds.yml

logds will be deployed

Initially Prof., made a mistake in the manifest file where he inputed v1/apps on the api versions. That is, he did   

apiVersions: v1/apps

So he was unable to create and start up the log agent or DaemonSetPod, he was been told that there is no such api version.

What he did was to check out the api version for k8s objects by ruuning

How to get api Versions in k8s?

  kubectl api-resources 

  To check the api version for DaemonSet, we run

  kubectl api-resources | grep -i DaemonSet 

  To check the api version for replication controller and ReplicaSet at the same time, we run

  kubectl api-resources | grep -i replica*

Running the above command, we now know that the apiVersion for DS is apps/v1. That is how he corrected the manifest file and it worked out.

To know if the DS was deployed, run

let us first of all check how many number of node that we have by running

kubectl get node

It say it deplayed 4 working Nodes and 1 masterNode

When you now run

kubectl get ds

You will discover that we have on the same line with the DSName, 4 desired, 4 current, 4 available .

This means that ds was deployed to the 4 worker nodes that is existing in the k8s cluster even without defining it in the manifest file.

What DaemonSet does is that it will create one replica pod in each node that is existing in the k8s cluster without even defining it or passing it in the manifest file.

If you run

kubectl get pod -o wide

You will see that it created the 4 ds pod in the 4 nodes that are existing.

NOTE:

DaemonSet can not be scaled. It is because it is not replicaMode-based. It is globalMode based.

If you running

kubectl scale ds dsName --replica=6

You will see that it will not work.

IQ: You maybe asked in an interview

How have you deployed applications/workloads in Kubernetes?

Answer: 

We have been deploying applications in k8s using 

1. Pods, ReplicationController, ReplicaSet, DaemonSet, and Deployment

2. Deployment is the recommended kubernetes object to
      1. run workloads in k8s  and 
      2. to deploy applications in Kubernetes


VIDEO 127  KUBERNETES

In k8s cluster, we have what is called a masterNode and if possible workerNodes 

In the case of Docker desktop k8s cluster, we have jst a master node. It is a single node k8s cluster.

In the case of KOPS k8s cluster, we have a master node and worker nodes. This is a multi-node k8s cluster.

QUESTION

can pod be created in masternode or controlPlian?

Answer:

Yes, pod can be created in the master node if it is a single-node k8s cluster

For a multi-node cluster, pod should not be deployed on the master node.

When the Kubernetes cluster is first installed, a Taint is set on the master node. This automatically prevents any pods from being scheduled on this node. You can modify this by yourself and deploy pod to master node  if required but for best practice, do not deploy application workloads on a master server.

In k8s, once you have deployed the pods, it means that you have deployed the applications

By default upon k8s installation, Master node has it own pods. These are the components of master node. they include

1. api server This is used for making api calls like kubectl get pod etc. The api server is a pod and it has container running in it

2. etcd: this is also a default pod, having container running in them. They are used to store or persive data whenever you make an api call.

3. scheduler: this is also a default pod in the mast node, having container running in them. It is used to schedule a pod in the worker node and assign it to a node.

4. Controller managers ( these are cloud-controller manager and kube-controller manager)

For you to verify that the above listed are pods, run

kubectl get pod -n kube-system

You will see that the listed above are listed as deployed as containers running in pods with their names and master attached to it for you to know that they are pods in the master node by default upon full installation of a multi-node k8s cluster.

To get more info about the pods in kube-system, you can run

kubectl get pod -o wide -n kube-system

Above are the components of masternode

To us check if we have a daemon set in the kube-sytem by running

kubectl get pod -o wide -n kube-system

You can see that we have ds in kube-proxy and weave-net

kube-proxy and weave-net are used for networking. They were deployed as a deamonSet pods to be attached in each of the node by default. They are networking agents.

components of the worker nodes

1. kube-proxy: kube-proxy is a network proxy that runs on each node in your cluster. It implement part of the k8s service concept

2. kubelet: kubelet. An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod

3. container run time. like containerd or docker: In a containerized architecture, container runtimes are responsible for pulling container images from a image registry. It is used for managing the entire lifecycle of the container. 

We use service discovery to access application in k8s both internally and externlally.

For pod1 to be able to talk to pod2 in the same cluster, they need a service called clusterIp service which is the k8s default service. 

Note: 

1. Since it is pod1 that wants to talk to pod2, the service that pod1 would talk to that will carry the message to pod2 will be pod2-service  (pod2-svc).

Meaning that the service should be mounted on or have the details of pod2 when writing the manifest of clusterIP service for pod2

2. If it is the pod2 that want to communicate with pod1, the name of the service is also called clusterIP service and the name should be pod1-svc

Meaning that the service should be mounted on or have the details of pod2 when writing the manifest of clusterIP service for pod1


For end user to be able to communicate with a let say pod1 inside a k8s cluster, the service that is needed for the end user to do that is called NodePort service or Loand balancing service.

What happens is that once the end-user initiate the communicate, it is routed to the masternode with the help of api server inside the master node will route the traffic to the right service which the the nodePort service or Load balancing service with the help of port forwarding. It is one of this service that will now route the traffic or request to pod1. 

what the user would have access to, in order to talk to pod1 is

nodeIpadressOfPod1:serviceNodePortNumber

serviceNodePortNumber is between 30000 to 32700. You can choose any numbers within the said range by defining it in the service manifest file. If you did not define it in the manifest file, the system would choose the number for you and it fall with the numbers above



The writing of the manifest file for service should reflect details of pod1. and so you may name it as pod1-service

DEPLOYMENT 

Deployment As One Of Th Object Of k8s in deploying containers running in pods

Deployments:

IQ:

You maybe asked, how do you deploy with k8s in landmark?

Answer:

We deploy with k8s in our environment using a Kubernetes object called "Deployment" to deploy  our applications in containers running in pods

IQ:

You maybe asked, What is your deployment strategy: 

Answer:

With kubernetes the Deployment strategy are 

        RollingUpdates 
        Recreate 
        blue/green 
        canary 

The apiVersion =  apps/v1 supports equality and set-based selectors 
IQ:     What is the default Deployment strategy in Kubernetes? 
answer: RollingUpdates

Note:

A Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.

Deployemnt is k8s object used to deploy containerized applications running in RsPods in k8s

Deployment does not directly create pods but create replicaSet that create pods

Deployment is the most advanced way of deploying replicaSet or Rspods (That is replicaSetPods) in k8s because of the loaded feature it brings to the table as listed below from 1 to 8

Deployment is the most recommeded way of deploying containerized applications.

Using Deployment as K8s object, you can do the below

1. You can create a deployment to rollout a replicaSet. This means that the creation of deployemnt, a new replicaSet is created

2. You can update a deployment to a newer version of deployment

3. You can pause and resume a deployment

4. You can rollback older version of a deployment

5. You can check a deployment status which determine the state of replicas

5. You can scale deployment up and down

6. You can run a clean up policy of a deployment

7. 3. You can run a canary deployment

According to prof Note below

IQ: In kubernetes, What is the default Deployment strategy 

Answer: The default Deployment strategy in k8s is the RollingUpdates.

If we do not define any strategy when writing the manifest file for Deployment k8s object, the strategy would be RollingUpdates by default.

Deployment Strategy: These are

        RollingUpdates: This is the default strategy
        Recreate 
        blue/green 
        canary 
        e.t.c

Let us create maunifest file for Deployment

Let us check the api version for Deployment by running

kubectl api-resource | grep deployment

You will see that the apiVersion is apps/v1 

If you check the api version for replicaSet by running

kubectl api-resouces | grep replicaset

You will see that it is also apps/v1

The api version for DaemonSet is also apps/v1

This apiVersion = apps/v1 supports equality and set-based selectors 

If you check the api version for replication controller by running

kubectl api-resources | grep replicationcontroller

You will see that the api version is v1

api version is v1 only supports equality which is key and value only


IQ:     What is the default Deployment strategy in Kubernetes? 
answer: RollingUpdates

kubectl api-resources | grep deployment    
deploy.yml  


LET US DEPLOY A FRONT END (fe) APPLICATION USINF DEPLOYMENT OBJECT

apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: webapp  
  namespace: facebook    
  labels:
    tier: fe  
spec:
  strategy:   
  replicas:   
  selector:  
  template:

Note: 

1. If we do not define any strategy when writing the manifest file for Deployment k8s object, the strategy would be RollingUpdates by default.


2. Also, If we do not define number of replicas when writing the manifest file for Deployment k8s object, the replicas that would be created is just 1.


---
ReplicationController apiVersion = v1  

Therefore, the labels or selector is jsut key mapped with value. That is 

    selector: 
       app: webapp


Deployments, ReplicaSet, DaemonSet apiVersion = apps/v1

Therefore, the selector is in the format like below

      selector: 
        matchLabels: 
          app: webapp
      selector: 

The above is called equality-based selectr

You can also have the selector format as below

  selector: 
    matchExpressions: 
   - key: <key>
    operator: <in/not in>
    values:
    - <value1>
    - <value2>

 The above is called set-based selector

 Yoi can use any of the two SELECTOR in writing your Deployment manifest file. 

 Let us use equality-based selector

 apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: webapp  
  namespace: facebook    
  labels:
    tier: fe  
spec:
  selector: 
    matchLabels:
      app: webapp
  template:
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      containers:
      - name: springapp 
        image: mylandmarktech/spring-boot-mongo  
        ports: 
        - containerPort: 8080 
        env:
        - name: MONGO_DB_USERNAME  
          value: devdb   
        - name: MONGO_DB_PASSWORD 
          value: devdb@123  
        - name: MONGO_DB_HOSTNAME
          value: mongo    


vi into d.yml and paste the above and save-wuit 

Let us deploy the Deployment manifest file by running

kubectl appy -f d.yml

It will deploy the application in facebook namespce because we define namespace in the file

you can verify by running

kubectl get deployment -n facebook

You will see that the 1 Deployment of RsPod was deployed because we did not define number of replicas in the manifest file

If you run

kubectl get all -n facebook.

you will see that the Deployment that was created rollout or created a replicaSet and the replicaSet created a pod.

Deployment object is what we are going to be suing to deploy workload in kubernetes because it is more powerful.

Let us create a service to be able to access the above application runing in development-rs-pod

 apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: webapp  
  namespace: facebook    
  labels:
    tier: fe  
spec:
  selector: 
    matchLabels:
      app: webapp
  template:
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      containers:
      - name: springapp 
        image: mylandmarktech/spring-boot-mongo  
        ports: 
        - containerPort: 8080 
        env:
        - name: MONGO_DB_USERNAME  
          value: devdb   
        - name: MONGO_DB_PASSWORD 
          value: devdb@123  
        - name: MONGO_DB_HOSTNAME
          value: mongo    


---
apiVersion: v1
kind: Service
metadata:
  name: fcappsvc
  namespace: facebook
spec:
  type: NodePort
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 31500  #30000-32767

make sure that service and the Deployment manifest are in the same namespace, the labels should match with the selector in service, target port shoudl match with container port.

via into deploy.svc and paste the above service manifest and save-quit and run

kubectl apply -f deploy.svc

You will see that the deployment-rs-pod was created.

You can verify by running

kubectl get deployment -n facebook -o wide

You can check the endpoint  to see if the service is truely routed to the application running in dep-rs-pod by running

kubectl get ep -n facebook -o wide

You will see that the clusterIp address of the service is mapped with the contanerPort.

You can scale this deployment by running

kubectl scale deployment deploymentName --replicas=numberOfRelicasNeeded -n facebook

let us scale to 4. we run

kubectl scale deployment webapp --replicas=4 -n facebook

If you run

kubectl get deployment -n facebook -o wide

You will see that the total number of dev-rs-pods is now 4

Note that anytime you scale to create more pods, nodes are also created as a result of that in which the scheduler will assign the creatd pods to.

To access the application internally, we run

curl -v podClusterIpaddress

To access the application externally, we run

curl -v NodeIpaddresOfPod:serviceNodePortNumber

You can alway copy any of the node ip address to test the access in web page.

TYPES OF DEPLOYMENT OBJECT IN K8S

There are different typeS Strategy in Deployment object. These are

RollingUpdates 
Recreate 
blue/green 
canary 

1. rolingUpdates

Deployment with rollingUpdate means a default deployment. rollingUpdate manifest file and its service manifest file is below

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  selector:
    matchLabels:
      app: mavenapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 30
  replicas: 4
  template:
    metadata:
      name: mavenapppod
      labels:
        app: mavenapp
    spec:
      containers:
      - name: mavenappcontainer
        image: legah2045/maven-web-app
        ports:
        - containerPort: 8080

---
apiVersion: v1
kind: Service 
metadata:
  name: app-svc 
spec: 
  selector: 
    app: mavenapp
  type: NodePort
  ports:
  - targetPort: 8080   
    port: 80
    nodePort: 32700  

NOTE:


If you want to see all the resources in you context namespace, you run

kubectl get all

If you want to see all the resources in a namespace different from the context namespace, you run

kubectl get all -n namespaceName

If you want to delete all the resources in your context namespace, you can run the following

kubectl delete all --all

If you want to delete all the resources in a namespace different from the context namespace, you can run the following

kubectl delete all --all -n namespaceName

In the above Deployment manifest file, the strategy is rollingUpdate. Remember, even if you did not define any strategy inthe file, the strategy will still be rollingUpdate because that is the default strategy

Note. The service is used to exposed the containerized application running in depl-rs-pods

Let us explain the Depl. manifest file above regarding rollingUpdates under strategy

Let us assumed developers has written another newer version of the application commited to github, this v2 is going to be tested, built, backed up and ready for deployment.

But we have v1 of the that a containerized application has been deployed before in 4 replicas pods, pod1 pod2, pod3 and pod4.

Now we want to rollout another version called v2 thats has been written by developers to replace v1. That is what the mannifest file about rollingUpdates means.

In rollingUpdate or to rollout the new version v2, there is a strategy, one of them is

maxUnavailable: 1  = this means since we have  v1 running in 4 pods in the cluster, during the rollingupdate or rollout to v2, only one pod like pod1 of v1 is expected to be taken down which makes it to be unavailable and then replace it by v2-pod1. The maximum pod that would be unavailable should be 1. if it is maxUnavailable: 2   This means that the maximum that should be taken down and replaced at the same time is 2.

Once the new version v2-pod1 start running, another v1-pod2 is taken down and v2-pod2 replaces it. Once v2-pod2 starts running, v1-pod3 is taken down and v2-pod3 is deployed. Once v2-pod3 starts running, v1-po4 is taken down and v2-po4 is deployed. 

minReadySeconds: 30  = This means that it will take like 30 seconds for v2-pod1 or for the new pod replacement to start running

This strategy is called rollingUpdates. It will maintain a zero downtime until the entire process is completed. 

This means that during the taken down of the pods to be updated or replaced by a newer version do not distrupt or stop the application from running or cause an outage. that is, if pod1 in v1 is taken down, it will cause a total outage because v2 would be deployed to replace it within 30 seconds

WHAT IS ZERO DOWNTIME?

Zero downtime deployment is a deployment method where your website or application is never down or in an unstable state during the deployment process. To achieve this the web server doesn't start serving the changed code until the entire deployment process is complete.

Let us delete the resources we have in our context namespace by running

kubectl delete all --all

Create a directory and call it deploy by running

mkdir deploy

cd into deploy

cd deploy

via into app.yml and paste the above manifest file of the rollingUpdates DEPLOYMET and its service and then save-quite and then run

kubectl apply -f app.yml

NodePort service and 4 dep-rs-pods would be created and 

if you run


kubectl get deployment -o wide  or kubectl get all

You will see that 4 deploymet was created, 4 RS was created and 4 pods were created.

You can access it external as

curl -v nodeIpAddres:serviceNodePortNumber

or

curl -v nodeIpAddres:serviceNodePortNumber/applicationName 

You can also paste the below on the browser

nodeIpAddres:serviceNodePortNumber/applicationName or nodeIpAddres:serviceNodePortNumber

https://github.com/LandmakTechnology/kubernetes-notes
https://github.com/LandmakTechnology/kubernetes-manifests

The i forked the above k8s notes of prof in my github account.

If you want to check the history of application that you have rollout, you run

kubectl rollout history deployment deploymentName

kubectl rollout history deployment myapp


LET US UPDATE THE DEPLOYMENT ABOVE

let us update the Deployment called myapp. Let us deploy a new version of this deployment.

You can update the Deployment myapp by running

kubectl edit deployment deploymentName

kubectl edit deployment myapp

Let us change the image to another image. This means that we are changing to another version. let us change it to an imaged called mylandmarktech/maven-web-app

if you run 


kubectl edit deployment deploymentName

it will take you to ectd file and then click on i  to put it on insert mode and then scroll down to where image is and then change it from legah2045/maven-web-app  to  mylandmarktech/maven-web-app

save and quit.

You will see that it says deployment edited. This means that you have deployed another version of the application.

If you run

kubectl get rs -o wide

it will display the rs old version and the rs new version of the the application. This means that the old version is still very much around but not running. It is stored in the etcd. 

You can access it on your webpage by pasting

nodeIpAddres:serviceNodePortNumber/applicationName

It will display

Most of the time, it is this image and some other things you would be changing to upgrade or to rollout another new version of the application.

if you run

kubectl get rs -w

You will see that the old version of the pod is being taken down one after the other and replaced with new version. It continues until the old version becomes zero pod and the new version becomes 4 pods.

Note: The older version is still much around, it is still stored in etcd while the new version is the one running.

If anything goes wrong with this new version, you can rollback or revert to the old version.

NOTE: 

Rollback is possible with replicaSet created from Deployment object.

ReplicaSet as a object does not supoort rollback.

Let us rollback the old application that was replaced by the new one by running

kubectl rollout deployment deploymentName         

above is to to deploy a new version

kubectl rollout undo deployment myapp       

If you run the above command, it will say rolled back. This mean the old version has be rolled back.

above is to  undo the deployment of a new version to older version

if you run 

kubctl get rs -w

You will see that the new version of the pod is being taken down one after the other and replaced with old version. It continues until the new version becomes zero pod and the old version becomes 4 pods again as it used to be.

If you run

kubectl edit deployment myapp

You will see that the image has been reverted back to legah2045/maven-web-app

If you go back to your web page where you accessed the old  and refresh the page, you will see that it has changed the writings and the headings that was displayed on the page.

How to check The Whole Rollout History In Your Context Namespace

You run

kubectl rollout histroy deployment deploymentName

It will show you all the revisions of the application (in the form of version) that has be rolled out or deployed.

HOW TO Rollback Deployment to Specific Revision

This means that let say you have revision or version 1,2,3,4,5 of myapp that you have deployed via development object. That means, you have deployed up to 5 version of myapp. Now you want to rollback to a specific revision or version like revision 3, you can run the below

kubectl rollout undo deployment/deploymentName --to-revision=revisionNumber

kubectl rollout undo deployment/myapp --to-revision=3    

Since we have up to 5 revisions, once you have rolled back reviosn 3, revision will now become 6 which is now the latest version

Zero downtime is achieve USING RollingUpdates strategy to deploy in Kubernetes.

There is no downtime when you use rollingUpdate. This means that there is no traffic outage or running outage or failure in accessing the application by users during rollingUpdate because the pod that is being taken down would still be running until the pod that replaced it starts running. that is when it be storp running. The outage would be experinced. It will maintain a zero downtime.

Zero downtime is the advantage the rollingUpdate strategy has. There would not be an outage or out of reach or out of service during the upgrade or update of your application when a rollingUpdate is used.

rollingUpdate strategy is initiated by default when you did not define strategy in your deployment object manifest file.

Prof encounter a problem in one of the deployment to a new version f the application. He was unable to access the application online when he modified the image. He found out by troubleshooting. He had to get inside the pod to ls webapps directory because deployment in tomcat takes place in webapps directory. He run

kubectl exec myapp-97dfdf5c9-8ng8c ls webapps

kubectl exec podName ls webapps

He discovered that he was using a wrong applicationName to access it on web browser. The application name that displayed on the webapps was web-app but he was using maven-web-app.

By the time he now use web-app, he was now able to access it.

Deployment object support rollback and rollout but rs object does not.

QUESTION:

How do you manage you memory of size of your volume ?

Answewr: if you can delete any replicaSet you think you do not need anymore. Whether the rs is running or not. Any replicaset that is not running or that has been replaced by a new version is still existing but stoired in etcd. You can ppermanently delete it from ectd if you think roolback will never be initiated forever or if you have any version that is the same with it. by running

kubectl delete rs rsName

2. RE-CREATE STRATEGY.

We also have what is called recreate strategy. 

Recreate strategy is always the recommeneded strategy because it causes a total outage while waiting for a new version to be deployed or created.  Your application would be out of service and users can no longer access it because all the whole pods linked to the old version would be taken down and a new pods with the new version would be created. total outage or lack of avaiability is not good for any business.

High availabilty is our priority as  dEVOPS engineers and so we will not recommend RE-CREATE stratey for our clients.

The major advantage of using rolloingUpdate strategy is that you are going to achieve a zero downtime. This is means there will not be any form of network or pod access outage or unvailability. 

Let usd deploy an apllication using re-create strategy in deployment object of k8s by using the manifest recreate format below:

=======
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: hello 
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: hello
  replicas: 2  
  template: 
    metadata: 
      name: hello
      labels:
        app: hello
    spec:
      containers: 
        - name: hello
          image: mylandmarktech/hello:1  
          ports: 
          - containerPort: 80


paste the above in hello.yml  and save-quit and run

kubectl apply -f hello.yml

You will see that the rsPod would be deployed. This is not recommenede by it leads to downtime. application access outage until another is deployed.

We have just deployed 2 replicas using recreate

If we want access to the above application online, we can just create a service for te recreate 

apiVersion: v1
kind: Service
metadata:
  name: hello-svc
spec:
  type: NodePort
  selector:
    app: hello
  ports:
  - port: 80
    targetPort: 80
    nodePort: 32200

paste the above in into hello.yml, save and quit and then run

kubectl apply -f hello.yml

If you noticed, the rs were created at once.

You can then access it on your web page externally and internally of you environment.

Let us deploy a new version of this application by editing the application.

kubectl edit deployment hello

Let us change only the image version from 1 to 2 and then save-quit

If you run

kubectl get deployment

You will see that version 2 was deployed at once and so the all the pods in version 2 will had to start running before it can be accessed. sometime, not all the pods will be ready at the same time. The once that are not ready will be out of outage completely until it starts running. This can cause a downtime or an outage.

NOTE: You can pause a deployment and you can also resume a deployment

TO PAUSE A DEPLOYMENT

kubectl rollout pause deployment/DeploymentName

TO resume A DEPLOYMENT

kubectl rollout resume deployment/DeploymentName


3. Blue/Green strategy.

In blue-green strategy, 

is that we have deployed a blue version of an application running in production with 4 RSpods with blue-service to expose the application and then we want to deploy a new version of the application called greem version but we would deploy it to tesing environment for further testing and looked upon by the QA team and BA team. When the client says they are happy with the new version green, the green version would be switch from testing to production.

We will now have two different version of the application running in the same cluster

This will be our cluster to be overloaded and will consume more resources or size or volume that you have.

Let us deploy the blue version of the application first by the manifest below

Note: The the version of the image that would be pulled from image registry is the version that would create a container application that would appear with blue display upon web access.


blue.yml 
=======
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: blue 
spec:
  selector:
    matchLabels:
      app: blue
  replicas: 2  
  template: 
    metadata: 
      name: blue
      labels:
        app: blue
    spec:
      containers: 
        - name: hello
          image: mylandmarktech/hello:3  
          ports: 
          - containerPort: 80
---
 apiVersion: v1
kind: Service
metadata:
  name: blue.svc
spec:
  type: NodePort
  selector:
    app: blue
  ports:
  - port: 80
    targetPort: 80
    nodePort: 31200

 paste the above in lue.yml  and then save-quit.

image: mylandmarktech/hello:3  is an image would display a blue colouration upon web access when a respliSetPod have been created

run

kubectl apply -f deplyment

The bluw version is running in production now. 

Let us deploy the green version

green.yml 
=======
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: green 
spec:
  selector:
    matchLabels:
      app: green
  replicas: 2  
  template: 
    metadata: 
      name: green
      labels:
        app: green
    spec:
      containers: 
        - name: hello
          image: mylandmarktech/hello:4  
          ports: 
          - containerPort: 80
---
 apiVersion: v1
kind: Service
metadata:
  name: green.svc
spec:
  type: NodePort
  selector:
    app: green
  ports:
  - port: 80
    targetPort: 80
    nodePort: 32200


 paste the above in green.yml  and then save-quit.

image: mylandmarktech/hello:4  is an image would display a green colouration upon web access when a respliSetPods have been created

run

kubectl apply -f deployment

The two version of the application is now running.

The blue version is running in production with 4 Deployment-replicas. Your customers are already accessing this blue version of the application.

Green version is running in the test/QAT environment because that is what is expeted to be done firstly before switching it to production.

Both versions are running but blue is running in production where customers can access it and green version is running in QA/UAT or testing enviroment.

In realtime, some deployment can last for 8 hours.

 We want to avoid downtime, Since the green version is now running in testing/qaT env., if the testing is passed, how do we deployed green version to production to avoid downtime or to achieve zero downtime?

Only one version is expected to be deployed to production and blue version is already a prodduction. 

You can only switch green into production by configuring the blue service to route traffic to green version instead fo blue version.

We can achieve this by modifying only the blue service selector key:value to that of green service key:value. That is from app: blue to app:green.

To avoid downtime since customer are already accessing the blue version, the first thing that you would do is to

a. delete the green service that route traffic to the green version by running

kubectl delete svc serviceName

kubectl delete svc green.svc

This means that green.svc is no longer routing traffic to green version. It is only the green version RSpods that are running

b. You would edit the blue service so that it will stop routing traffic to blue version but will now start routing traffic to green version. By running

kubectl edit svc serviceName

kubectl edit svc green.svc

scroll down and you will see selector, under selector, remove blue and replace it with green and the save-quit

Once you save-quit, blue-service will now automatically starts routing traffic to green version of the application without any form of service outage or downtime by the customers. Downtime is completely avoided. Maintaining high availability is very paramount.

If you refresh the page that you used in showcasing the blue version on your web page, you will see that it has changed to the green version of the application.

Since we have now deploy the new version which is the green version of the application, we can now delete the old version which is the blue version to maxaimize resources.

Note: With blue/green strategy, the new version of the application also called green is usaully deployed to testing environment and it can maybe runs for 1 weeke or 2 weeks and if it passed and free from bugs, vulnerability, and wetc, it can now be switched to production by editing the service of the blue to route traffic to the new version so that a zero downtime can be achieved. 

WHERE BLUE/GREEN CAN BE UTILIZED

Blue/Green deployment is very good to deploy in the area of health. When managing health clients.  Maybe when you want to deploy a new vaccine. You would want to deploy it in testing env. where lots of proper testing can be carried out before switching it to production.

4. Canary strategy.

With canary strategy, this is the process of deploying the new version of the application to areas with less user traffic for a long time maybe for 3 months in a way of testing how the new version works while the old version of the application should be directed to areas with more user traffic.

It is a way of splitting the traffic into 2. where the rollout version would be routed to some users while the old version would routed to the other users.

The new version directed to area with less user traffic is a way of testing the application, like tesing the vulnerabilty, bugs, health checks etc. And this deployment can run for a long time before considering switching to the new deployment to other areas with high user traffic.


Using facebook and geographical zones as an example.


  Facebook  
    CHILDREN 12-17  ----->begin to use the new version as a way of testing the new version
    YOUTHS        ----->  and other below can continue to use the old version for a long time
    ADULTS      
    SENIORS 

Continents
    AFRICA  ----->begin to use the new version as a way of tesing the new version  
    EUROPE 
    AMERICAS 
    ASIA 

Traffic management : 

This result to what is called traffic management. Areas or zones with higher traffic should continue to use the old version while zones with less traffic will begin to use the new version for a long time as a way of testing the new version

With canary, testing can go on for a longer duration of time

END OF THE VIDEO

We will be going to the k8s development advance concept hennceforth like

   Volumes -- NFS  
   configMaps, Secrets, these are secret objects.
   Nginx-Ingress: This is layer 7 load balancing
   helm  --- this is the package manager for k8s. it is used in writing manifest files   
   haProxy: this is a self-managed loadbalancer
   EKS/kops   
   SCALING: HPA (horrizontal port auto-scaller), VPA (vertical port auto-scaller), CAS (cluster auto-scaller) 
kubectl scale deploy. This was a manual auto-scaling

   APM  (apllication performance monitory): using prometheus and Grafana  
   Log mgt: using  EFK (elastic search, file beat and kibana) or ELK (Elastic Search, Logstash, and Kibana ) 
   Kubernetes security  

Ansible  
Bootcamp  = September 1, 2022 



VIDEO 128. KUBERNETES CONTROLLER MANAGERS

NOTE:

1.  When user wants to talk to the backend application running in pods,  the user will type in

nodeIpaddress:NodePortNumber

2. As soon as the node or workerNode receive the traffic from the enduser, he would route the traffic to the right podService called service through the serviceIpaddress and the servicePortNumber. That is 

serviceIPAdress:ServicePortNumber

Advantage Of controller managers

1. Contrroller Manages help to manage the state of the pod.

The types of ControllerManagers in k8s objects are

a. ReplicationController
b. ReplicaSet
c. DaemonSet
d. Deployment
     RollingUpdates Deployment strategy
     Blue/Green Deployment strategy
     Canary Deployment

e.StatefulSet

2. It help in desaster recovery


      NOTE

After you have configure or created your k8s manifest file, you can run a command that will tell you what would be create using the manifest file. Thi command is

kubectl -f apply DirectNameOfTheFile --dry-run=client

If your directoryName where the declarative yml file is created is manifest. Therefore

kubectl -f apply manifest/ --dry-run=client.

Note: video 128 is the same with video 127. Let dive straight to video 129

Video 129 KUBERNETES


AN APPLICATION THAT REQUIRES VOLUME

When you want to deploy an application that requires volume, it becomes important volume management is very important when it comes to k8s

Volumes:

Kubernetes Supports different types of volumes. These are

hostPath
nfs
aws Elastic Block Store
google Persistant disk
azure File
azure disk
persistant Volume
persistant Volume Claim

Let us look at some application we have deployed before

SPRINGBOOT-APP 

This springboot application has a platform where you inform are entered and these info need to be capyured and store in a database. 

springboot application requires a database, where user info need to be stored and can be retrived anytime. This is why it i called a stateless application

 is STATELESS Application. This application requires a database. This is why it is called a stateless application. 

We will be using a database called mongo

database -mongoDB = is a STATEFULL Application. A stateful application is a database application used in backing up and storing data. It must maintain its state 


Why is volume concept important in k8s?

Answer: It is important because when you deploy database as a container inside a pod, the database is meant to capture and store user data or info that comes in from another stateless application. when the stateless application container stopped running or is killed, you can retrieve back the info or data of the user from the database to keep the new stateless application or pods on track.  There would not be any lost of data because of the database container.   If the data is not backed up or stored, if there a restart, there could be lost of data

Let us go into prof github to see the repo for spring-boot-app with the link below. I have fucked this link also.

https://github.com/LandmakTechnology/spring-boot-docker 

The image of the spring-boot-application is in prof dockerHub which is below

https://hub.docker.com/repository/docker/mylandmarktech/spring-boot-mongo

The image of the mongo database-application which you can search, copy and pull from duckerhub image registry is 

https://hub.docker.com/_/mongo

Note above: the image is mongo. You will need to add the abosulte path to the imageName by copy the url. The url which you can found on the url bar upward anytime you are inside the imageName and then highlight it and copy it. That is why it is 

https://hub.docker.com/_/mongo

url is very important. It is like an address locator. URL = uniform resource locator



If we want to deploy a spring boot application, we also need to deploy database application. The twwo 

1. =##Spring Boot App

Deployment is done using a kubernetes object. Amongst the k8s objects, the best one is Deployment.

We should not use RS because we can not rollback old version if the need arises though its supports set-based and equality-based selector. We can rollout a new version or launch a new version of the application but with downtime. 

This is, you can move from version1 to version2 with downtime but you cannot move from version2 to versiobn1. You cannot rollback the old version. Since there also downtime, it is not good because we can not avoid to jeopardize high availabilty of our application. No application outage is required

We should not use pod directly because we can not scale if the need arises

We should not use RC because it does not support a set-based selector which is very cricial but only supports equality based selector (key:value)

  v1 ---> v2 with downtime
 RS cannot Rollback v2 ---> v1  


kubectl api-resources =  is used to check the api version of any k8s object that you want to use

Also know that if you did not define or pass any strategy to the application manifest, it will be deployed using the default strategy which is RollingUpdate

Anytime you see an ap1version that has apps/v1  It means that kind of k8s object having that api version is the one that supports set-base and equality-based selector.

According to springBoot code found in src, we have

mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}

This means that the name of the database that the spring boot application is to go with is mongodb

Let us make the userName = devdb

Let us make the userName = devdb@123

Let the hostName = mongo

Note: hostname is can also be called serviceName because he is the one that route traffic to the mongo database. If the serviceName is called mongo, it means the hostName would be mongo.

Note: We mentioned that serviceName can also serve as a DNS or domaine name service to the application.

====================== 
apiVersion: apps/v1     
kind: Deployment         
metadata: 
  name: myapp
  labels:
    app: fe
spec: 
  replicas: 2  
  selector:
    matchLabels:
      app: tesla
  template:
    metadata:
      name: app  
      labels:
        app: tesla
    spec:
      containers:
      - name: springapp 
        image: mylandmarktech/spring-boot-mongo  
        ports:
        - containerPort: 8080  
        env:
        - name: MONGO_DB_USERNAME
          value: devdb    
        - name: MONGO_DB_PASSWORDD   
          value: devdb@123 
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: tesla
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30000
  type: NodePort



2. Mongo Database

You caqn get the environmental variable or environment from the mongo office dockerhub by typing mongo on the search bottom of your dockerhub and click on mongo, click on mongo, scroll down and you will see where it says environment. In the form of below

    e- MONGO_INITDB_ROOT_USERNAME=mongoadmin e- MONGO_INITDB_ROOT_PASSWORD=secreet  

    which can be broken as'

   e- MONGO_INITDB_ROOT_USERNAME=mongoadmin \ 
   e- MONGO_INITDB_ROOT_PASSWORD=secreet mongo
  
 just copy MONGO_INITDB_ROOT_USERNAME and MONGO_INITDB_ROOT_PASSWORD and paste it where it ought to be in the manifest file.

 Note: The password has to be the same with the springboot app that you want to deploy

Since it is not our developer that has written the mongo data base, it is an intelectual property of mongo, we copied the image in dockerhub and so we can not make a more replica pods.  We would not define any number of replica pod, so that it will create the required default replica pod set which is going to be just 1 replicaPod. so we have to use a k8s object that cannot scale but supports set-based and equality-based selector. which is ReplicaSet

With spring boot application, it was written by our company called landmark and so we can write more version of the application if we need to just to upgrade our app. it can be from v1 to even v6 

Note: The default containerPort for mongodb and mongos instances is 27017

Note: Where volume comes in, is the fact that since we are going to be storing data in the database, a mountPoint sometimes called volume has to be created and mounted on the database pod so that pod would have enough resources to store data.

The default mountPath or volumePath or storagePath of MongoDB where data are stored in mongo is /data/db.

Let us also create a custom mountPoint or custom Volume where mongo db mountPoint or volume can synchronize data to, if need be. Let the mountPoint ba /tmp/mydata under path

This custom volume or mountPoint that you have created has to be mounted. In this case, it has to be mounted on the mongo default volume of mountPoint.

We want to deploy this database as a replicaSet

=##mongo
apiVersion: apps/v1 
kind: ReplicaSet
metadata:
  name: mongo  
spec:
  selector:
    matchLabels:
      app: db  
  template:
    metadata:
      name: mongo   
      labels:
        app: db   
    spec:
      volumes:
      - name: hostpathvol  
        hostPath:
          path: /tmp/mydata 
      containers:
      - name: mongdb   
        image: mongo    
        ports:
        - containerPort: 27017    
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb    
        - name: MONGO_INITDB_ROOT_PASSWORD   
          value: devdb@123 
        volumeMounts:
        - name: hostpathvol
          mountPath: /data/db  
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: db
  ports:
  - port: 27017
    targetPort: 27017

 When writing a manifest file, a list always starts with -

 FOR EXMAPLE

   ports:
  - port: 27017
    targetPort: 27017

     ports:
        - containerPort: 27017    
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb    
        - name: MONGO_INITDB_ROOT_PASSWORD   
          value: devdb@123 
        volumeMounts:
        - name: hostpathvol
          mountPath: /data/db 


ports and env  are not a list. they are a dictionary but any word that comes under them as shown above are a list and so they can start with  -

NOTE:  If you take note, you will discover the assigned service type is clusterIp on the service file codes. If nothing was passed to the service, it will still be clusterIp service.

We are dealing with database here. It is not good that user should be able to talk to database via its service directly since the database is linked with springBoot app. The service is meant for springBoot app to be able to talk to database because they are in the same k8s cluster.

The springBoot app service type was NortPort service because the app is for the externla users to be able to talk to the app via iits service. NodePort service is both for external and for internal.

Let us via into mongo.yml and paste the above into manifest file containing mongoDB and its service and then save-quit. run

kubectl apply -f mongo.yml

It anything is wrong with the manifest file, it will quote the line/lines where things are wrong. 

To see the line, vi into the file and type in

 :se nu

It number all the lines in you

Prof did encountered issue when deploying the application. It says line 12 was wrong.  That is why he had to number the lines in the file to know the particular line the issue was.

the line 12 seemed to be right but manifest file could not create a deploymentPod. 

Prof copied the written command that will create the pod and paste it in a yaml file validator online (by typing yaml validator in google) and click GO. it says, valid YAML

SOLUTION

Prof decided to separate the service command from the deploymentPod codes by creating a new file called mongosvc.yml for service codes and cut and paste service codes inside the file. It was now left deploymentPod codes

He then ran

kubectl apply -f mongo.yml

and the DeploymentPod was created

He also ran

kubectl apply -f mongosvc.yml

and the mongosvc.yml was created

To created the reploicaSet that will create pod, he via into  app.yml and paste only the deployment pod command inside the file and save quit. run

kubectl appy -f app.yml

When you run

kubectl get pod -o wide

The Deployment object created 2 replicaSet as request and the replicaSet created 2 pods

The 2 Deployment created always the same Which is the first name at the left.  You can always bane the Deployment Object by yourself

The 2 replicaSetName that was created by Deployment object is also the same name even though if they are in multiple but like an encrypted token and it always a the middle. It is the machine that name the replicaSet for you. That is why it is encrypted

The pods that was created by the replicaSet is always at the right which is also like a short encrpted token. the pods name are always different from each other. It is the machine that chose the pods name also.

In all of this, it is actually the pods that is needed but when you want to copy the pods, you must copy the absolute path to the pod. That is, you will copy both the deploymentName, deplicaSetName and the pods all together. like below

myapp-23yr5577fre6-7ygvrd


To Access The Aplicatiopn

when prof accessed the application by running

nodeIpaddress mapped with nodeportNumber.  that is 


curl -v nodeIpaddress:nodeportNumber

It was running. it means that the app would be accessible externally

When he pasted nodeIpaddress:nodeportNumber on the web browser. it was succesful. When he filled the form of the appli that deplayed and click nexr or save, it could not proceed, it showed error.

Her troubleshoot to know what the problem while its unable to save. He needed to check the environtal variable of both application is by running

kubectl describe pod podNamme

He did this for both applications


kubectl describe pod myapp-23yr5577fre6-7ygvrd

kubectl describe pod mongo-1gyr5577fre6-zygvrd

He discovered that the enviroment variable where password is, password was spelled wrongly.he had to edit the pod by running

kubectl edit deployment deploymentName

kubectl edit deployment myapp

      OR

You can correct the manifest file where the error is and correct it and save-quit and then re-run the application. That is 


kubectl apply -f  myapp.yml

Because you used APPLY, k8s will update or configure the pod running.

If you use the word CREATE, that is kubectl create -f  myapp.yml, it will create another pod and the first pod will remain un-updated or corrected. To manage space and to avoid overlap or confusion, use APPLY.

After the correction has been done on the pod, the springboot app was able to write on the app and then save on the database

The default volume or storagePath of the mongodb is called mounthpath which is = /data/db

The custom volume or custom storagePath of the mongodb is called hostPath which you can ofcourse create. This is why it is called custom mountPoint.

DIS-ADVANTAGE OF USING CREATED OR CUSTOM MOUNTPOINT LIKE TMP DIRECTOR FILE

The advantage is that is mongodb dies, the controlller Manager would automatically created another. The one created may be in another worker node because the schedular is the one that assign a node where the new one will be created. If another nountPoint is now created in a different node, the data in the volume or mountPoint of the first node would not be able to synchronize into the volume or mountPoint of the another node.

This is why we said it may lead to data storage inconsistency. This only happens when the custom replicaSet and mouthPoint is automatocally recreated in different node by the controllerManager (replicaSet)

Let us delete our pod by running

kubectl delete pod PathToThePodName

if you run

kubectl get pod 

You will see that another ReplicasetPod and a custom Mounthpoint that will replace the first one has been created. The repliSet pod may be created or schedule on a new node or on the same node

If the replicaSet and the mouthPoint volume is recreated on the same node, data would be consistent. That means, the custom mountPoint or volume mounted in the former mongdb that was deleted will synchronize into the mongodb that was created by the replicaSet. the data was saved in /tmp/mydata was not deleted and so when CM creatd another mongodb automatically, the  /tmp/mydata was mounted on the new mongodb.

If the controllerManger created another new replicaSet on the same node, let us kill the node by terminating the node in aws ec2-instance

After ytou have done that, if you run

kubectl get pod -o wide

You will see that the controllerManager created another pod and the schedular assign the pod to a another node.

If you refresh you springboot link that is in browser and filling in some info, you will see that data could not be saved. This is because the node that housed the custom mountPoint volume called hostpath vol where our data was stored  has been killed. This will definitley lead to lost of data. Another pod can be created in another new node and so another mountpoint is auntomatically replicated on the same new node with the new pod. data could not be retrieved. 

Whenever a node is down, the controllerManager like replicaSet has the capacity to create another new node to replace the dead node. If a pod is housed by the node, the replicaSet will ofcourse create another pod and the scheduler will assign the pod to any node that has enough resources to accommondate such pod.

This type of custom volume creation where a mountPoint in the tmp director is called a HostPath volume. HostPath vol does not result to PERSISTENT DATA (this is a data that is consistent. A data that can be retrieved even when the pod or node dies).

NOTE: The HostPath volume is like the bindMount volume in docker. It does not persists data. It lead to data inconsistency. data may be lost using them

HostPath volume lead to what is called data inconsistency

If our node is down, there is a software that would monitor and informed us if our node is down. That software is called APM (Performance Monetary) software like Promethius and Grafana


HOW TO CONFIGURE PERSISTENT VOLUME USING KUBERNETES

How to we fixed the issue of data inconsistency k8s?

1. Answer: We can fix the issue of data inconsistnecy by created an external volume that that data can be stored and will be able to synchronize should incase something goes wrong with our nodes or pods so that we will not loose our data. 

The creation of the external volume would be to create node or server that will have a volume or storage system like NFS or EFS which is very good for storage and synchronization.

CREATION OF A SERVER WITH AN NFS VOLUME.

The creation of this external NFS volume can be achieved by launching a server and create NFS volume or mountPoint on it.

The default mountPoint of NFS is /mnt/share

a. Once you create this mountpoint, your data would be stored externally outside the cluster. If any goes wrong inside the cluster, you data would be saved.

b. You would install NFS client or agent software in all the nodes inside your cluster.

  If we install NFS client software in all the nodes, data in the external NFS volume can synchronize into any nodes where you database is scheduled provided the node has the NFS client software installed. This why you should install the NFS client software in all the nodes inside the cluster because scheduler can schedule the database pod in any of the node in the cluster.


DataBase Deployment and the Creation Of aN eXTERNAL Volume called NFS in the Database Like Mongo

The File To Capture the Creation of a external Volume or MountPointis Like NFS in mongdoDB is below

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: mongodbvolume
         nfs:
           server: NFSServerIPAddress  
           path: /mnt/share
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodbvolume
           mountPath: /data/db


2. You can also be able to solve the problem of data inconsistency by the creation of PERSISTSNET VOLUME inside the k8s cluster. Like the creation of nfs,ebs,azurefile,azuredisk in a k8s cluster. 

Persistent Volume --> It's a piece of storage (hostPath,nfs,ebs,azurefile,azuredisk) in a k8s cluster. 

Note: persistent volume could be hostPath,nfs,ebs,azurefile,azuredisk

HOW TO MAKE YOUR STORAGE OR VOLUME A PERSISTENT VOLUME IN A CLUSTER.

Exanmple

If you have node1, node2, node3

node1 has a stateful application or pod which is a database application or pod  called mongoDB running

This mongoDB has a custom mountpoint or volume or a piece of starage that was created to store the data.

For volume that was mounted on the mongoDB pod to be persistent, you need to create another volume called PERSISTENMT VOLUME in the cluster that is not in the same node with mongoDB pod.

This Pv in the cluster would be manage and control by the k8s cluster called .kube   
Any of the node are not in control of the volume 

After you have created the PV

This persistentVolume has storage resources. Let say its storage size is 8GB of RAM

For the piece of storage that was created in databasePod to claim data or any storage size from the PV in the cluster, another volume called persisterVolumeClaim needs to be created (PVC) that will be attached to the mongDB pod Anytime databasePod needs a piece of staorage, it will claim it via that PVC from the PV

Just like in docker, If you want to create Persistent Volume, you run

kubectl create volume volumeName

Persistent Volume that you create exists independently from the pod life cycle which is consuming.

If you use PV, you will create a piece of staorage that does not depend on the life cycle of the pod


Persistent Volumes are provisioned in two ways, manually or Dynamically.

1) Static Volumes (Manual Provisionging)

    If you are being trained as a k8's Administrator, you will create a PV manullay so that pv's can be available for PODS which requires it.

  Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 

  PVC is like a mountPoint created between your PV and your stateful pod

 If you use kubeadm to install k8s cluster, you will mostly provision or create your persistent volume manaually 
  
2) Dynamic Volumes (Dynamic Provisioning)

This is mostly what is commonly used.

It's possible to have k8's provsion(Create) volumes(PV) as required., provided we have configured storageClass.

So when we create PVC even though we have not created PV, Storage Class will Create PV dynamically.
   
If you use kops/eks/aks to install k8s cluster, the provisioning or creation of persistent volume would be dynamic 
  

PVC = persistantVolumeClaim

If pod requires access to storage(PV), it will get an access using PVC. PVC will be attached to PV.


PersistentVolume – the low level representation of a storage volume.

PersistentVolumeClaim – the binding between a Pod and PersistentVolume.

Pod – a running container that will consume a PersistentVolume.

StorageClass – allows for dynamic provisioning of PersistentVolumes.

PV ACCESS MODE

PV Will have Access Modes as follows:

ReadWriteOnce – the volume can be mounted as read-write by a single node   hostPath 

ReadOnlyMany – the volume can be mounted read-only by many nodes           NFS 

ReadWriteMany – the volume can be mounted as read-write by many nodes      NFS  

In the CLI, the access modes are abbreviated to:

RWO - ReadWriteOnce
ROX - ReadOnlyMany
RWX - ReadWriteMany

Claim Policies:

This explains how we can claim data that is managed by persistent volume

A Persistent Volume can have several different claim policies associated with it including

RETAIN: When the claim is deleted, the volume remains. That is, if the PVC is deleted, the volume containing the data will be stored in PV. which can easily be retrieved automatically when another PVC is created

Recycle: When the claim is deleted the volume also remains but in a state where the data can be manually recovered. This means that when the PVC is deleted, the volume and its data inside are stored in PV and when another pvc is created, the data can manually be restored back or retrieved.

Delete: If the  claim (PVC) is deleted, the PV (The persistent volume) would be deleted. You will lost of all your data.

As an administrtor, the best claim policy that I will advise is the RETAIN POLICY

COMMANDS TO TAKE NOTE OF IN PERSISTENCE VOLUME

kubectl get pv
kubectl get pvc
kubectl get storageclass
kubectl describe pvc <pvcName>
kubectl describe pv <pvName>

NOTE: Our k8s cluster that we are using is a PRODUCTION READY CLUSTER. Remember that we installed KOPS in our system. 

KOPS = k8s operations

Let us found out what this k8s cluster came with upon installation by running

kubectl get sc

sc = storage class ---->  to know what storage class we have in this k8s cluster

kubectl get sc

It will show you all the storage class you have in the k8s cluster that you have installed.

Let us see, if we have PVC  and PV bu runnong

kubectl get pvc        also    kubectl get pv

You will see that we have have them

kubectl

Since we do not have pvc, pv, let us copy a manifest file from our prof github account which i have also forked that contains pvc, pv and other

Complete Manifest Where in single yml we defined Deployment & Service for SpringApp &
PVC(with default  StorageClass),ReplicaSet/StatefulSet & Service For Mongo.
storage class, pv, pvc, 
configMaps / Secretes

apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: mylandmarktech/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30030
  type: LoadBalancer

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodbpvc 
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 16Gi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: db
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: pvc
         persistentVolumeClaim:
           claimName: mongodbpvc     
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: pvc
           mountPath: /data/db   
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017

Let us delete the resource we have here before by running

kubectl delete all --all

all the pods, nodes,and others in our cluster will be deleted.

If you run

kubectl get all

You will see that only service called lusterIp which is the default IP of our k8s cluster is left.

Let us copy the entire script above and paste it in just one file and name it app1.yml

Note: When we launch install kops cluster using aws, a default load balancer was also created in aws. The load balancer created is for our API server

We also going to create another service which has a service type called load balancer as defined in our manifest file. This service will create another load balancer in our aws.

Let us run the command

kubectl apply -f app1.yml

It will create the resources listed above

If you run

kubectl get pod

you will see that all the pods are running



If you run

kubectl get svc

You will see that the loadBalncer service type is created. Though it will take some ninutes before it is fully initialize. it will be showing pending for some minutes.

If you go to your aws, under laod balancer, you will see that a new loadbalancer has been created.

NOTE: You can access the application externally by using 2 things

1. You can access the app by using

  nodeIpaddress:nodePortNumber


1. You can also access the app by using the load balancer url that was created.

You can get the link by running

kubectl get svc

the copy the load balance url and paste it in your browser

You will see that it worked. You can write and submit on it and it will be stored in database mongo.

NOTE: 

If you install KOPS cluster in your environemnt, you would have access to provision and utilized other aws services.

If we were using kubeadm, it would not have created loadBlancer in aws for us because kubadm cluster does not have the benefit of proviosning aws resources as of that of KOPS.

Load balancer created using Kops were not just created but it was well configured. That is, the listener, health check,  

The load balancer would be routing traffic to the nodes we have in our cluster. This means that external user can use any of the nodes mapped with the nodeport number to also access the application or pods apart from the load balancer url that was created.

If the below is the loadBalncer that was created, we cannot issue out such elb url to external user

a4e0df0fe996a4feb8b4c36924c1ab23-455473050.us-east-2.elb.amazonaws.com

All we need to do is to make more ideal by creating what is called a records or DNS in aws which we can actually get this DNS from route53 in aws

Once you are able to create DNS in route53. Let say you created a records called myapp.com

If you paste myapp.com in your browser, it will route traffic the traffic or request to the elb. That is

myapp.com --> a4e0df0fe996a4feb8b4c36924c1ab23-455473050.us-east-2.elb.amazonaws.com



CREATE a records using DNS service in aws [Route53]

HOW TO CREATE A-RECORDS

Go to route53 in aws

if you already have a hosted zone (if you dont have, create one), then click on hosted zone (prof hosted zone was dominionapps.net   let us use prof hosted zone), click on create records in order to create a-records, type in your recordName as maybe springapp, click on alias, select application and load balancer under route traffic to, select the AZ where you loadBalancer were created under choose region, select the load balancer URL that you have created (do not select the one with API) under choose load balancer, click on create records.

You will see that the a records has been created and the recordName would be

springapp.dominionapps.net

dominionapps.net is hosted or bought by prof, if you want to create an a-records using that hosted zone, it might not work for you. But you can try or create you own name. 

if you type this a-records in your browser, it will dispply the apaplication because this springapp.dominionapps.net will route traffic to the service which is the loadBalancer

a4e0df0fe996a4feb8b4c36924c1ab23-455473050.us-east-2.elb.amazonaws.com 

and this elb or service will route traffic to the springboot app

If the springboot app wants to talk to the external user, springboot app will inform the node route traffic to the service via ServiceIpaddress:ServiceNumber and the service will now route the traffic to the external traffic.

NOTE: In the above full manifest file, you will notice that there is a service in which the app can only talk to mongoDB and that servicetype is clusterIp. This is because database want to keep the communication between them an inner communication and not external. For security reasons. The app can only be able to talk to database via clusterIp serviceType 

Name:
  springapp.dominionapps.net
type: A records
      users can now use this records to access their application
value:  
 a4e0df0fe996a4feb8b4c36924c1ab23-455473050.us-east-2.elb.amazonaws.com


This entire infrastruction is a production grade deployment

Let us see other resource that was created as defined in our manifest file

kubectl get pvc     kubectl get pv

You will see that pv and pvc was created.

Remember that we did not define pv in our file but since we are using kops and we defined pvc in our file, PV will be created in a dynamic way. pv will be created automatically.

You can also see the volume created by going to your aws ec2-instance, click on volume, you will see that pvc volume that was created

You can see that KOPS cluster can provision and manage other aws resources. This is whay KOPS cluster is called PRODUCTION READY K8S CLUSTER

The above is the concept of persistent volume and loadBalancerService 

NOTE:

For Dynamically provisioned PersistentVolumes (that is, a pv that is created in cluster like KOPS), the default reclaim policy is "Delete". This means that a dynamically provisioned volume is automatically deleted when a user deletes the corresponding PersistentVolumeClaim

Note:

PV can only be automatically or dynamically provisioned when PVC is intentionally created


volumes concepts in kubernetes:

This is what we have been able to look at today summarily.

In summary,

kubernetes supports:
  
 1. hostPath: that is, when you mount the volume on the host. Thst is, if the volume is mounted on the node where database is hosted. This can lead to data inconsistency (may lead to lost of data) when the node is deleted, if another database is created an we have serveral nodes in the cluster where scheduler can actully assign the creation of another pod to. the database pod may now be created on a different node and then the volume created in the deleted node will be lost.

The disadvantage of this are, data lifecycle also depends on the PODS lifecycle, its lead to data inconsistency

 2. NFS/EFS ---> we can create an external nfs/efs server as a volume in aws where our data will synchronize. We must make sure that all our nodes in the cluster we have what is called nfs client are installed so that data synchronization can be possible. It gives us data consistency.

The disadvantage of this is also, data lifecycle also depends on the PODS lifecycle. This means that whenever the pods are dead, we are faced with the risk of also loosing the data

     
3. Persistent Volume = This will create a piece of storage or volume in our cluster which is manageed by .kube service or by k8s.   

PV IS Managed by the .kube service  

  PV / PVC / SC  

 SC ---> storage class, it comes with dynamic provisioning.

 Persistent Volume also has the below resources to store and persist our data in k8s

    hostPath 
    NFS/EFS
    awsBlockStore 
    azureDisk  


NOTE:

Containerization makes you application to be portable and light weight

If developers has written their code as below

    mongodb:
      host: db
      port: 27017
      username: devdb
      password: admin@1234 

It means that developers has hard-coded, That means that they would written a code that would not be re-usable or easli modified and that would have lead to code smell during code analysis using sonarcloud or sonarqube


WHAT IS HARD CODING

 Hardcoding is writing code that is not easily modified or reused. A hard coded information cannot be easily changed without changing the source code of the program itself.

mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}

If developer wrote a code in the form of above. This is called Dynamic way of coding. It codes cab be modified and re-usable

This is why developer should write a portable code that can be deployed in any environment seaminglessly

Using the lis below
   
            dev     qa       uat       prod  
hostname    mongo   mongodb   db        mydb  
username  devdb     qadb     watdb     proddb  
password  dev123    qa234    uat124    prod987

You can see that the above dynamic code can be incoporated into or used in different environment. Different environment can have different hostname, username and password


NOTE:

If you have a databse that is not hosted or found inside your k8s cluster but is in an external environemnt, your k8s cluster will be able to communicate with that database like RDS (sql database like mysql database) via a service called   ExternalName service.  It is part of the serviceType in k8s

NOTE: 

mongoDB is non-sql database it is writing json format. it is Non-RDS (RELATIONAL DATABASE SERVICE)

There are six database engines which RDS provides, and they are:

Amazon Aurora.
PostgreSQL.
MySQL.
MariaDB.
Oracle Database.
Microsoft SQL Server.

What are service types in Kubernetes?

They are

1. ClusterIP. Exposes a service which is only accessible from within the cluster.
2. NodePort. Exposes a service via a static port on each node's IP.
3. LoadBalancer. Exposes the service via the cloud provider's load balancer.
4. ExternalName.

cluster :
  10 applications:
        userMGT / CustomerService / moneyTransfer /      



Tesla  ---- BOA =    


QUESTION IN CLASS

For microservices, do all the applications in a microservice deployment is required to have a mountPoint or volume?

ANSWER:

Note all the applications in micro services that is required to have a mountPoint or volume. Only the application that has to do with database that would have a mountPoint or volume to be attached to tha application.

Examaple

SpringBootApp and MongDB deployment are two applications that were decoupled and would be deployed. That makes them a microservice.

It is only the MongoDb that is a database and so it is required that a volume or mountPoint can be created so that data can be stored there.

Another example are

When we deploy ES (elastic search), fileBeat and Kibanaare micoservice. it has 3 applications.

1. fileBeat is deployed as a Daemonset (it is a logs). fileBeat will gather the logs

2. fileBeat will talk to the ES via ES service by using clusterIp serviceType since they are in the same k8s cluster

3. Elastic Search is only the one that requires a database or volume or mountPoint because for all the logs that would be collected, they are going to be scrabbed or dumped in the elastic search. ES will store all the logs.

Elastic search will do data process of the logs and then it will do what is called indexing (meaning, it will classify the data)

4. Kibana will makes us to see what is happening. Kibana helps us to visualize the logs

5. For database to be able to talk to kibana since kibana is the channel to which user can visualized or have access to the logs, we would use a service called kibanaSVC. 

database can dispense the data to kibana for user visualization or for user to see.

the type of service that is would be recommeded for kibana should be nodePort service because external user need to also access the database via kibana.

INTERVIEW QUESTION

explain you experience in Kuberneetes?

Answer: Thank you very much for that question. My experience in k8s include, 

deploying applications in k8s cluster which I have been able to set up using kops. This is a production grade k8s cluster

I have been able to deploy both stateful and stateless applications using k8s

While deploying my stateful application, i made use of persistent volume concept

That Kops cluster will manage in our environment comes with dynamic storage class which makes the process of creating persistent volume very easy because when you create persistent volume, the persistent volume will automatically be created in a dynamic way.

With the use of persistant volume, you data will persist or be stored.

When we write our manifest file, we make sure that our stateless application can communicate and write data in the stateful application or database application

For the stateless app to be able to see or discover the database app, we need a service which can expose the database app to the stateless application. 

For the stateless application to be able to write and store data in the database app, you need to pass the right environmental variables.

VIDEO 131  K8S

Stateful applications always required databases or volumes or mountPoint. They are mostly database applications like mongoDb

Workload deployment in kubenetes is done using
   kubernetes objects:

Deeployment is the recommended Kubernetes object used to 
deploy stateless applications in Kubernetes in our ENVs

we use ReplicaSet in our environment to deploy 
STATEFULL Application including PV/PVC & storageclasses.  This meas that we create or intergrate volumes with the replicaSet

StatefulSet is the recommended Kubernetes object used to 
deploy statefull applications in Kubernetes in our ENVs. The statefulSet usually comes with volume by default.


LET US DEPLOY A NODEJS APPLICATION

NodeJS web application
======================

https://github.com/LandmakTechnology/nodejs-application

I have forked the about prof github url into my github

the nodeJS app has be written, built and pushed to dockerHub

DockerHub = mylandmarktech/nodejs-fe-app

Above is prof dockerHub repository

Above is prof docker image

mylandmarktech/nodejs-fe-app:28

Let us deploy this app using pod as the k8 object.

Always go to your dockerfile and see what is the container port number. It is under the key word EXPOSE.

you can also find it on your dockerHub account If the image is already pushed to your dockerHub account, you can click on the image tag, scroll down and you will see where its say EXPOSE. They numbers on the EXPOSE is the container number that developers has assigned to the application.

NOTE

If you want to separate commands of applications that is in the same manifest file, we use 3 dashes after the first application command and the before another application commands as indiccated belowl

apiVersion: v1
kind: Pod  
metadata:
  name: nodeapp   
  labels:
    app: node  
spec:
  containers: 
  - name: node  
    image: mylandmarktech/nodejs-fe-app:28 
    ports: 
    - containerPort: 9981  
  imagePullSecrets:
    - name: dockerhubcred
  volumes:  
  podAffinity:   
---
apiVersion: v1 
kind: Service  
metadata:  
  name: nodesvc
spec:
  type: NodePort  
  selector:
    app: node  
  ports:
  - port: 80  
    targetPort: 9981  


Let us copy and paste in  (vi node.yml) and then save-quit

Let us create the 2 pods above by running

kubectl apply -f node.yml

To verify if pods are running

If you run 

kubectl get pod -o wide

after the above command, Prof discovered that the pods was not running. He troubleshooted to resolve it.

You can run the belwo command when troubleshooting

kubectl get pod

kubectl get event

kubectl describe pod podName

ubectl logs pod podName

kubectl exec pod podName

After the troubleshooting, it was discoverd that it was unable to pull image from image registry. access denied.

It needd authentication to be able to pull image from a private image registry.

For you to be authenticatd, you need to add or define what is called imagePullSecret to the manifest file.

For the other deployment that we have been doing in k8s, we have been pulling image from a public image registry. Like mongoDB, we pulled mongo image from dockerHub image registry

pulling docker iamges from private repository/registry requires authentication. That is
  username and password/token  

Example of other private image registry: are

    dockerhub 
    ECR  (elastic container registry powered by aws)
    nexus
    Jfrog


CREATION OF SECRETE IN DOCKER FOR AUTHENTICATION

The command to create secrete in dockerHub for the purpose of authentication are

kubectl create secret docker-registry credentIalNAME --docker-server=docker.io \  --docker-username=NAME --docker-password=PASSWORD


kubectl create secret docker-registry dockerhubcred --docker-server=docker.io \ --docker-username=mylandmarktech --docker-password=Mercy000014

To know if the secret has been created, you run

kubectl get secrete  or kubectl get secrete -o wide

kubectl get secrete -o yaml

You will see that the secrete will created is been encrypted. the username and password has ben encrypted. Difficult to memorized. This is for security reasons. it is now like a token.

After he created the secrete, he passed a secretePull on the file and entered the secrete name as dockerhubcred run the command again

kubectl apply -f node.yml

the pod was now running.

He also ckech the logs of the running pod by runnning

kubectl logs podName

Th logs stated that the pod is running and show the url where the pods are running. in the form of


http://13.59.8.163:31500/landmarktechnologies

If you want to verify the url he display above, you runm

curl -v 13.59.8.163/landmarktechnologies

HOW DO WE ACCESS THE APPLICATION


If you get into your nodejs application written by developers, you can go to where the code where written.

Inside the code, you will see where it says app.get (*/landmarktechnologies)

ou will also see below where it says app.get (*/html), app.get (*/jsonData) in the code or script

These are called restful API

if you want to access them online, you copy and paste on the web browser in form of

 http://nodeIpAddress:nodePortNumber/WordStyle       like   

 http://13.59.8.163:31500/landmarktechnologies

  http://18.119.143.159:31000/jsonData

  http:18.119.143.159:31000/html 

Examples of Restful APIs are

RestfulAPIs  
 
    http://18.119.143.159:31000/landmarktechnologies
    18.119.143.159:31000/landmarktechnologies
    18.119.143.159:31000/jsonData
    18.119.143.159:31000/html 
    18.119.143.159:31000/queryparam
    18.119.143.159:31000/queryparam
    18.119.143.159:31000/redirect

When to use RESTful API?
The most common scenario of using REST APIs is to deliver static resource representations in XML or JSON. However, this architectural style allows users to download and run code in the form of Java applets or scripts (such as JavaScript).


Secret and configMaps
=====================
Qualities of a good software/codes
==================================
1. portable: This means, we must be able to use the code in any environment.

2. The code should be re-usable. For a code to be re-usable, write the code using Environment variables.   

configMaps =  
   1. Are used to store data in key value form  [database: hostname: devdb] 
   2. generally stores  non confidencial data in kubenetes [hostname, username] 
   3. Data is store in clear text  
   4. It captures configurations that weren't hardcoded in the dockerfile or by developers 

Secrets = 
   1. Are used to store data in key value form  [database: hostname: devdb] 
   2. generally stores confidencial data in kubenetes [passwords, ssh-keys, tokens] 
   3. Data is store in encrypted form using secrete 


Developers as a best practice should write:

  portable codes

  The shouldn't hard code

  Dockerfile with username below

    db-username: admin  

    above is called HARD-CODE

  Dockerfile with username, hostanem, password written in form of  below

    db-username: ${username}
    db-hostname: ${hostname}
    db-password: ${password}   

Above is a good code. which can be re-usable.

Duckerfile is used to create images. After images are created by docker, it would be push to image registry.

container orchestrators like kubernest and dockerswarm will manage image by pulling the image from image registry and then create a container inside a pod.

Container Orchestration:

BEST PRACTISE FOR DEPLOYMENT OF APPLICATION

    containers      images                             k8s object recommended:\
1.   mongodb        mongo                              RS/RC/SS  
2.   springapp      mylandmarktech/spring-boot-mongo   Deployment  

Environments       Dev     stage        prod           K8s object
hostname            mongo   mongo2       mongdb          configMaps
username            devdb   stagedb      admindb         configMaps
password          devdb@123    admin      admin@123      secrets/configMaps 

CconfigMaps is part of k8s object. It is used to store non confidential data

k8s objects

pods.
Namespaces.
ReplicationController (Manages Pods)
DeploymentController (Manages Pods)
StatefulSets.
DaemonSets.
Services.
ConfigMaps

For our database to be completely deployed and exposed, we need 6 Kubernetes Objects. These are below

ReplicaSet, PVC,  PV, configMap and Secret and database ClusterIP service 


configMap manifest file

Let us see how we can create a configMap

In deploying with configMaps, you replace spec with data. there is no spec in configMap

configmap.yml

apiVersion: v1    
kind: configMap  
metadata:
  name: mongo-configmap     
data:  
  db-username: uatdb   
  db-hostname: mongodb  
  db-password: uatdb@123   


Secret Manifesr File

In deploying with in secret, you replace spec with data. there is no spec in secrete alos.

The password for secrete has to be encrypted. We cannot enter the password regularly the way we entered it in configMap because for secrete, the password is confidential.

Yo encrept a password, we use what is called BASE64

We run the below command to encrypt a password

#echo -n 'password' | base64

if the password that you want to put is uatdb@123  then you run the command

echo -n 'uatdb@123' | base64

That will encrypt the password uatdb@123

secret.yml
=========
apiVersion: v1 
kind: Secret
metadata:
  name: mongodb-password    
Opaque:
data:
  db-password: dWF0ZGJAMTIz  

From the above, it is recommended that we should use secret instead of configMap in deploying application with confidential data.

Let us re-deploy our spribgboot application we did beore by inputing secret and configmaps as shown below:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: mylandmarktech/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          valueFrom:
            configMapKeyRef:
              name: mongo-configmap
              key: db-username
        - name: MONGO_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-password
              key: db-password
        - name: MONGO_DB_HOSTNAME
          valueFrom:
            configMapKeyRef:
              name: mongo-configmap
              key: db-hostname
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: db
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: pvc
         persistentVolumeClaim:
           claimName: mongodbpvc
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           valueFrom:
             configMapKeyRef:
               name: mongo-configmap
               key: db-username
         - name: MONGO_INITDB_ROOT_PASSWORD
           valueFrom:
             secretKeyRef:
               name: mongodb-password
               key: db-password
         volumeMounts:
         - name: pvc
           mountPath: /data/db

copy and paste in secp.yml and then save-quit

run  

kubectl apply -f secp.yml

Prof encountered issue. He was unable to create configMap because he used lower care c for confirgMap instaed of upper case C in the manaifest file and he resolved it.

He found out when he troubleshoot with

kubectl describe pod podName

If also did

kubectl api-resources | grep configmap

he found out the configmap should be written on the manifest file as ConfigMap  under kind

For you to check if the secret and configMap was created run

kubectl get secret

kubectl get cm                                       cm = configMap


Explain CreateContainerConfigError in Kubernetes??

This is an error that occur during the creation of a container which makes the contain not to be up and running because it is waiting for some configuration of the yml file that must have been done in an error.

He was still unbale to create configMap. He spent so much time troubleshooting but never succeeded.



HELM

We can use Helm to generate all the manifest files

He you run the command

helm create

It will create all your manifest file for you

You may be required to deploy an application that is about 200 lines and above. You do not want tyo write 200 lines of code. Helm will write it for you. Helm will write everything that you need.
  
NEXT THING WE WOULD TALK ABOUT IS AUTOSCALING 

Auto scaling= Horrizontal Pod AutoScaler(HPA)/ vertical Pod AutoScaler(VPA)/ cluster autoScaler (CAS)  

For that to work effectivedly, we need that would be use t measure resource that would be assign to each pods that would be created.

RESOURCE MANAGEMENT

Each pods are consuming resources.

1. We would talk about how much REQUEST of resources can be made and what is the limit of request you can make for a pod

2. We will also look at resourceQuota. This is how much resource will can assign to Namespace while deploying an application

For example.

REQUEST: An application can request for a CPU size of 500Mi and Mem size of 200Mi.

  Mi= milicore      500Mi = 500Millicores

This means that before the application or pod is assigned to any node by the scheduler, the node should have up to or more than the above resources in terms of size

LIMIT: under limit, the minimum resource it can get for CPU size of 500Mi and Mem size of 200Mi but the pods need more resources, the maximum limit it cna get is CPU size of 800Mi and Mem size of 500Mi. This means also means that the node should have up to this amount of resources for CPU and memory.


When creating a pod, using pod as a k8s object. you can have manifest file configured in the form of

apiVersion: v1  
kind  : Pod
metadata:
  name: app    
spec:
  containers:
  - name: hello  
    image: mylandmarktech/hello  
    ports:
    - containerPort: 8080      
    resources:
      requests:
        cpu: "500Mi" 
        mem: "200Mi"  
    limits:
      cpu: "800Mi" 
      mem: "400Mi"  

You vscode can also will be able to arrange the manifest file if you type in pod, you will see how he is able to arrange were the resource will look like, you will have request, memory, CPU and limit

Let us assume that all the nodes in your cluster is 13 nodes

Let us assume The 13 nodes has CPU=100000MB  and  MEM= 40000MB  in total


80% of 13 nodes of CPU = 80000MB  and 80% of MEM = 32000MB  

QUESTION

IF a pod can consume up to 800MB of CPU and 500MB of memory, how many pods or replicas can we create with 80000MB of CPU and 40000MB of memory

You can say 80000BM divide by 800BM = 100

You can say 40000BM divide by 500BM = 80

 ANSSWR

This means that he can create up to 100 pods of CPU and 80 pods in memory

        CPU       MEM  
PODS = 100 PODS / 80PODS    

This means that our cluster capacity can be able to hold 80 pods. The memory is what you go with.

Since our cluster can only handle 80 pods, we can say that

 HPA:
  min pod: we want 40 pods to be running  
  max pod: we want 70 pods to be running, so that we dont run full capacity. our full is 80

  If there is a spike in use traffic, the autoscaler will just be creating more pods and if the suer traffic is cooling down, it will also reduce the number of pods in the cluster. this is horrizontal pod autoscaler

There is a software that would be use to measure pod parameters to know whether we need to add pod or reduce pod. That measuring instrument is a k8s operator called Metric Server

kubernetes operator called = metric server  

IQ  YOU may be asked questions about what we did today in k8s

Answwer: In our environment, security is inherent. Developers are always advised to not HARD CODE. They are advised to always write a portable applications. Which means that they should write application that can be deployed in different environment and the application should be re-usable or can be modified. Most times, the only think that would be modified are environmental variables by using configmaps/secrets to define our environmental variable.

ConfigMap/secret are additional configurations that were not defined in our Dockerfile.

The ConfigMap are used to pass non-confidential data

If we want to pass password, we use secret which which usully looks like token.

You can create secret from you password. There a command that is used to encrypt your passord to now look like a token. That encrypted passwrod is called secret. The command is below

echo -n 'passwordName' | base64

Note:

We can also use ConfiMap for volumes


VIDEO 133

kubernetes11, Rancher, Helm & nginx-ingress  Nov 12, 2022

We said that we are going to be deploying k8s using CLI and UI. 

Let us use UI to deploy k8s. We are going to be using Rancher as a UI for deploying k8s

RANCHER AND KUBERNETES

Rancher is a k8s platform that is used to import and manage multiple k8s cluster

Rancher can also be used to provision k8s cluster like EKS, AKS. GKE 

Rancher provide a dashboard for k8s.

Rancher is deployed using docker

HOW TO DEPLOY RANCHER IN K8S

You can deploy rancher using the below command

sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher

Once you run the above command in your docker environment, you rancher container will be deployed.

To ACCESS RANCHER ON THE BROWSER.

1. Since we want to use rancher as the ui platform to be able to manage our k6s cluster. Let us access the rancher on the browser by copying only the ip address of the rancher environment. The ip address of ther server that rancher was created.

2. paste the ip address on web browser. It will say WARNING. POTENTIAL SECURITY RISK AHEAD. jus click on advance, click on ACCEPT THE RISK AND CONTINUE, it is now asking for password. it gave us the command to run in docker environment that we can be able to get the rancher password from. the command was 

docker logs container-id 2>&1 | grep "Bootstrap Password:"

We are deploying rancher using docker

To know you container-id, run

docker ps -q

if the container-id is q12ggret5457

copy the container-id and paste on the command above

docker logs q12ggret5457 2>&1 | grep "Bootstrap Password:"

It will give you an encrypted password like token

paste this password in the place where it is requesting for password in your rancher web browser, then click LOGIN WITH LOCAL USER.

It will say welcome to RANCHER, it will tell you to create new passsord, make sure that you password is up to twelve chracters. once you do that, it will be inside racher.

With provisioning of this rancher, a local k8s cluster is created by default. Rancher comes with a local cluster

You can also import and create k8s clusters.

Rancher has what is called kubectl shell. If you click on cluster, kubectl shell sign or logo is at the top right of the page. it is like > sign.

If you click on the kubectl shell, it will take you to a command line indoe the rancher.

You can run any k8s command there.

If you run

kubectl get pod

It will say pod not found. This is because we have not created any pod here. If you run 

kubectl get node -o wide

It will show you the local node was created by default as you start up rancher in kubernetes. just i node. it is running in container run time called containerd. That node is our controlPlain or master.

Inside the rancher, you can also import a yaml file

Whatever task you want to do in k8s, it can be done here.

POD creation in rancher

We are trying to create a pod using UI in rancher.

If you want to create a pod in with k8s objetc like pod in rancher, click on pod, click on create, enter podName on where it says name like appnew, select the namespace or you can leave it at default, select STANDARD, enter your ImageName (some like mylandmarktech/hello), select your pullSecret in you have, on PullSecret, click on add ports, select serviceType (like NodePort or ClusterIP or LoadBalancer or ExternalName. let us select NodePort), enter the serviceName (appsvc), enter private container port number which is service port number is 80. then click create

You will see that the pod will be created and it will be running.

To check if your pod is running. click on workload, then click on pod, you will see that the pod is running.

If you click on service, you will also see that your service pod is running.

If you also want to check if the pod and service is running using CLI, you can click on kubectl shell, and the run

kubectl get pod

kubectl get svc -o wide

kubectl get all --all

if you run kubectl get svc -o wide   

you will notice that it created both nodeport service and also clusterIP service

IMPORTING OR CREATION OF CLUSTER IN RANCHER

If you click HOME, click on IMPORT EXISTING or click CREAT

if you click IMPORT EXISTING you can import a cluster from anywhere. No matter the type of cluster that was installed. Be it kubeadm or kops cluster or a managed cluster like AKS, EKS, GKE.

I you click on CREATE, you can create a new cluster in rancher UI platform. click on CREATE, cancel the CLI with the x sign. It will show you different type of cluster you wish to create. 

k8s cluster like 

amazon-EKS. This is a managed k8s cluster, powered and managed by aws

amazon-ec2. This is Self Managed k8s cluster where you can install k8s like kops cluster 

Google-GKE. this is a managed k8s cluster, powered and managed by google

Azure-AKS. his is a managed k8s cluster, powered and managed by microsoft azure

Azure-server. This is Self Managed k8s cluster where you can install k8s on azure virtual machine.

digitlaOcean

and so many others


if you click on IMPORT, you will be able to import an existing cluster of a managed and self managed cluster.

If you click on the import, you can click on any of the existing managed cluster like aks, eks, Gke that are already existing or you can click on GENRIC.

if you click GENERIC, you will be able to import an existing self managed cluster like the kops cluster we installed before. If the kops cluster is still up and running, we can as well import the kops cluster to this rancher platform. 

HOW TO IMPORT GENERIC KOPS CLUSTER

1. Go to your kops cluster server and make sure that the kops cluster is up and running. If the kops cluster is not running. If you have already installed the kops cluster on the ec2-instance, start up the kops ec2-instance master and wokernodes on aws then ssh into your mobaXterm with the kops master Ip address, Try and connect the kops by switch user to kops

sudo su - kops

and then to connect to kops, run

ssh -i .ssh/id_rsa unbuntu@kopsMasterIpaddress

Once you run the above command, it take you inside the kops clsuter

.ssh/id_rsa   is the sshkey or token to get authenticated to connect to the cluster

You can cat .ssh/id_rsa to see what inside by running

cat .ssh/id_rsa

You will see that it is an encrypted ssh key.

2. Make sure that the kops you are about to import to Rancher has not been imported the rancher before now. 

If it has been, you need to first of all delete the namespaces that was created as a result of the importation. 

If you run 

kubectl get ns 

if you see something like cattle-fleet-system, cattle-impersonation-system, cattle-system

It means that this kops cluster has been imported to rancher before. 

In order to import the kops system to the same rancher, you need to delete the namespaces that was created as a result of the kops importation to rancger by running

kubectl delete ns cattle-fleet-system, cattle-impersonation-system, cattle-system

It would delete but it will take about 20 minutes or even more to have it deleted. Prof try to delete but it was taken to long and so someone in the class gave prof a command that can be used to forcely delete the namespaces.

3. click on IMPORT, write kops19 under clusterName, write production cluster under descriptio, click on CREATE

it will display various command that you can use to import kops to rancher. Please copy the first coammnd and paste it in your kops environment in mobalxterm and then press enter key. if it is not successful, you can try the second command and then third command in your rancher.

According to prof importation, the first command displayed by rancher did not work. It was the second command that worked. and the kops cluster was successfully imported to rancher platform.

If you click on EXPLORE, it will show you the cluster dashboard. we can run workload on this cluster.

If you click on workload, you will see all the deployment that we have had before.

You can delete all the running ressources you have in your kops cluster before my running

kubectl delete all --all

Go to your rancher platform and try to create a deployment

1. Deployment Using Rancher By Uploading or copy/paste a yml file

Using the spring-boot-app and service manifest file or yml file that we have deployed before. Please copy it above under Deployment k8s object.

click deployment, click on create, upload a yaml file (manifest file) or copy the one you have in your kops server and paste inside the rancher by clicking on upload yaml file, paste the yaml file content inside the box, and then click on import. After few minutes, the deployment will be successful.

if you click on Deployment, it will say 2 replcaSet pods has been created. The two pods here is the springbootapp and it service

2. Deployment Using Rancher by Manual Creation

click on Deployment, click on CREATE, write webapps as DeploymentName, select 3 replica under replicas, select STANDARD CONTAINER, enter you ImageName as mylandmarktech/hello, click on containerPorts, select NodePort as serviceType, enter webappsvc as the name of the service, let th containerPort be 80 and then click on create.

You will see that 3 pods are created and they are up and running. if you click on pods, you will see all the pods that you have created so far.

If you want to access the application, you copy and paste the below on your web browser

workerNodeipAdress:NodePortNumber

It will deplay the application

RANCHER can also helps us to benefit RBAC (role based access control) to determine who can access our cluster..

Click on home on the dashboard, click on kops19 that you have imported, you will see that it tells you how much of the cluster resources or capcity it has used. it will tell you how much momery it has consumed. you can see if anything is wrong with rancher here. It has monitory by default

With rancher, we can also create members

You can decide that you want to have access to your rancher platform from the command line. For you to do that, you will need to download kube config file. kubeConFIGfILE sign is at the top right that looks like copy sign.

Let us download kubeConfig file by clicking on the kubeConfig and the kubeConfig will download.

Example

Maybe a company just hired you and sent you a url in the form of 

https://32.12.366.09 

that we take you to rancher platform. And you are given

username: admin
password: admin@123class30

Once you have gain access the the rancher platform and you want to have access via command line,

1. first of all download the kubeConfig by clicking the kubeConfig sign that looks like copy sign file inside the rancher platform, copy the downloaded kubeConfig

2. go to your kops server and Exit yourself from kops master, and then create a directory called .kube by running

mkdir .kube

vi .kube/config

in the above command, it means you are creating a file called config inside .kube directory

paste the kubeConfig file that you dowmloaded from rancher into the file you just created in your server and then save-quit

Then you can run

kubectl get all

You will see that you can access your rancher cluster via command line. 

Note: you exited yourself from kops master in kops server. It is rancher cluster you in now after running the above command.

SECURITY QUESTION

It is better to manage your k8s cluster via the k8s masterNode or manage your k8s cluster out the master by using another node?

Answer: it is more secured to manage your k8s cluster outside your master node.

It is possible to delet the resources in our rancher via CLI in k8s ubuntu server?

Yes. we can do in the directory where rancher kubeConfig file is saved. If you are not inside that directory, any k8s coomand will not work because you exited master as well.

let us delete webapp that we created via Deployment in rancher via UI by running

kubectl delete deployment deploymentName

kubectl delete deployment webapp

It would be deleted if you verify via CLI by running

kubectl get deployment or kubectl get deploy

If you also go to rancher platform and click deploy, you will see that webapp deployment is no longer there. All the 3 pods that was deployed under webapp are all deleted.

You now manage your rancher from the command line or from the UI.

NOTE:

Above implies that, for you to manage and make an API call externally or out of you masterNode in k8s cluster, your external server must have the below

1. kubectl  = This is the command line utility for k8s.

       This is what enables you to be able to make API call or run a k8s command 
       You must install kubectl for you to be able to manage cluster 

2. You must save .kube/config file in external server. it is .kube/config file is what authenticate you`


HELM AND HELM CHART IN KUBERNETES

HELM is a package manager for k8s

Just like YUM, APT, PYTHON-PIPE are package manager for linux, ubuntu and python server respectively

We can deploy workloadS in k8s easily using HELM

WHY WE USE HELM

K8S can be difficult to manage with all its object like ConfifMaps, secretes, pods, services etc that you need to maintain but HELM can manage all this for you. 

k8s as a powerful instrument for container or containerized application orchestrator can be complex. Helm will simply all the whole complex process in k8s orchestration.

HELM simplifies the process of creating, managing and deploying applications using HELM CHART.

HELM also maintain a version history of every chart or application of installation and if anything goes wrong, Helm can call HELM-ROLLBACK and if you want to launch a new version of the chart or application, helm can call helm-upgrade

WHAT IS HELM CHART?

Helm Chart is a collection of manifest files organized in a specific directory structure that describe a related k8s resources.

There are two main component to a chart. These are TEMPLATES AND VALUES

These TEMPLATES AND VALUES goes through a TEMPLATE REDERING ENGINE to produce a manifest that can easily digestible by k8s

HELM uses chart to assemble or pack all the k8s component to deploy, run and scale an application.

Normally if you want to deploy an application like spring-boot-app using Deployment object

You will create

PV, PVC, ConfigMaps, Secretes, service, HPA which all we have to be on your manifest file.

Using helm, helm can create all of that for you.

HELM INSTALLATION

kops:  
eks:  

Helm
====
  - CLI = helm
      helm install     
  - Engine 
      helm create springapp  
  - Charts 
      helps to store manifest files in helm repository
      helm repo add nginx https://helm.nginx.com/stable  
      helm repo add springapp https://landmaktechnology.github.io/helm-app23/      
     // git remote add  https://helm.nginx.com/stable  

Install Helm 3:

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
 ./get_helm.sh

Generally we have:
  custom applications  
  third party applications

Deploying Nginx ingress using helm in kubenetes:

Validate helm installation by executing the helm command.
# vi /etc/sudoers
$ helm

Hub.helm.sh

Now, add the public stable helm repo for installing the stable charts.

$ helm repo add nginx-stable https://helm.nginx.com/stable

helm repo add nginx https://helm.nginx.com/stable

$ helm repo update

Let’s install a stable nginx chart and test the setup.

$ helm install nginx nginx-stable/nginx-ingress  
$ helm install <deploymentName> <chartName>
List the installed helm chart
$ helm ls
----====

Let us install helm 3 in our cluster (on the rancher cluster that we are accessing via CLI in kops server).

Helm 3 architecture does not require an agent. You can run helm command inside helm server

Helm 2 architecture require an agent before you can run helm command inside helm server

Please copy helm 3 lines installation command from prof github or running note and then create a script/file

vi helm.sh

and paste the helm installation command into it and save-quit.  and then run

sh helm.sh

Helm will be installed

Let us create an application called javawebapp by running

helm create javawebapp

The javawebapp will be created.

If you run

ls

You will see that javawebapp directory is there. 

cd into javawebapp

You will see  chart.yml, values, templates directory and charts directory.

You can be able to run some command in helm

1. helm show values javawebapp

2. helm show template javawebapp

If you run 

helm show template javawebapp

It will show you the manifest file for which the application was deployed. You will see that it deployed just one replicas, just 1 pod, it will determine the type of service it used, etc, it will use Deployment as it k8s object and so on

You can modify the manifest file if there are some values on the template that you do not want.

If you run 

helm show value javawebapp

It will tell you the values like replicaCount, the type of image that would be used. It will kind of confirmed the template of the deployment. It give details of what the deployment is.

When you run helm create, any deployment template or configuration that you see is by default and you can be able to modify the template to suit your purpose.

MODIFICATION OF THE VALUES

Let us modify the helm values. Let us redirect the helm values to another file called values.yml by running

helm show value javawebapp > values.yml


It will be redirected to that file. You can confirm by 

ls

You will see that it is there

vi values.yml

Let us change replicaCount = 3, image= mylandmarktech/java-web-app, put in the tag number if the image has a tag or version number, tag = 2 in prof dockerHub, If you dont want a serviceAccount then change it to false, serviceType to NodePort. Save and quit

NOTE: All the files in template directory, their configuration inside the files are in varaiables. Which means that they are be modified. They will not hard coded. This is whil you can actually modified files.

Since you have created values.yml   let us create a pod from the file by running

helm install DeploymentName helmAppplicationDirectory -f modifiedValueFileName

or

helm install DeploymentName helmAppplicationDirectory -f modifiedValueFileName



helm install app2 javawebapp -f values.yml


when prof verify the pod by running

kubectl get deploy

kubectl get pod

He discover that the pod were not created.

TROUBLESHOOTING

He troubleshoot by via into template/deployment.yml and template/deployment.yml and discover that the containerPort in template/deployment.yml has http as port Number and the TargetPort in template/service.yml also has http as port Number.

But the image that was defined or pass for the deployment of the pod has a containerNumber of 8080. Prof now went back to values.yml he modified and changed the image from mylandmarktech/java-web-app to mylandmarktech/hello with tag=2 because mylandmarktech/hello with tag =2  has containerPort as 80

NOTE

The Deployment default value for containerPort and service default for targetPort  came with 80. If you containerPort which is the expose port of your image is not 80. 

You will need to either change THE Deployment default value for containerPort and service default for targetPort to match with the ExposePort in image or change the image EXPOSE port or use another image with the required port.

When he deployed the application once again by ruuning

helm upgrade app2 javawebapp -f values.yml

NOTE: The reason why we use helm upgrade in the above command is because helm upgrade will modified the helm install that was used before. That means, he will update or modifiy the deployment if you want the deployment to be modified.

If you now run


kubectl get deploy

kubectl get pod

You will see that 3 pods are created.

If you now run


kubectl get ep

You will see that the 3 pods are mapped with the service port. This means that 3 pods are connect to the service via it port Number.

endpoint = podIpaddress:servicePort

This is what helm chart is all about.

NOTE

Just as we use Github as SCM for collaboration with developers. You can do the below for helm


1. create custom helm chart for application deployment


2. push helm charts into helm repositories in github pages, helm.io and etc

3. update existing helm charts into helm repositories

4. use helm charts for easy deployment within a team

HELM REPOSITORY

helm repo is below

https://github.com/Landmarktechnology/helm-app23

I have just forked the above heml repo from prof github into my github

helm github pages from prof is below

https://landmarktechnology.github.io/helm-app23

NOTE: There is difference between github pages and github repository

You can only be able to clone github pages using helm command into helm environment

If you want to know if you have any helm repo in your environment, that is why you not in your master, remember you install helm in rancher cluster been accessed in another environemt. if  you run

helm repo ls

You will see if you have helm repo or not.

If you ssh back to your k8s master by the coomand

ssh -i .ssh/id_rsa ubuntu@MasterIpadress

if you run

helm ls

You will see the helm chart that we deployed earlier today with 2 revision meaning 2 times we did the deployment. 

If you want to delete or unstall helm deployment that you have, you run

helm uninstall deploymentName

helm uninstall app2

If you run 

helm repo ls

you may not have any helm repo before. If you have, you may decide to add to the repo by running

helm repo add aliasName repoPageURL

If the repo page is https://landmarktechnology.github.io/helm-app23  and the aliasname= helm-app

helm repo add helm-app https://landmarktechnology.github.io/helm-app23

After you have cloned the repo.

Update the manifest file to make sure that if there is any changes latest version in terms of software or dependencies, it will be update. by running

help repo update

it will update all the helm repo you have

If you run

helm repo ls

you will see all the repo

For us to see the helm chart or values that we have on this new heml repo we cloned, we run

helm search repo aliasName

helm search repo helm-app

It will show you the helm chart, the version, the name of the chart. name of the chart would be 

aliasName/aliasName     that is how the chartName is

helm-app/helm-app

If you want to see values to know the configuration of the manifest file in the cloned repo, run

helm show values chartName 

helm show values aliasName/aliasName

helm show values helm-app/helm-app

You will see the image is spring-boot-mongo, number of replicas, version or tag is v3, service is loadBalancer and others that it has.

Let see if we can change anything by running

helm show template helm-app/helm-app

the template shows that the containerPort is 8080, servicePort is 80, the is liveliness probe, it has Horizontal autoscaler etc

We can always go back to the values file to modify if we need to do that.


what is liveness probe:

Liveness probes: These probes help you evaluate whether an application that is running in a container is in a healthy state. If not, Kubernetes kills the container and attempts to redeploy it. These are useful when you want to ensure your application is not deadlocked or silently unresponsive. 

We do not want to creeat a load balancer service for now. So we need to modify the value file making a copy of the file and then modify the custom file and deploy it. That is 

helm shoe values helm-app/helm-app > value.yml

via into values.yml

change the serviceType to NodePort and the save quit. The deploy the application by running

helm install RepoPageAliasName ChartName -f ModifiedManifestfileName

helm install helm-app helm-app/helm-app -f value.yml

Let us access the application using CLI and web browser by

curl -v NodeIpaddress:NodePortNumber

It will be running

 paste NodeIpaddress:NodePortNumber in web browser, it will dispaly the application

 NOTE: Do not use the masterNode ipaddress to access the application. It will not run. Always use any of the workerNode to do that.

 FEW THINGS TO KNOW ABOUT HELM CHART

1.  We can great a github page just like https://landmarktechnology.github.io/helm-app23

2. Then push your helm chart into that github page. Once you are able to do that. Other members of our team can make use of that helm chart by just running the helm add command to add the repo to the list of their helm repo if they other helm repo before.

To clone helm repo page from github, you run

helm add aliasName URLofRepoGithubPageOfHelm

NOTE: If you want to see all the commands that we have run on our environment today, run the history command by

history

DEPLOYING THIRD-PARTY APPLICATIONS USING HELM

 Deploying Nginx Ingress using Helm

 Why is Ngynx Ingress need for application deployment?

 EXPLANATION

 Initially when external use want to access the application running in pod, they do that by

 nodeIpaddress:NodePort

 The above is a security risk. Not recommended. You cannot expose your node ip address where your pod is running to end users.  What you must do is below

 1. You would create an independent load balancer like ELB in aws in which the loadBalncer has a public subnet where external traffic can get into our cluster. The ELB will now stand as a proxy or security guard to our MasterNode. If any traffic comes external, it will go through to security checks and see if the traffic is safe and if it is, the traffic will now be routed to our Node. We do not expose our Node to the public, it is only the public subnet ip address of the ELB that is exposed external and why it private IP address is hidden from external.

 2. As soon as traffic get the Node, the Node redirect the traffic to a k8s operator called Ngynix ingress service which will usually have a rule in place. The rule would determine which of the pod in our cluster is meant to receive the traffic. If we have a pod called java-app and web-app. There will be a rule in Ngynix ingress service that may be

 /java:java-app-svc and  /web:web-app-svc

 and so if enduser typed in anything that relate to /java, the Nginx ingress service will inform that Nginx-ingress-Controller about it and the ingynx controller will route the traffic to java-app-svc and then the java-app-svc will route the traffic to java-app but if the end-user type in /web in the same vein, the Nginx controller will route the traffic to web-app-service and the web-app-service will route it to web-app

 The Nginx ingress controller is a pod. It is the pod that provide layer 7 load balancing

 LET US DEPLOY NGINX INGRESS USING HELM

 First of all, run 

 helm repo ls

 to know if you have any helm repository

 If you have am ngynx software helm chart in your helm repo, you can deploy it to install ngynx but if you dont have it, you need to add it to your helm repo in your helm environment

1. add the nginx helm chart  to your helm repo by running

 helm repo add aliasName URLOfTheHelm-NginxChart

The installation url of ngynx using helm is below

https://helm.ngynx.com/stable

Above url is where ngynx helm chart are found

helm repo add aliasName https://helm.ngynx.com/stable

let the aliasname be nginx-ingress

helm repo add aliasName https://helm.ngynx.com/stable

2. after adding it to your helm repo. you need to run

helm repo update

to get the latest update of the helm repo

If you now run

helm repo ls

You will seee that the repo has been added

LET US DEPLOY NGINX

Before we deploy the application, we need to know that chart name by running

helm search Aliasname

helm search repo

You will see that it has so many chart name but the one that we need is nginx-ingress/nginx-ingress

nginx-ingress/nginx-ingress description is that it will deploy nginx ingress controller

Let us also see the template chart and the value chart of our application by running

helm show values aliasname

helm template nginx-ingress

You will see that the name of our deployment name or metadata name is nginx

helm show values nginx-ingress

To deploy, we use the command 

heml install deploymentName chartName

deploymentName is nginx   and the charName is nginx-ingress/nginx-ingress

heml install nginx nginx-ingress/nginx-ingress

Nginx has been installed. The nginx ingress controller has be installed

Based on this deployment, this nginx ingress will create a load balancer in aws

If you also check your rancher platform, you will see that nginx ingress is running. That is why it shows on your rancher platform also.

Type of load balancer

LoadBalancers   :

  Network LoadBalancers 
     PROVIDE Layer 4 support
       layer = Transport
    10 applications  = 10 Network LoadBalancers

  Application LoadBalancers
     Provide layer7 support 
        Layer 7 is the Application Layer
    10 applications  = 1 LoadBalancer

Vendors:
  Nginx Ingress   
  HaProxy

Application running on  our infrastructures      
    Application can be designed

        Monolithic --> 3 Legacy Monolithic applications

        Microservices --> 7 Microservices applications
                          microservices is 10*7 = 70 


LET US DEPLOY MULTIPLE APPLICATIONS


make a directory called app and name the file myapp.yml my running

mkdir app/myapp.yml

Paste the below applications codes or script written in yml in the above file

# Complete Manifest Where in single yml we defined Deployment & Service for SpringApp & PVC(with default  StorageClass),ReplicaSet & Service For Mongo.

4 applications in myapp.yml

kind: Deployment
apiVersion: apps/v1
metadata:
  name: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  replicas: 2
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: mylandmarktech/maven-web-app
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /maven-web-app
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
kind: Service
apiVersion: v1
metadata:
  name: appsvc
spec:
  selector:
     app: myapp
  ports:
  - targetPort: 8080
    port: 80
---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: mylandmarktech/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo
---
apiVersion: v1
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodbpvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 16Gi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: pvc
         persistentVolumeClaim:
           claimName: mongodbpvc
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: pvc
           mountPath: /data/db
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017
---
apiVersion: v1
kind: Service
metadata:
  name: javasvc
spec:
  type: ClusterIP
  selector:
    app: java
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: Pod
metadata:
  name: javaapp
  labels:
    app: fe
spec:
  containers:
  - name: javaapp
    image: mylandmarktech/java-web-app
    ports:
    - containerPort: 8080

Paste the above in myapp.yml file you created and run

kubectl apply -f myapp.yml

If you run 

kubectl get pod

kubectl get deploy

kubectl get rs

kubectl get svc

You will see that pods, deployment, replicaset and service that has been deployed.

LET US CREATED NGINX INGRESS RULE USING HOST-BASED ROUTING

Let us via into nginx-ingress.yml 

We are would create a service called nnginx-ingress service where the rule on how nginx traffic would be routed to all the service pods in the cluster.

and paste the below inside the file

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myingress
spec:
  ingressClassName: nginx
  rules:
  - host: springapp.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: springapp
            port:
              number: 80
  - host: app.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /maven-web-app
        backend:
          service:
            name: appsvc
            port:
              number: 80
  - host: java.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /java-web-app
        backend:
          service:
            name: javasvc
            port:
              number: 80

Paste the file in nginx-ingress.yml and then run

kubectl apply -f nginx-ingress.yml 

You will see that the nginx-ingress rule for a host based routing will be created

You can verify by running

kubectl describe k8sObjectType MetadataName

kubectl describe ingress myingress

You will see how these host route traffic to their service and service route the traffic to their pods for response.


 There are 2 ways of routing Traffic in Nginx. We have

 1.  PATH-Based Routing: This is ideal for micr-services

Example of Path-Based-Routing are:

    dominionapps.net/
    dominionapps.net/login 
    dominionapps.net/registration
    dominionapps.net/account
    dominionapps.net/transfer
    dominionapps.net/payBills

    login
    registration 
    create account
    transfer 
    payBills
       scaling
       security 
       speed
       coding 
       light weight 
       avoid total failure Common in a Monolithic architecture  
       Monolithic poor use of resources

 With path-Based-Routing, it has single host but with different paths to another phase (dominionapps.net/registration, dominionapps.net/login. that is, path to the host).


 2. hOST-Based Routing:

   If we have multiple applications, we use host based routing.

   For example

  Application 1. The host is springapp.dominionapps.net  and it path is  /

  host: springapp.dominionapps.net/

If someone type springapp.dominionapps.net/ in his web browser, ngynix controller will route the traffic to the backend service called springapp, according to the nginyx rule above


In application2. The host is app.dominionapps.net  and the path is /maven-web-app

  host: app.dominionapps.net/maven-web-app

If someone type app.dominionapps.net/maven-web-app in his web browser, ngynix controller will route the traffic to the backend service called appsvc, according to the nginyx rule above

In application3. The host is java.dominionapps.net and the path is /java-web-ap

  host: java.dominionapps.net/java-web-ap

If someone type java.dominionapps.net/java-web-ap in his web browser, ngynix controller will route the traffic to the backend service called javasvc, according to the nginyx rule above

You can type in all these host and path in your web browser to know if they are really working.

With just one LoadBalancer which is the nginx loadbalancer, we are routing traffic to multiple pods. This is because when you install nginx ingress controler in your environment, you have installed a layer 7 laod balancer which is application load balancer.

FURTHER EXPLANATION

When users type in java.dominionapps.net/java-web-ap, the web browser first all does a ns-lookup (nameServer lookup) by qwery the global DSN or search global DNS (global domain name system). Once the search is complete, user web browser will identify where the traffic will go to, and will route traffic to the Elasti loadBalncer or webserver (els) because it is the elb that was tied to or has that a-records name and then ELB (that a records or dns resolution was created because the nginx url was very long) will route the traffic to the masterNode and the master node will route the traffic to nginx service operator where the rules on how to route the traffic to its appropraite pods are configured and then the ngynix service detect the service pod that the traffic is met to go and then inform ngynx controler, the ngynx controler will now route that traffic the required service pod and then service pod will route the traffic to the pod for response.

 NOTE: 

The first time user enter java.dominionapps.net/java-web-ap, it may take like 10 miliseconds to get response but the second time he will type the same java.dominionapps.net/java-web-ap, it may take like 2 milisoecods. This is because the first time he enter that name in its browser, that info is going to be stored in a place in its browser called CACHE. That is why he does it again and again, it will tale less time to get response.

Anytime you type something on your web browser, it is storeed in your web broswrr as CACHE. Your web browser, OS, ISP will recognize that dns name.

LET US CREATED NGINX INGRESS RULE USING PATH-BASED ROUTING

This only have just one host and the others are paths

Example

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-path
spec:
  ingressClassName: nginx
  rules:
  - host: dominionapps.net
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: springapp
            port:
              number: 80
      - path: /maven-web-app
        pathType: Prefix
        backend:
          service:
            name: appsvc
            port:
              number: 80
      - path: /java-web-app
        pathType: Prefix
        backend:
          service:
            name: javasvc
            port:
              number: 80

How can we achieve the transition to microservices??
   Change request 
      Monolithic  to microservices   
   train other colleague 


QUESTION IN CLASS

Which namespacxe should be deploy nginx ingress?

Answer:  You can deploy nginx ingress in your current namespace. Depending on the namespace you are currently on with. 

But Note that, it is not recommended to deploy your nginx ingress in your controlPlane or MaterNode. That is not advisable for security reasons. It can be deployed in another node like the rancher platform we were using.

Some importance of nginx is that once it is deployed, it provide an additional load balancing layer in your cluster (layer 7). This means that with Nginx, you can use one load balancer to deploy or route traffic to multiple backend applications or pods.

With Nginx, you can comfortably deploy your frontend applications using clusterIp service because Nginx comes with an additional layer of security because you can now use your ingress rule to expose those applications to external traffic or end users.

IQ

What is your experince in K8S?

My experience in k8s is that I have been able to provision a production grade k8s cluster for using KOPS deployment .



VIDEO 35 KUBERNETES 136

MONITORING APPLICATIONS PERFORMENACE (APPLICATION PERFORMANCE MONITORY)

prometheus and grafana are both open source application peformance monitory software. We also have SPLUNK which is also APM but it is not an open source. 

How do we ensure that the performance of applications deployed are monitored?

Prometheus is an exxmaple of a software used for monitoring the performance of applications

DEPLOYMENT OR INSTALLATION OF PROMETHEUS USING HELM

You know that we use HELM to deploy or install 3rd party applications/software

We also use HELM to deploy or install custom applications/software

 NOTE:

A third-party application is a software application made by someone else or made by a licensed software company

Custom applications is an application that is developed specifically for a particular client or organization. They are tailored to meet clients' specific needs, requirements, and preferences.


WHAT IS PROMETHEUS?

Prometheus is an openSource monitoring and alerting toolKit that is consist of different components listed below:

1. Prometheus Server: This collects and store time series data, based on a pull mechanism

2. Rule Engine: This allows the generation of alerts based on the collected or scraped metrics

3. Alert Manger: This is used for handling alerts

4. Multiple Integrations: This is used for graphing and dashboarding

What Is Server Monitoring?

Server Monitoring is a way of looking up or checking your server to see how your server are performing in real time.

Server Monitoring  can be used for troubleshooting and capacity planning espacially when you discover that the server is performing the way it is supposed to

Server monitory is most done at 3 levels. These are

1. Network (This involves prometheus checking the traffic routing, the bandwidth (sometimes when your bandwidth is so low. if you do not have good internet or you subscribe for lower badnwidth, it can lead to nethwork issues and prometheus can track that) and the latency (time it takes for user to receive feedback. prometheus can track that. Latency is measure in milliseconds.) )

2. Machine (This involves you checking the CPU, memory and the storage)

3. Applications (This involves you checking the commits, logs, rate of user commands etc)

WHY DO YOU MONITOR SERVER?

1. To avoid reactive panics. It is better to avoid something to occur than for you to allow it to occur and start to find solution to solving it. It can lead to panics and frustration

2. It icreases uptime

3. It makes you to plan for future on how to make best use of your resources.

4. It increases hardware and software performance

PROMETHEUS is an open source software that is used to monitor, collects and manage server and applications metrics

NOTE

 THINGS ABOUT PROMETHEUS

1. Prometheus comes with the Prometheus server that has 

a. HTTP server: 
b. it has a Time Series Database (TSDB)
C. it has a RETRIEVAL system : This is what collects data from target. It scrape data via a pull mechanisn

prometheus has what is called NODE EXPORTERS: This gathers all the things that are happening in the nodes

prometheus-web UI is used to access prometheus data or information

You can also use what is called Grafana or API clients to access prometheus data or information instead of prometheus-web UI

What is so important about prometheus is the alertManger


alertManger is used to trigger alerts. Like for example we can set a rule in our promethus server that when pods are down, an email should be sent to parties involved to be aware and resolve the issue 

WHAT ARE THE THINGS PROMETHEUS MONITORS?

1. Linux/Windows server

2. Apache server (HTTPS)

3. Single application

4. Database Server

5. it can monitor the CPU state of a server

6. it can monitor the memory or disk space usage of a server

7. it can monitor request count. That means it can track how many users are accessing the applications

8. it can monitor exceptions count

9. it can monitor request duration: This means that prometheus can track how long it will take for users requests to be processed.

It can also check METRICS

How does it check metrics?

It can track metrics in 3 ways. these are

1. COUNTER : means how many times an event may have occured?
2. GAUGE: means what is the memory value of that is left in your resource?
3. HISTOGRAM: How long or how big?

Prometheus also depend on EXPORTERS. we have what is called node exporters.

We also have prometheus database. This is where data is stored.

NodeExporter: This periodicall (1 seconds) gather all the metrics in your system. It monitors your fileSystem, disk. CPUs, memory and also your network statistics.

NodeExporter collects data about your nodes

Kube State Metrics: This is a service that listens to k8s API server and generates metrics about the state of the objects, including deployments, nodes and pods.

alertManger is used to trigger alerts. Like for example we can set a rule in our promethus server that when pods are down, an email should be sent to parties involved to be aware and resolve the issue 

WHAT IS GRAFANA?

Grafana is an open source software that makes it possible for you to be able to visualized information


Grafans will collect data from prometheus and put them in a graphical format. Grafana comes with dashboard

What are Grafana DashBoard?

Grafana DashBoard takes information from monitoring server like prometheus and display the information 

HOW DO WE DEPLOY PROMETHEUS AND GRAFANA IN REAL TIME USING HELM CHARTS

monitoring (micro-service) applications with  
Prometheus and Grafana
======================================
deploy prometheus using helm chart:

I just forked the below prof url about prometheus and grafana into my github

https://github.com/myLandmakTechnology/prometheus-grafana-ELK-EFK

Let us create a namespace called apm

Make sure that the memory of the node you want to use is up to 4GB of RAM. aleast t2 nedium.

to check how many namespace we have before, we run

kubectl create ns

 Let us create a namespace apm

kubectl create ns apm

Let us set apm namaspace as the context namespace by running

  kubectl config set-context <context_name> -- namespace=<ns_name>

       or

  kubectl config set-context --current --namespace=apm 


Since we want to set apm as context namespace, we run

  kubectl config set-context --current --ns=apm

Helm chart repo is below

 https://charts.helm.sh/stable

 Let us add helm chart repo to our repo here since we have installed helm in our environment. by running

 helm repo add aliasName helmRepoURL
  
helm repo add apm https://charts.helm.sh/stable

Whenever you add a repo, always update the repo by running

helm repo update

If you run

helm repo ls

You will see the repo apm you just added

Let us check how many charts are in helm by running

helm search repo apm

You will see that there are so many charts. Since we want to use helm to install prometheus, let us grep for prometheus by running

helm search repo apm | grep prometheus

You will also seee that there are so many chartName in prometheus. But we are looking for the one with the name apm.

Ther first chartName on the list is the one we need. That is apm/prometheus

Let us found out what was written in the manifesr file by running

helm template apm/prometheus

You will see all the resources it is going to create. it is a large file


helm values apm/prometheus

If you want to update anything, you can modify the values file by running

helm show values apm/prometheus

You will see the prometheus value file has a lot of resource in it which would be created as soon as we install it. It is a very large file. you can go through and if you want to modifiy the values file you can do so.

let us redirect the file to p.yml to create a copy and then modify it. by running

helm show values apm/prometheus >> p.yml

If you run 

ls

You will see that you have p.yml

vi into p.yml

let us see how many lines of command is in this script by typing

:se nu

The number of lines will display.

NOTE:

I just forked the below prof url about prometheus and grafana into my github

https://github.com/myLandmakTechnology/prometheus-grafana-ELK-EFK

If you go to the above url from prof, prof said he has arranged the prometheus values file in a way that you can edit it to suit your purpose. You can just change the alertManager info and pout in your emaail address details so that once you install, you will get alert in your email. Its just for practice purpose. inder the file, under alertmanager, go to receiver.

Let us copy this values file from prof in his github repo in the above url and past it in a new file called n-values.yml   make your you erase prof emal details and put yours in the alertmanager.


LET US INSTALL PROMETHEUS with the file n-values.yml that we have just edited from prof values file in github by running


helm install prometheus chartName -f prometheus-yamlFile

helm install prometheus apm/prometheus -n-values.yml

If you run

kubectl get pod

kubectl get all 

You will see all the resources in that file that was deployed. including alertmanager.

if you run

kubectl get svc

You will see that services wheere also deployed. The sservice that was deployed was clusterIP service which runs traffic within the cluster.

We want external traffic to be able to come to the cluster. 

What can we do to access the cluster externally?

We need to deploy ingress rule as show below 

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-resource-1
  namespace: apm
spec:
  ingressClassName: nginx
  rules:
  - host: alert.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: prometheus-alertManager
            port:
              number: 80
  - host: prometheus.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /maven-web-app
        backend:
          service:
            name: prometheus-server
            port:
              number: 80
 
copy and paste the above in ingressR.yml and the run

kubectl apply -f ingressR.yml

Let us describe our ingress by running


kubectl describe ingress ingressName

the ingress metadataName here is ingress-resource-1

kubectl describe ingress ingress-resource-1


You will see alert.dominionapps.net  and prometheus.dominionapps.net as the frontend url that you will give to enduser.

If you copy alert.dominionapps.net or prometheus.dominionapps.net and paste them in your web browser, they will not display. This is because we have not created a record set or dns record for those domain names.

Let us go to our route53 in aws resource and create a record set. by create a host called prometheus.dominionapps.net

Prof already register a domaine called .dominionapps.net and so what he did was just click on thhe host in route53 and the click on create, enter alert under recordName, select the alias, to applicatiolLoad balance, select the right AZ (US-WEST-1), click on create record. He also did the same for prometheus.

When he refreshed the web page whnere he paste alert.dominionapps.net 

It displayed

when he also paste prometheus.dominionapps.net on the web pagae, it displayed. you can even click on where it says alert and you will see thing that he can alert about.

Above is prometheus UI

We do not want to access prometheus using UI, we want to access prometheus using dashboard. That is where grafana comes in. 

With grafana, you can access prometheus dashboard, where you can see metrics and others. Prometheus UI is not the best way to explore or to display or visualize data

GRAFANA

You can also deploy grafana grafana using helm.

let us ckeck for grafana chartName by running
 helm serach repo apm | grep grafana

 You will see that the chartName is apm/grafana

 Let us install grafana by running

 Let us see the values file of this grafana by running

 helm show values apm/grafana

 Since the serviceType in grafana as show in the values file is clusterIP, we can just go ahead to install grafana because we alsready install ingress rule which will help user to be able to access the application externally.

helm install grafana apm/grafana

After the installation, it will tell you 2 things

1. it will tell you to enter a command to enable get the password to login to your grafana dashboord

2, and one other things here

Let us go back to your ingress rule file and and grafana dns record and service by 

vi ingressR.yml

paste the below

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-resource-1
  namespace: apm
spec:
  ingressClassName: nginx
  rules:
  - host: alert.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: prometheus-alertManager
            port:
              number: 80
  - host: prometheus.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /maven-web-app
        backend:
          service:
            name: prometheus-server
            port:
              number: 80
    - host: grafana.dominionapps.net
    http:
      paths:
      - pathType: Prefix
        path: /maven-web-app
        backend:
          service:
            name: grafana
            port:
              number: 80

You can know the serviceType for grafana by running

kubectl get svc

You will see that the service type for grafana is grafana

Let us spply the modification by running

kubectl apply -f ingresR.yml

If you now run

kubectl describe ingress  ingress-resource-1

You will see that grafana.dominionapps.net will join other a record

if we paste grafana.dominionapps.net on web page, it would not dispaly anything.

Let us create a record for grafana as we did before above.

If we have created a record or dns record for grafana.dominionapps.net

If we now paste grafana.dominionapps.net in the browser, it will display

Note: It will take a little why for it to display but just keep refreshing until it displayed. 

It took a while to display because it is going through a global dns registration before it can route traffic.


it will display a dashboard for you to enter your username and passowrd.

The default username is is admin but the passowrd can be gotten from the command the system asked you to run on your enviroment.

Once you have run the command, copy the passwrd in form of a token and past it in the grafana dashboard and then click.

NOTE:

(Prometheus: has a server that collects or scrape data from the kube-state metric (k8s objects like deployment, replicaset etc), nodeExporters (memory, cpu, disk)

Grafana: will be use to visualize. This means any data produce by promethues will be visualize by grafana

prometheus can scrape data from mongoDB or mysql and grafana will  visualize that data in form of a graph)

Inside the grafana dashborad, you will see where it says DATA SOURCE. click on it, click on prometheus, enter the name of the prometheus server. You can get that by running

kubectl get ep

You will see that the name of the prometheus server  is  prometheus-server

put prometheus-server under name, and also put prometheus-server under url

To create dashborad

Click on plus sign at the left up side, clcik import (to import a dashboard from somewhere), click on discard, click on import, select and click on the digit number (3119, 6417, 7249,8919, and others these are dashboard IDs that has been developed by developers, you can make use of any of them, let us make use of 3119 because i love the display 3119 brings), click on load, slect prometheus-server under prometheus, click import, it will display, it shows you want you have interms of memory, shows what you have in your cluster CPU, it also shows you your cluster fileSystem usage.

All this displays helps you to monitor how much resources of your server you have used or utilized. Everything happening in our nodes, pods and everything in our eks cluster is been visualized and monitored by prometheus and grafana.

During ths monitory, if anything goes wrong. Let say one of the node is irresponsive or killed or stop running, alert will be sent by alertManager that we have configures in our cluster though another node will be provisioned immediately with the help of our ControllerManagers.

IQ

WHY IS GRAFANA LINKED TO PROMETHEUS.

ANSWER: 

Grafana has to be linked to prometheus because grafan needs data source and if there is no data source, grafana will have no data to display. grafan displays the data or information that has be collected or scraped by prometheus server. Grafana display the data in a form of graph. grafan use prometheus as a data source.

HOW WOULD PROMETHEUS TALK TO GRAFANA?

ANSWER: 

These monitoring applications talk to each other via service discovery. Prometheus tal to grafana via service. 

how are service discovered?  or how would application like prometheus discover the service of grafana?

answer: service are discovered via their service name. it will discover grafana service via grafana service name.

NOTE: Prometheus has what is called time-series database (tsdb) and also have promethesu server that is used to scraped or collects all the metrics or data from k8s objects and nodes or collects all information happeneing in the cluster.

Prometheus has what is called kube-state-metric. this gathers all the metrics coming from your nodes, k8s objects and then prometheus server will now collects or scrape the metrics or server.

Some company use schlum for APM and schlum is paid for but most company go for prometheus and grafana because it is free.

QUESTION ASKED IN CLASS.

Kops can create other aws resources but kubeadm will not create other resources in aws.

Note: 

Most 3rd parties softwares like es, kubana, prom and grfana, filebeat are deployed via helm chart


138 VIDEO KUBERNETES

INSTALLING EFK AND ELK STACK IN K8S CLUSTER USING HELM

Monitoring, alerting and logs aggregation are essential for the smooth functioning of our production grade k8s cluster.

Application logs and system logs helps us understand what is happening inside the cluster.

Logs are particularly used for debugging problems and monitoring activities in the cluster

FOR EXMAPLE

if you want to check how application is running in our k8s cluster, we can check the logs by running

kubectl logs podName

To know the pods running in your k8s running

kubectl get pod                          in your currect namspace

kubectl get pod -A                       all pods running in all the namespace in your k8s


Note:

You can only check the logs of a pod.

If you run 

kubectl get pod -A 

You all see all the pods in all the namespaces.

You will see the following pod running if you install prometheus in your k8s and also if you deployed pods earlier

1. prometheus alertManager pod = this send alerts if something is going on in your cluster.

2. kube-state-metrics = this gathers all data in your k8s objects like RS, RC, pod, 

3. Node exporter gathers data in the node

4. prometheus server = this will collects or scrape all the data into the prmetheus time series data base. Once that data has been collected, we can visualized the data using Grafana

and the other pods running in the k8s cluster that you deployed earlier

Let say you have 100 pods running in your cluster. If you want to check the lods on each pod, it will be a tideous task

NOTE: If you try to check the lods of any pod and it was not susccessful, make sure you include the namespace of that pod.

WHAT IS ELASTC STACK.

Elastic stack is group of open source application from elstic designed to take data from any source and in any format, and then search, analyze and visualize the data

Elastic stack was formerly called ELK stack meaning elasticSearch. Logstash and Kibana and another additional fourth application called Beats

ElasticSearch is a distributed storage, analytic engine designed for horrizontal scalability.

We can use ElasticSearch to store data and to index data,

ElasticSearch will act like a container. It is going to store all the logs.

Kibana  is an open source analytic and visualiztion software designed to work with ElasticSearch. 

Kibana is used to visualize logs or data that are stored. Just like grafana in monitoring application for visualization, kibana in logs management is used to visualize logs or data stored.

Kibana can be used to search, view, interact with data stored in ElasticSearch and visualize data in a variety of charts, tables and maps.

Beats: are what we called data shippers. It is an open source that can be installed as an agent to send operational data directory ElasticSearch or logstash.

Types of Beats

1. fileBeats. this collects data from the logs file and sends it to logstash 

2. MetricBeats: deals with metrics like CPU, memory

3. PacketBeat: Network data

4. Uptime monitoring: this checks how long your server has been running

HOW COMPONENTS OF Elastic Stack or EFK INTERACT WITH EACH

It is stated below

1. fileBeat: this collects data from the logs file and sends it to logstash 

2. logstash enhances the data and sends it to elasticSearch

3. elasticSearch stores and index the data

4. kibana display the data that are stored in elastic search

Basically, Beats collects data and ship them to logstsh which processes the logs and ship them to elastic search for storage and indexes, while kibana will now display the data was stored in elasticSearch

beats collects data from logs file----->logstash for processing----->elasticSearch for stargae and indesxes------>kibana for display or visualization and sharing of the data.

HOW DO WE DEPLOY ELASTIC SEARCH

1. Your k8s cluster must have a storage resource like PV (persistentVolume). We created a PV for promentheus-server because pvc was enabled and also have a mongodb pvc. We have dynamic storage class was created as a result of that have pvc created

If we have deployed k8s cluster using KOPS, it would have also comes with dynamic storage class.

2. You require a k8s node of 4GB of RAM with 2 core

3. You need a server with kubectl and helm installed in it.

HOW TO INSTALL ELASTIC STACK OR ELK/EFK

1. create a namspace and called it elk by running

kubectl create ns efk

2. add the url for helm-elastic-stach to your helm repo by running

      helm repo add elastic url-for-helm-elastic-stach

   url for helm-elastic-stack is https://helm.elastic.co

 helm repo add aliasname https://helm.elastic.co

  let the aliasname be elastic

  helm repo add aliasname https://helm.elastic.co


3. update the repo by running 

helm repo update

to verify the addition of the helm-elastic repo, run

helm repo ls

You will see that elastic repo is added.

4. let us search the repo to get the chartName by running

helm search repo repoName

helm search repo elastic

you will that it will bring out different chartName. The one we will choose is the one that says

elastic/elasticsearch

5. Let us see the values file for the chartName above by running

   helm show values elastic/elasticsearch

In other to modify the values file if need be to suit your purpose, let us first of all create and directory by running

mkdir elk


Let us redirrect the the elastic values file into another file called elasticsearch.values that will be created inside the new direcory by runninhg

 helm show values elastic/elasticsearch >> elk/elasticsearch.values

 cd into elk and ls

 you will see that the file is there

 vi into elasticsearch.values

 Because we are just doing a demo or practise, so we need to change the replicas to 1 and also change the minimumMaterNode to 1 also. That is all we want to modify here. You room to modify anything you wish if you so desired.

Before we deploy elasticsearch, let us first of all check if we have elasticsearch in our elk namspace before now by running

kubectl get all -ns elk

You will seee that we do not have. Then run

 helm install elastic chartName -f valuesFile -ns elk 

helm install elastic elastic/elasticsearch -f elasticsearch.values -ns elk 

You will see that elastic search has been installed


To verify this, run 

kubectl get app -ns elk

You will see that the pod or elastic search is display as a statefulset application

If you run

kubectl get pvc -ns elk

You will see that PVC was also created with it under volume in according with the default values file of elastic search. 

This means that elasticService has claimed some volume out of the volume. Since PVC is created, PV is also created also because the elastic search has a dynamic storage class  y default

Let us check if the elastic search pod has been created and runnng by running

kubectl get pod -ns elk

You willl see that the pod is running

Let us check if a service was deployed also by running

kubectl get svc

You will see that a sewrvice was also deployed as a resource of installing elastic search.

LET US DEPLOY KIBANA

Let us check the values file for kibana by running

helm show values elastic/kibana

You will see that we have just 1 replacas, we have clusterIp as serviceType. 

1. NOTE: Since kibana is used to visualized or share the datas or information that is stored in the elasticSearch, it means kibana needs to be able to access externally and internally. Normally, we are supposed to use a nodePort or loadbalancver servicetype to see that it data van be visualized externally and internallly if Nginx were not installed in our cluster. Since nginx is installed in our cluster, we go use clusterIP servivetype because Nginx will be able to expose the application externally for end users to be able to visualiztion.

2. You do not need to add kibana because kibana and elasticsearch are in the above repo called elastic.

Let us get the chartName for kibana by running

helm search repo repoName

helm search repo elastic

You will see that the chartName for kibana is elastic/kibana

Let us deploy kibana by running

helm install kibana elastic/kibana -ns efk

We did not define the yml file where kibana will be created from, because we are still using the same values file. We did not change or modify anything.

kibana will be installed.   if you run

kubectl get deployment -ns efk


You will see that the kibana that we deployed was done as a deployment application .

Kibana was deployed using deployment as k8s object

But the elasticSearch that we deployed was deployed as a statefulset application

THE REASON WHY ELASTIC SEARCH IS DEPLOYED AS A STATEFULSET is simply because elasticSearch comes with a database and database are stateful application.

ElasticSearch is like a database. all the logs are shipped into it

kubectl get statefulset -ns efk

all te logs are ship into elasticsearch by fileBeat

FILEBEAT DEPLOYMENT

filebeat is the one that gather all the logs and then ship them to elasticsearch.

the chartName for filebeat would be elastic/filebeat

You chartName always reflect as

AliasRepoName/softwareName

We can still deploy filebeat with the same values file because by running

helm install repoName chartName

helm install elastic elastic/filebeat

NOTE: The k8s object used in deploying logs is Daemonset

If you want to verify if the application was deployed by daemonset, run

kubectl get ds -n efk

You will see that the pod will dssplay.

ACCESSING KIBANA EXTERNALLY

Since we already have nginx ingress running in our cluster. 

Let us now create ingress rule for KIBANA as indicated below.

Let us create a file called ingress2.yml

#ingress2.yml
apiVersion: extensions/v1beta1  # networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-rule-1
  namespace: efk
spec:
  ingressClassName: nginx
  rules:
  - host: kibana.dominionsystem.net
    http:
      paths:
      - backend:
          serviceName: kibana-kibana        #kubectl get svc to know servicePort and serviceName
          servicePort: 5601

save and quit and then run

kubectl apply -f ingressRuleFileName

kubectl apply -f ingress2.yml

ingress rule will be created.

Let us describe the ingress rule pod created by running

kubectl describe KindOfK8sObjectUsed metadataName

kubectl describe ingress ingress-rule-1 -ns efk

You will see that the a record or host called kibana.dominionsystem.net is routing traffic from external via loadbalancer service to the cluster and when it hit the cluster, it talks to kibana service  and the kibana service route the traffic to the container.

You can create the a record kibana.dominionsystem.net in aws route53. prof already have dominionsystem.net  he created a record from it or edit existen record and put kibana and then save.

Once we have done that, you can paste the url kibana.dominionsystem.net on your browser and it will route traffic to kibana.

If you run

kubectl get ingress -n efk -o wide

You will see the long address of loadbalancer of our ingress. 
If you run

nslookup a recordName

nslookup kibana.dominionsystem.net

It will tell you the ipaddress of that record, ipadress of the server etc

Anytime end user type kibana.dominionsystem.net in their web browser, it will qwery global dns and do a dns lookup and when it find the loadbalancer-url where the a record is tied to, the traffic will hit the ingress-server where the ingress rule is defined, since the rule indicated that kibana.dominionsystem.net is host of kibana-kibana, it will alert the ingress-controller will will route the traffic to kibana-kibana service and the kibana service will route the traffic to the kibana pod for data visualizrion.

Let us type in kibana.dominionsystem.ne in our web browser

it will display a kibana dashboard with name elastic because that was what you name the url. 

click on the option simbol, click on discover, let us create an Index pattern by clicking on CREATE AN INDEX PATTERM, type filebeat as the name, select timestamp, click on create index pattern

NOTE: Whatever is happening in the cluster is going to be captured by the filebeat base on the timestamp.

IQ

How long do you retain logs in the elastic search?

answer:

It all depends. You cab retain logs in elastic search for about one month because logs are going to be generated regularly as you use your cluster and so our retainship can just be a month after which, you can move the logs to aws s3 bucket for storage.

What is a logs?

 log file is a textual data file that stores events, processes, messages, and other data from applications, operating systems, or devices. They provide information based on the actions performed by users, playing an important role in monitoring IT environments.

 Logs are variable files and these files are constantly changing. They are variable events.

 If you run a top command by running

 top

 it brings out applications or pods that are in your cluster

Summary About K8s 

   Talking points:

1. k8s is a Container Orchestration sofwtare use to manage or orchestrate containerized applications or containerized microservice applications

      kubernetes Orchestrate containerise applications 
      kubernetes Orchestrate containerise micro-service applications 

2. k8s has a Self healing capacity= which means if something goes wrong, it can fix it back immediately, scalability, Disaster recovery, LB,  
   
3. it has automated rollouts and rollbacks

4. k8s architecture. k8s architecture are

1. masterNode also called controlPlain

The masterNode comprises of

-apiserver: This receives the api call. This is the one that process api calls
    if you run

   kubectl get
   helm install
   UI= using k8s dashbopard like Rancher

  above command means you are making api called

You are making api calls

-etcd: this is the database in k8s used for storing log files or data and transfer it to the scheduler.
-scheduler: the scheduler talk to the workerNode via kubelet and then assign a pod to the appropriate workernode that would be able to accomondate the pod.
-controlerManagers
-k8s cni (container networks interface)= kube-proxy, kube dns are k8s cni


2. workerNode

The workerNode comprises of

-kubelet: The kubelet is the one that establish a communication between the masternode and the workernode. The masternode or controlplain talk to the workernode via the kubelet.

-container runtime= k8s docker or containerd as a container runtime
-kube-proxy



5.We use k8s objects to deploy workloads in our k8s cluster.

Some of the k8s objects are

service
pod
replicaSet
replicationControler
volumes
namespace
daemonset
deployment
configMap
Secret
statefulset
ResourceQuota
node
nodeSelector
nodeAffinity
podAffinity
security


Kubernetes objects:
  services = service discovery : does loadbalancing. expose the pods to use or other pods
    users/admin/app--->SVC--->pod[APPcontainer]
    service-TYPES:
      ClusterIP : this is the default servicetype. the most secured servicetype
      NodePort: external and internal communication
      LoadBalancer: establish external and internal communication
      ExternalName: This is used to establish a communication outside your cluster
      Ingress: app. deployed using clusterIP and ingress will expose the application externally
        spec:
          type  
          selector:
            app: myapp 
      NodePort   
      LoadBalancer  
      ExternalName 
      ingress 
         ClusterIP 
  pods--->houses containers. the lifecycle is short. not recommened because cannot scale 
     SingleContainerPods  
     MultiContainerPods 
  PodTemplate:
    metadata:
      labels:
        app: myapp 
    spec:
      containers
  ReplicationControllers: This is one of the controllerManagers.
    spec:
      selector:
        app: myapp 

  ReplicaSet:
    selector:
      matchExpressions [set-based]
      matchLabels: [eqaulity based]
        app: myapp 
  StatefulSet: This is used to deploy database application

  DaemonSet: used to deploy logs files application

  Deployment: This is most recommended among k8s objects because it can rollout and rollback

  volumes: creating a persistent volume by creating and manage volume within the cluster

  configMaps: use to defined addition configuration that is not included on the dockerfile. They are used to defined non-confidential data like usewrname,  db-username=${username}

  secrets: they are used to defined confidential data like password db-password=${password}. used for authentication when you want to pull image from a private registry.

  namespace : help to isolate apps. . it is sued to separate the different phase of application deployment lifecycle. it is good for security. You can decide that developers can only have access to the dev namespace. They will have not busines with the prod namespace.

  ResourceQuota: This is used to maximized the k8s resource or cluster storage size we have so that we do not exhaust them 

  node: by default the scheduler assigns pods to be  created in node with sufficient resources 

  nodeSelector: use to restrict the node(s) where pods should be created. use to allow pod to be created in a particular node.

  nodeAffinity: This ia also the same as nodeSelector,  use to restrict the node(s) where pods should be created 
  podAffinity
    PriorityClasses   
      prod-app [high priority]  
      dev-app  [low priority]
      test-app [meduim priority]
    dev-app is running   
    prod-app is pending due to insufficient resources 
    If podAffinity is enforced dev-app pods will be evicted and    
    prod-app pods will be running 

  Health Checkers:
  liveness probes: if pod is not ready, liveness probes will restart it up. it is health chacker
  readiness probes: It prevent a pod from receiving traffic when the pod is not ready.
     users/apps/admins --->appSVC---> 4pods[appsContainer] 
     http:
        /java-web-app
    curl -v localhost:8080/java-web-app 
  startup probes: This are used to start up containers atht are slow to start
  security:


CLASS QUESTION

Why would log files taken away from elastic serach and be stored in s3 bucket ?

answer: log files can be kept in elasticSearch for maybe one month and you may move the logs to your aws s3 bucket.

let us check if we have aws s3 here in our environment by running

aws s3 ls

To create log file bucket in our s3, we run

aws s3 mb s3://bucketName --region regionName

aws s3 mb s3://logs2 --region us-east-1

ElasticSearch stored log files in k8s cluster and the log files are gotten from nodes and pods

NOTE: Rancher is an enterprise-grade platform that facilitates consistent administering of multiple Kubernetes clusters from a single UI—while addressing key Kubernetes pain points, such as cluster and workload deployment, security management, workload monitoring across multiple clusters, and scalability.

VIDEO 139 

KUBERNETES SECURITY

SECURITY BEST PRACTICES FOR K8S DEPLOYMENT.

https://kubernetes.io/blog/2016/08/security-best-practices-kubernetes-deployment

You can get more details about kubernest security with the above url

WHY DO YOU SECURITY YOUR K8S CLUSTER

1. To ensure that only authorized images are used in your environment.

docker images runs to formed container. The running ec2-instances of a docker images forms a container. 

where are images primarily stored or kept?

We can keep images in images registry like dockerHub(public), AMAZON ECR (elastic container registry=private registry), Nexus/Jfrog (private)

2. To limit direct access to your k8s nodes. We need to manage or be sure of who can have access to our k8s nodes.

command to run To get access to node are

kubectl get/describe/edit/update/label/delete/

3. To create administrative boundaries between resources

 Like the use of namespace to create isolation in the different stages of application deployment. that is

 You restrict developers to be able to have access to only the dev namespace

 Maybe the Sr. Engineers may have access to the entire cluster

4. To define Resource Quota. Like if you have a dev namespace as an example and you want to deploy a resource quota as below

apiVersio:
Kind: ResourceQuota
Metadata:
 name: dev-resource
 namespace: dev
spec
 -resources
   cpu: 14Gi
   mem: 128Mi
   pods

With above, resourceQuota simply means that we want to defines how much resources that can use in the namespace called dev out from the total resouces we have the the cluster. The amount of resources that will be assign to dev namespace only. This means that no matter resource you have in your dev namespace, you will not utillize more that 14Gi of CPU and 128Mi of memmory.

5. To implememt network segmentation. You can also use namespace to isolate applications, to isolate environemnst like dev, uat, prod., etc.

6. To apply security context to your pods and containers. To determine who can access certain pods and containers

7. To log everything. Using EFK to manage all logs that are generated from our application

8. In suammary, we can use what is called role base access control (RBAC)to achieve the above listed security measures.

KUBERNESTES ROLE BASED ACCESS CONTROL

WHAT IS K8S ROLE BASED ACCESS CONTROL?

Before a request is sent to api-server or before an api call is made, the requestor needs to be authenticated by environment or system to make sure that the requestor is recognized (use of username and password), if recognized or known, then the requestor would go ahead to make the call or make request and the call will pass through authorization process to see if the requestor or api caller is authorized to make sure an api call or to make such a request. If the requestor does not have the permission to make sure call, the request will be declined.

RBAC

Authentication

Api call or request

AUTHORIZATION


RBAC defined which user can do work within the k8s cluster

QUESTION

If you have been working with k8s for sometime and you may have been faced with a situation where you are told to give some users limited access to the k8s cluster.

For example

You have told to only grant a developer called Michael to only have access to development namespace and nothing else. How are you going to achieve this Role Based Access?

Answer: I will use the concept of authentication and authorization in kubernetes

There are 3 kinds of users that need access to k8s clusters. These are 

1. Developers and Admin
2. End-users
3. Applications and Bots


1. Developers/Admin/Engineers

These are users that are responsible to do some administrative or developmental task on the cluster. Like upgrading the cluster (changing the version of k8s in your env to a newer version), creating resources or workload on the cluster

2. End-Users: These are users that have access to the application after the application has been fully deployed in the k8s cluster.
Restriction access to this application will be carried out by the application itself. Like web application will have security mechanisms put in place to prevent endusers unauthorized access.

app.com is a-record or registered dns name that is connected to the load balancer URL and then the LB will now route the traffic the backend applications.

With this secured app.com. That is https://app,com, we have ensured that user data enterance into the app.com is encrypted for security reasons using certificate management like aws SSL/TLS certification that enables website to be secured..

end-users-->app.com-->LB-->node-->ingressService-->ingressController-->podSVC-->pod

Enduser traffic cannot talk to our backend app directly. That is a security bridge

3. Applications and Bots: There ia a possiblity that other application may need access to your k8s cluster, typically to talk to resources or workloads inside the cluster. K8s ensure this with the use of pod service. the pod service will grant access between pods or applications within the cluster. 

For exmaple when we created an ingressService account. role and rolebinding were defined in that ingressService and this service was able to talk to another service like the pod service with the rule that was defined.
  role
  rol binding

  RBAC IN K8S IS BASED ON 3 KEY CONCEPT

1. VERB --> there are many verb that can be executed on a resource. These are

create, read, update, delete, list, get, watch, patch

2. api resources: this are objects that are present in the k8s cluster. Like, pods, nodes, service, PV, ReplicaSet, Deployment and other k8s objects

3, subjects: These are object that allow access to the k8s api resources based on verb and resources. Example of subjects are Users, Groups, Service Account. You may create a group and in that group, a role may be assigtn to that group that it is only people in that group that have the permission or access to do an api call like kubectl delete. etc.

EXMAPLES

CREATING ROLE AND ROLEBINDING IN RBAC

RBAC Role

A Role example named example-role which allows access to the mynamespace with get, watch, list, etc. operations:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
 name: deployment-manager
 namespace: office
rules:
 -apiGroups: ["*", "extensions", "apps"]
  resources: ["deployments", "replicaets", "pods"]
  verbs: ["create", "delete", "edit", "get", "update", "watch", "patch" "*"]

 the essence of the ycap * means that the above group can still execute other verb, resource or apigroup command that are not listed.

 In the rules above we:
1.	apiGroups: ["*"] – to set core API group
2.	resources: ["pods"] – which resources are allowed for access
3.	["get", "watch", "list"] – which actions are allowed over the resources above

RBAC RoleBingding

RoleBinding is when a role created is assigned to a particular subject like a user or a group or serviceAccounts or any other subject.

Let us create a roleBinding and assigned it to the Role above.

 kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
 name: deployment-manager-binding
 namespace: office
subjects:
 -kind: user
  name: InsightemployeeName
  apiGroup: rbac.authorization.k8s.io
roleRef:
 kind: Role
 name: deployment-manager
 apiGroup: rbac.authorization.k8s.io

Note: apiVersion of any "Kind" can be know by running

kubectl get api-resource | grep kindName

if the kind is Role and RoleBinding, we run

kubectl get api-resource | grep Role

kubectl get api-resource | grep RoleBinding


Role vs ClusterRole

Alongside with the Role and ClusterRole which are set of rules to describe permissions –
Kubernetes also has RoleBinding and ClusterRoleBinding objects.

The difference is that Role is used inside of a namespace, while ClusterRole is cluster-wide permission without a namespace boundaries, for example:
	allow access to a cluster nodes
	resources in all namespaces
	allow access to endpoints like /healthz


what is the difference between role and clusterRole

Role is limited to a single or one namespace while clusterRole is covers the entire cluster (roles for all the namespace in the cluster)

A ClusterRole looks similar to a Role with the only difference that we have to set its kind as ClusterRole:

ClusterRole in RBAC

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1 
metadata:
  name: deployment-manager
rules:
 -apiGroups: ["*", "extensions", "apps"]
  resources: ["deployments", "replicaets", "pods"]
  verbs: ["create", "delete", "edit", "get", "update", "watch", "patch" "*"]


ClusterRoleBinding in RBAC

kind: clusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
 name: deployment-manager-binding
 namespace: office
subjects:
 -kind: user
  name: employeeName  #it can be bode or simon or kim etc
  apiGroup: rbac.authorization.k8s.io
roleRef:
 kind: Role
 name: deployment-manager
 apiGroup: rbac.authorization.k8s.io

 Keep in mind that once you’ll create a Binding you’ll not be able to edit its roleRef value – instead, you’ll have to delete a Binding and recreate 

•	Kubernetes uses RBAC to control different access levels to its resources depending on the rules set in Roles or ClusterRoles.

•	Roles and ClusterRoles use API namespaces, verbs and resources to secure access.
•	Roles and ClusterRoles are ineffective unless they are linked to a subject (User, serviceAccount...etc) through RoleBinding or             ClusterRoleBinding.
•	Roles work within the constraints of a namespace. 
    It would default to the “default” namespace if none was specified.
•	ClusterRoles are not bound to a specific namespace as they apply to the cluster as a whole.

HOW TO USE ASSIGN RBAC USING EKS (elastic K8s service in aws)

FOR AUTHENTICATION

There are ways a user can be authented or Identified in K8S. These are via

1. x-509 certificate
2. LDAP
3. IAM - EKS in aws 

k8s does not have direct way you can get authenticated to be in k8s environment but you can integrate k8s with the above listings (IAM, LDAP, x-509 certificate) to be able to get authenticated.

FOR AUTHORIZATION

We use the below for authorization in k8s

1. Role
2. RoleBinding

HOW TO CONFIGURE ROLE AND CLUSTER-ROLE IN OUR K8S CLUSTER LIKE EKS

Prof provision or create his EKS cluster using terraform. after the creation of the EKS

When he ran the command

terraform show

It deplayed some resouces in the kube config file that has a config map in it.

On the part of the configmap manifestfile, the configmap was deplayed in the form of below

apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::921483055369:role/EKS_Node_Role
      username: system:node:{{EC2PrivateDNSName}}
  mapUsers: |
    - userarn: arn:aws:iam::935840844891:user/Mithun          # Update your user arn here
      username: Mithun                                        # Update your user name.
kind: ConfigMap
metadata:
  creationTimestamp: "2020-10-19T03:35:20Z"
  name: aws-auth
  namespace: kube-system
  resourceVersion: "792449"
  selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth
  uid: 8135dcd1-90e6-4dfb-872f-636601475aca

  If you check the metadata name of the configmap, you will see aws-auth  that is what will authorize you. It is what will authenticate users. You know you use configmap to grant authentication for non-confidential data. The configmap was created in kube-system namespace

Since the configmap was created in kube-system ns, let us run

kubectl get configmap -n kube-system

You will see the configmap in that namespace

To know the cluster running in your aws, you run

aws eks list-clusters

It will list the cluster running in your aws. Prof cluster called terraform-eks-demo was among the list.

NOTE THE BELOW COMMAND IN EKS

- for you to know if you are authorize to perform some verb (delete,create,get,update) in aws eks, you run the below command to know

kubectl auth can-i delete/get/create/update/ object

let say you want to know if you can delete a pod, you run

kubectl auth can-i delete pod

kubectl auth can-i update pod

kubectl auth can-i get pod

kubectl auth can-i delete service/node

kubectl auth can-i get service

kubectl auth can-i create service

If you are authorized to do so, it will tell you "yes"

- If you want to know if you can do everything in the eks cluster, you can run

kubectl auth can-i "*" "*"

This means that if you can do any verb and also any object. If you can do everything in the eks

If authorized to do so, it will tell you "yes" if yes, it means that you are a SUPER-USER

EXAMPLE

1.  us go to our aws IAM console and create a group.

click on group, click on create group, UserGroupName be fintech-dev, click on create.

Let us create another group called fintech-devops-engineers using the same procedures as above. 

No policy is attached to either of the group yet

2. Let us create users

click on user, click on create user, username be bode, select programmatic access for bode (that will grant him access to programmatic tools like AWS API, CLI, SDK, and others but does not grant access to aws console. This means that he cannot create a group, or users. It is only console access that can do everything etc ), click on next permission, select add user to group, select the group (fintech-dev), click on review and click on create user. You have just assigned the user to fintech-dev

This user created will have a ACCESS KEY and a SECRET ACCESS KEY

subject
user: bode

ACCESS KEY = AKIAYE6GSHF3TRTW73TB

SECRET ACCESS KEY = 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 

Let us grant the user restricted access.

Let us search for bode ARN

To search for bode arn, click on IAM, click on user, under summary, you will see where they wrote user-ARN, copy the ARN url. it in the form of below

arn:aws:iam::345527142:user/bode

arn = amazon resource name

This arn details is what identifies the user bode

Know where you eks cluster is running. check your aws eks to know where it is running

prof eks cluster is running in N-california

LET US JUST DO THIS ON A LOCAL TERMINAL USING MY WINDOWS LAPTOP

make sure that aws CLI is installed in your environment or server that you want to use. confirm if you have aws running in your windows desktop.

1. enter aws configure

it will ask you access key, paste the access key you created for bode and press enter, it will ask for secret access key, paste the secrete access key and press enter, it will show you the regios, press enter, it will see table format, press enter.

If you run

aws s3 ls

It will say access denied. this is because this user does not have the permission yet to do that.

If you run

aws eks list-clusters

access will be denied.

Let us go to IAM and grant access.

Click on IAM, click on policies, click on create policy, click on service (which service do you want this policy to cover?), type in eks and click on eks and tick what you want, lets us tick read, write, list but do not tick all. if you tick all, it means the policy will grant user to be able to do anything in eks, click next, if you click specific under resources, (let assume that we have up to about 10 cluster that we are managing in our company and this cluster may be in different availability zone, you can tick on cluster, click on edit, you delete the ycap and type in the zone that you the cluster to be restricted to is under region like us-west-1 = this means that only the cluster in us-west-1 that he has access to, your can enter the account name under account but you can tick any if you do not want to specify the accfount name, uder cluster name, you can delete the ycap and enter the cluster name that you want this policy to cover) for now just tick any for all and click save changes, click on all resources to grant full eks resouces access, click next, click review, under review policy name put my-eks-policy, click on create policy, click on action, click on attach, tick bode, click policy, We mow have full access for eks

Let bode go to his window desktop cli (gitbash), and run the command

aws eks list-cluster

It will bring out bode aws clusters available in the aws account. 

In the aws, terraform-eks-cluster was created.

if bode run

kubectl get pod

it will execute because bode need a kube config file 

For bode to access the cluster, he will need kube config file

The command to directly import kube config file of the eks cluster created into your aws server like windows desktop is

aws eks update-kubeconfid --name eksClusterName --region regionDetails

if the eksClusterName is terraform-eks-cluster and the regionDetails is us-west-1. the command will be

aws eks update-kubeconfig --name terraform-eks-cluster --region us-west-1

the region where the eks cluster were provisioned should be defined as in the above.

if you run the above command, the .kube/configfile of terraform-eks-cluster will be imported to this windows environment

With the importation of the above .kube/config file, we can now be able to make api calls.

but when bode run

kubectl get pod

he was told that he is not authorized. unauthorized

WHAT DO YOU DO FOR BODE TO BE AUTHORIZED TO PERFORM THE ABOVE COOMAND (kubectl get pod)

The superUser or admin (someone who can perform everything in the eks cluster, who have the admin access) can be able to grant bode access by editing the configmap in the .kube/config file.

1. In the superUser environment, first of all, let us know th name of the configmap by running

kubectl get configmap -n kube-system   

You will see the the name is aws-auth

2. Let the superUser edit the  configmap by running

kubectl edit configmap aws-auth -n kube-system

The configmap is the one where our access is.
 
3. add the user info (bode info) by

under mapRoles, add mapUsers and specify the user by arn and username of bode in the fomr below

apiVersion: vi
data:
 mapUsers: |
  -userarn: arn:aws:iam::345527142:user/bode
   username: bode
 mapRoles: |


we will only did in the above is just to add mapUsers and leave the configuration as it is and then save-quit. This now makes bode to be authenticated or recognised by the eks cluster

if bode now go to his windows desktop and run

kubectl get pod

Though he now have access to the k8s cluster because he has been authenbticated but still, he deos not have permission to perform the above api call.

What will the SuperUser Do To ensure that Bode has right to list pod?

1. In the superUser environment, let us create a role like role-dev.yml by 

vi role-dev.yml

and paste the below into role-dev.yml


kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: readonly
rules:
- apiGroups: [""]  #This represents apiVersion resources v1 = pods, service, PV, configmap
  resources: ["pods"]
  verbs: ["get","list"]
- apiGroups: ["apps"]
  resources: ["deployments","replicasets","daemonsets"]
  verbs: ["get","list"]

---

  kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: full_access_role_binding
  namespace: default
subjects:
- kind: User
  name: bode                           # Map with username
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: readonly  # the roleRef name must ne the same with the metadata name.
  apiGroup: rbac.authorization.k8s.io

  and the save-quit

  Then run

  kubectl apply -f role-dev.yml

  The role and the roleBinding for bode will be created

  If bodey now go to his environment and run

  kubectl get pod

  You will see that he will be able to list all pod in the default namespace. Note: for role and rolebinding, you can only have access to a single namespace which is the default that we specified in the manifest file above. we can always edit the role of the manifest file to suit any user or change the namespace., etc.

  If there was not pod running in the default namespace, it will say no resources found in the default namespace.

Let bode try to list services by running

kubectl get svc

You will see that it was restricted. 

This can be resolved by the superUser by going back to the role manifest file and add services to the resources. He just have to only edit the role and not the roleBinding except if he wants to add another user or replace the user,  as shown below:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: readonly
rules:
- apiGroups: [""]  #This represents apiVersion resources v1 = pods, service, PV, configmap
  resources: ["pods", "services"]
  verbs: ["get","list"]
- apiGroups: ["apps"]
  resources: ["deployments","replicasets","daemonsets"]
  verbs: ["get","list"]

 [""]  #This represents apiVersion resources v1 = pods, service, PV, configmap

 After you have edited your manifest, run

 kubectl apply -f role-dev.yml

If bode now run the above command again, 

kubectl get svc

he can be able to list service

If superUser wants bode to be able to list all the resources, he can just only edit the role part and put "*" where resources is under roles as shown below

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: readonly
rules:
- apiGroups: [""]  #This represents apiVersion resources v1 = pods, service, PV, configmap
  resources: ["*"]
  verbs: ["get","list"]
- apiGroups: ["apps"]
  resources: ["deployments","replicasets","daemonsets"]
verbs: ["get","list"]

 After you have edited your manifest, run

 kubectl apply -f role-dev.yml

 Bode will now be able to list all resources

NOTE:

Note: Bode can always access his permission in any Os. whether windows or mobexterm

Let su say another developer called simon need to get access to the eks using his mobaxterm.

What the superUser or admin need to do is to go to AWS IAM and create another user called simon and add him to a developer group. This means that whatever permission developer has in that IAM group, simon will inherit the permission. Upon the creation of simon as a user in IAM, an access key and secret accesskey will be generated for simon user and the admin will send it to simon.

Simon will now go to his mobalxterm server where aws is installed and then run

aws configure

It will ask for access ke, put in your access key, it will ask for secrete access key, put in your secrete access key, it will ask for region, put in the region where the eks cluster were created from, it will ask for format, type in table and press enter

Simon has suucessfully login into the aws account of the superUser or company aws account.
If simon run

aws eks list-clusters

All the cluster that in the aws will show. You will see that the aws eks cluster called terraform-eks-demo that was created by the superUser is present. 

Simon will have to first of all update his environment by running

aws eks update-kubeconfig --name terraform-eks-cluster --region us-west-1

The above command will import and update .kube/config file directly from the eks cluster created, into simon aws server.

If simon run

kubectl get pod

It will not run because simon has no role access and configmap access yet even though he might have the IAM access.

First thing Simon to do report to the superUser or admin that they should grant him access.

What admin we do is to

1. to create a fresh role and rolebinding for simon and define the actual role that he can perform or if admin want simon and other developers like bode to have the same role, the admin can just add simon to the rolebinding only just to add simon as a user in that file as below  

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: readonly
rules:
- apiGroups: [""]  #This represents apiVersion resources v1 = pods, service, PV, configmap
  resources: ["*"]
  verbs: ["get","list"]
- apiGroups: ["apps"]
  resources: ["deployments","replicasets","daemonsets"]
verbs: ["get","list"]
---
  kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: full_access_role_binding
  namespace: default
subjects:
- kind: User
  name: bode    
- kind: User
  name: simon                        # Map with username
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: readonly  # the roleRef name must ne the same with the metadata name.
  apiGroup: rbac.authorization.k8s.io


After you have only edited the roleBinding manifest by adding simon and then save and quit, run

kubectl apply -f role-dev.yml

After you have deployed the edited rolebinding, simon will now have permission to carry out task but does not have permission to the eks cluster yet and so he can not still run any task on the eks cluster. 

The only way simon can have permission to run task inside the cluster is for simon to be added to the configmap file

2. Ladmin will modify the configmap file and the configmap of the eks cluster was deployed in the kube-system namespace and the name of the configmap was aws-auth. we need to edit the confimap by running

kubectl edit configmap configmapName -n kube-system

kubectl edit configmap aws-auth -n kube-system

go to here mapUsers are and add

userarn: arn:aws:iam::274353219:user/simon
username: simon

After admin have edited the configmap and save-quit

Admin will inform simon that he can now have access. 

If simon run

kubectl get pod, if he has the role to do so, he will now be able to see pods that are created indoe the eks cluster.

VIDEO 141  REVISION TILL K8S

IQ QUESTIONS

What is devops to you?

Devops is all about automation

It is a bridge between developing and it soperations . this bridge is all about automation.

We automate application developement by developers, build, testing, deployment and the application performers monitoring, security

we also automate infrastructure creation, provisioning, configuration and security

How do you automate?

We automate by using technologies stack or tools like

1. commands (the use of CLI over UI)
2. scripts (bash-shell scripts, groovy (jenkinsfile), yaml scripts (kubernetes manifestFile), .tf, docker-compose file, xml)
3. cloud computing like aws, microsoft azure helps you to easily automate task. we use aws more.
2. jenkins (we can integrate jenkins with github, maven, sonarqube, nexus, dockerhub, kubernetes, prometheus and grafana and ansible)
3.kubernetes
3. aws auto scaling group (ASG)
4. ELB
terraform
5. aws services like ec2-server, vpc, eks, s3, ebs volume, efs/nfs volume, elb, asg, pv, route53, cloudWatch, ecr, fargate, Lambda, IAM, LC, LT, RDS, ACM, 

In landmark, We create our infrastructure in aws cloud.

where or how can we deploy our applications?

we can deploy applications in virtual machine or in an ec2-server like deploying in tomcat server. That is tomcat software installed in a virtual machine or ec2-instance

we can also deploy applications in containers like docker, k8s, ecs, dockerSwarm, lambda function, fargate=(is a serverless service for aws), etc

How do you create or provision your infrastructure in AWS?

QUESTION

What are your experience using cloud computer platform like aws?

Answer: In our environment, we use aws for host our infrastructure are hosted in aws cloud platform

The services that I have used in aws are 

Infrastructure can be provisioned using ec2-server, vpc, eks, s3, ebs volume, efs/nfs volume, elb, asg, pv, route53, cloudWatch, ecr, fargate, Lambda, IAM, LC, LT, RDS, ACM,

Question: how did you create those infrastructures?

ANSWER: In landmark, we created and manage infrastructures using 

CLI: exmaple, you can run  aws s3 mb s3://buckName

GUI: using console

We can also use infrastructure as a code IaaC like ANSIBLE OR TERREFORM or cloudFormation. that is usinga file to create infrastructure.

How does engineers or admin grant permissions or access to infrastructures to employers

In landmark, admin can grant access to our infrasturcture using

aws-IAM

using IAM, access can be granted by

1. console access: where username and password are sent toi the employer
2. programmatic access where accessKey and secrete accessKey are sent to the employer.

Some of the access are readONLY access, writeAccess, FullAccess etec

USING IaaC like terraform, the use of programmatic access is what is obtainable (accessKey and secrete accessKey. they are like encrypted token).
Iaac as a code is the entire automation environment.

IQ

What sre the main architecture of k8s or k8s cluster?

ANSWER: k8s architecture are

1. masterNode or controlPlain
This is made up of : 
api-server
etcd
scheduler
controlManager

api-server is the administrator, it is the one that put to action any command or api call that is been made in k8s, the ectdc is the one that store the information or data brought by the api-server and then connect with scheduler, scheduler is the one that schedule or assign what ever info stored sn the etcd as pods to any node that has the capacity to accomondate such pods and so the controlerManage is the one that manages the situation around the cluister should incase something goes wrong, he will find soluation (like when one of the pod stopped running. The controlermanager would automatically create another pod and the scheduler will assign the pod to a node that has the resource to accomondate such pod).

2. WorkerName

This is made up of : 

kublet: this is the main agent running in workerNode. Kubelet is the one that establish and regulate the communication between the masterNode and the workerNode. He is the middle man

kube-proxy: This is the one that manages the network in the cluster.

containerRunTime (like containerd or docker): This is another important component of workerNode. He is the one responsible in creating and starting up a container inside the pod

IQ:

1. Someone was asked in an interview. What is the compiasition of your manifest file in k8s?

answer: Thank you for that question. our manifest file is composed of

KAMS

kind
apiVersion
metadata
spec

2. The same person what ask. explain tye difference between manifest files and helm chart

answer: Thank you for the question.

Manifest file are being written by an engineer or by someone but helm chart are been generated by the helm engine.

By default, helm chart contains all the deployment template and values that is needed for an application while in manifest files what is needed in an application to be fully and securely deployed would have to be defined manually in the file.

We can easily be able to deploy 3rd application like APM (prometheus and grafana) and efk (elasticSearch fileBeat and kibana) using helm chart than using manifest file because deploy such software will require about 1,500 lines of code which will be tideous.

helms makes it more easier to deploy application without resorting to the tideous use of writing a mainfest file manually.

helms is another good example of an infrastructire as a code.

You can store helm chart in helm repository and you can use them in the future for deployment

but you can store a manifest file in sourceCodeMangers like github.

when you install helm, it comes with

helm engine
helm chart
helm repo where helm chart a stored.

We can use helm to deploy 3rd party application and also a custom application

Let assume you want to deploy a microservice application like springapp for example, With manifest file, you will have to be write a manifest file for multiple deployment like

1. writing a manifest file for springapp
2. writing a manifest file for springapp-svc that will expose the application
3. writing a manifest file for configmap for authentication of the springapp
4. writing a manifest file for RBAC for authorization of permission of the springapp
5. writing a manifest file for db  because springapp is a stateless application that needs a database like mongaDb
6. writing a manifest file for db-svc that will expose the db application
7. writing a manifest file for secrete for authentication of the db
8. writing a manifest file for pv for  persistent volume
9. writing a manifest file for pvc for  persistent volume claime
10. writing a manifest file for HPA (horrizontal pod autoscaler) etc
10. writing a manifest file for sa (service account) etc
10. writing a manifest file for vpa (vertical pod autoScaler) etc
10. writing a manifest file for cas (cluster autoScaler account) etc

note: cluster autoscaller takes care or manage the number of nodes that can be create or be in a cluster. it may have minimum and maximnum number of nodes.

Metric-Server: 

this is a plugin that  can be install in k8s cluster that will enables us to be able to know what is going on in the cluster in terms of the resource utilization

This is a very tideous task writing these manifest.


 file but

- with helm chart, when you want to deploy springapp using helmchart by running helm create,

It will deploy the springapp because helm chart has all the necessary manifest files needed written by default. You may decide to edit or modify the file and the redeploy by editing the valuesFile that will fit your purpose if you so desire.

NOTE:

if you are deploying an application that requires a database, volume management is very imperative. This is where PV and PVC comes in in k8s. Example of a stateles app that requires a database is the spring-boot-app. database application is a stateful application.

List the k8s objects security that you know

namespace, role, rolebinding, clusterRole, clusterRolebinding, configmap. service, secrete

serviceDiscoery does dns resolution and als server as a load balancer or proxy. 

Types of service we have are

clusterIP, NodetPort, LB, externalName, Nginx ingress service, servicemesh

 VIDEO 144

What is EKS



EKS is an amazon managed k8s cluster that are used to deploy containerized applications. EKS can be able to do what we have done using KOPS.

HOW TO DEPLOY EKS CLUSTER

1. You can deploy it via GUI aws console
2. via commands like aws eks create-cluster

What can you use to deploy cluster?

1. We can use infrastructure as a code to deploy cluster like terraform

what is Terraform

from prof definition, terraform is a multi-cloud tool or software that use to automate the process of provisioning , securing and managing infrastructures in any cloud platform using file/code that mostly ends with .tf

HOW TO IMPLEMENT TERRAFORM WORKLOAD

1. 	init  = to initialize terraform dir and download providers plugins

2. create the tf script or codes

3. validate all the .tf files

4. terraform plan = this is used to plan the resources you want to provision

3. terraform apply = this is used to create resources

3. terraform format = this is used to re-arranged or re-organized the code that you have written

All infratsructure created by terraform are stored in main.tfstate file by default

7. terraform show = will list all resources created in terraform

8. cat main.tfstate  will list resources in the main.tfstate file 

9. terraform import = this will import and bring any rersources that was not created by terraform to now be managed by terraform.

9. modules: terraform supports the use of modules. this is a better way of writing terraform codes in a modular format that will make it easy to provision and manage resources in different environment with relative ease.

10. variables: variables and modules goes together.

11. outputs: you can permit authentication file like .kube/config file to be dispaly to us in output.tf file.

11. workspaces: terraform workspace create

the componeent of workspace as in git is

 default workspace
 dev
 stage

 Note: You can create dev and stage from default and it will inherit the all codes that has been written in default just like in git. all we is to modify or edit and put in the number of resources that we need.

 How many environments do you manage?

 We manage 5 environemnst. These are

dev, qa, uta, pre-pro, prod

HOW TO CREATE EKS CLUSTER USING TERRAFORM IS BELOW

It was forked from pro. under kubernetes notes, click on eks set up and then scroll down and you will eks cluster setup procedures

https://github.com/CLIFFOSA22/kubernetes-notes/tree/master/eks-setup-with-terraform



1. make sure terraform is installed on the server (ec2-instance) amd it is up and running

2. Try and first of all clone the code with the url below and use it to deploy eks cluster

https://github.com/CLIFFOSA22/kubernetes-notes

by running

git clone https://github.com/CLIFFOSA22/kubernetes-notes

cd into kubernetes-notes

rename the the eks-setup-with-terraform to let say eks-terraform by running

mv eks-setup-with-terraform eks

cd into eks

ls eks, you will see what is called variable file called variables.tf

3. ls eks, you will see what is called variable file called variables.tf

vi into variables.tf

You will see variable-key-pair-name called keyeks, check whether when you were creating your ec2-instance if the key pair name you created is keyeks by going to the ec2-instance, click on security, click on keyPair, if not please place your ec2-instance key pair name in variable-key-pair-name

3. Also go to the provider file called provider.tf by

vi provider.tf

You will see the region the file want to create the cluster is us-west-2, you can change the region to the region you want the cluster to be created, most likely should be in region your ec2-instance was created. For best practice, make sure that you do not have any other eks cluster running in that region.

4. since i want to provision an eks resource in aws, i need to be authorized by way of initializing with terraform so that terraform can be able to manage the files (Files that ends with .tf) by running

terraform init

It will download terraform plugin like provider plugin called provider.aws   etc

5. For us to provision the eks resource in aws, we must be authenticated.

If your ec2 server has an IAM role attached and the permission in the IAM role does not have the ec2FullAccess or admin access, you will not be able to provision any resources in that server. (if you want to detach any IAM role in your ec2, click on the ec2-instance name, click on action, click on security, click on detach, select No IAM, and then enter detach, and it will be detached. you can add IAM role by clicking on the v click on action, click on attach, click mon create IAM, select ec2AdminAccess and just follow the procedures).

Since we have IAM role that says admin access, then we can go ahead to provision the eks cluster. If there is no IAM, we can still go ahead but rest assured that IAM role is neccessary to manage access to your cluster and RBAC.

6. run

terraform validate

to validate all the files that is being managed by terraform. Files that ends with .tf

7. Before the installation of the eks cluster, you need to know the plan, what terraform will do to the .tf file and what is in the file that terraform wants to accomplished by running

terraform plan

8. You cabn now install eks cluster via terraform by running

terraform apply

If you want it ask you if yes or no, since we want to attain full automation process, we can run

terraform apply --auto-approve

It will install eks via the terraform file that end with .tf because we created a file that will be used to provision eks.

It will take up to 20 minutes for eks cluster to be installed because there lots of resources that would be installed along with eks. Like VPC, IAM, ASG, eks controlPalin, masterNodes, etc

QUESTION IN CLASS

What is the difference between .kube/config file and configmap?

ANSWEr: .kube/config file contains all the required aunthentication that is needed for a user to be able to access k8s cluster and also run workloads or task in the cluster.

While configmap is a file that written to reflect or capture some codes that developers did not include or define when writing their code. They are non-confidential data that is inputed into our application to make our application portable because of the environmental variables the configmaps contains.

ConfigMaps allow you to decouple environment-specific configurations from containers to make applications portable.

A Kubernetes ConfigMap is an k8s object that allows you to store data as key-value pairs.

Some application that maybe be written with a configmap is a database. 

We can also pass or define a volume in our application using configmap.

QUESTION IN CLASS

What are the functions of the different components in prometheus?

ANASWER. When you install prometheus, it comes with 

1. Prometheus-kube-state-metrics: This gathers object based metrics or data (gathers what is happening in your object like pod, service, deployment, replicaSet and then push the metrics to the prometheus server.

2. Prometheus-node-exporters: The gathers node based data or metrics (gathers data or happenings in the nodes and also push the metrics to the prometheus server.

3. Prometheus-server: This is the one the collects or scrape the metrics coming from kube-state-metrics and Prometheus-node-exporters and push them to prometheus-TSDBP

4. prometheus-TSDBP (time series database): This is has a database that stores all the metrics that was scraped by Prometheus-server and this metrics can be visualized using prometheus-UI or using another visualization software called  grafana.

This prometheus-TSDB becomes a dataSource to grafana

5. Prometheus-alertManager: This is the one that send alert to company admin or any engineer who have access to the company's alert details that was configured on the alertManager script.

Note: 

if you check the helm values file and template for prometheus, you will discover that  the Node exporter was deployed as a daemonSet and it is attached to all our nodes inside the cluster as soon as you deploy prometheus software.

You will also notice that a PVC was created along with the provisioning of prometheus usin helm because of the fact that there a database called TSDB that is present in prometheus so that TSDB will have sifficient volume to store metrics that was scraped by prometheus-server.

You can found our by running

kubectl get pvc -n (namespaceName)  

NOTE: 

We do recommend the use of Helm to deploy the Cluster Autoscaler. 
Helm is a package manager for Kubernetes applications.

VIDEO 145

DEVOPS END TO END, WHAT WE HAVE DONE SO FAR AND END IN DOCKER BEFORE ANSIBLE

Let us talk about DevOps foundation and software developemental life cycle

Dev-Ops Foundation
    SDLC (software developemental life cycle)
  Waterfall - R + 
  Agile
  Dev-Ops
    Develops  , Tests, Builds, qualification, backup  
     deploys & monitors applications 
     Applications are the output from Softwares development 


What is needed by developers to develop application

SCM: they need scm like below

   - Git/GitHub 
   create repo and organization or create teams and attached our team members to that team as a deveops engineer and add developers to that team so that they can be committing the codes to the github repo

   developers should always commit their code to githun repo once they are done coding. They should choose git rebase over git merge.

   developers should write a good file READme.md  so that it would be clear on what would be achieved with that code they are written.

We have branch security, we have secured it such that developers cannot directly commit code to the master branch. If any code is to be committed into master branch, that code or file must pass through a pull request so that so that other member of the team can be able to review the code before the code can be merged to the master branch.

Any file that not part of the code and we do not want git to track such file, such file should be moved to .gitignore file

   Other SCM are below

   SVN
   GitLab
   BitBucket
the developers also need IDEs


Software Testing: we use software testing tool like SELENIUM for unit test cases

- Unit test- cases- Selenium: we make sure that we install selenium in our environment that we automate unit test cases of the codes that was written

- CodeQuality: We use the below software for codeQualitry analysis.
sonarqube
SonarCloud = Serverless  
codeChecker

In our environment, we use sonarqube and sonarcloud.

with sonarqube and sonarcloud, we have a quality benchmark that we have created, we use quality bench mark to look at code quality, we look at the vulnerability level (if the code or application would be easy to hacked ), bugs in code, duplicate lines, code smell, code covergage, security hotspot. 

This bench mark is whatt we will use to create rules. we can create our qualityGates we can can say that duplicate lines (DL) MUST BE LESS THAN 5% , CODE COVERAGE must be greater than 90%, Bugs must be less than 1, security hotspot for vulnerablity shoudl be less than, code smell should be less than 4.

SonarCube [quality benchmark] [DL <5% /CC>90%/Bugs<1/SHS<1/SMELLS <4]

sonarqube has a  Scanner server whose components are

dataBase, webserver, searchEngine and computeEngine. 

That is  [ db / webServer () / searchEngine=ES, CE ].

-webserver: enable you to visualize the report on the web page and also act as a loadBalncer and a proxy server

-dataBase: used to store the code Quality analysis report

-computeEngine is used to calculate the percentage of the state of such analysis. It can tell you that the code smell is 1 or 2, tell you that vulnerabilty is 3% etc

SonarCloud = with sonacloud for codeQuality, you can achieve the same thing but it is serverless meaning, you do not need to create a server or ec2-instance in aws to install sonarcloud. You can only register a sonarcloud account and use it for codeQuality. it is called software as a service. 

aws cloud services is both IaaS, PaaS and Saas

Build: once the code passed the analysis, a build will be triggered by maven because we build on java based application and it is an open source software.

  Maven [Lifecycle  has 3 goals. these are= 

1. site/swagger (this does compilation by compiling java codes to create java classes which the JVM will understand ) 

example let say you have written a code like

print('hello') 

the above one line code was saved inside app.java file. the name of the code is now app.java

if you run mvn site,

It will covert that app.java to  app.class

the write code will change from print('hello') to something like binaries in the form of 01245312012  because that what JVM understands

2. clean goal= this we remove old maven builds or artifacts  

3. default = validate / compile / test  / package / sonar:sonar / deploy / install ] 

app.java=print('hello')   app.class [01245312012] JVM  
default = validate / compile / test  / package / sonar:sonar / deploy / install 

With running

mvn deploy

It will validate / compile / test  / package / sonar:sonar / install (install artifacts in maven localRepo) / deploy (upload artifacts to nexus)

If you run

mvn clean package

It will validate / compile / test  / package in the target directory of your project repo.

 ANt/Graddle
  MSBuild
Artifacts Repository:
  Nexus
  JFrog
Application Servers:
  Tomcat
  Wildfly/JBoss
   users ----> appServers  
   users  ---> WebServers/LB  ----> appServers  

WebServers/LB:

  Self Managed: They are self managed because you would create a server and then install nginx software in them. You would determine how traffic would be routed

  nginx /apache http/  = apt/yum install nginx httpd    
    Nginx Ingress
    HAProxy

  managed: You do not need an ec2-instance or server to provision this but you will determine and configure how traffic is routed.

    ELB = NLB / ALB   

CI/CD AUTOMATION:

Automation tools below are

-Jenkins (openSource version) / CloudBees Jenkins (enterprise version)

  What does Jenkins have to make sure automation happens?

  Jenkins comes with plugins. With the help of plugins, jenkins can exteend its functionality.

jenkins plugin like git, will enable jenkins to be able to pull code from github.

  -Bamboo
  -Travis CI
  -CruiseControl

APM1:  application performance monitoring are below
  Monitoring 
  NewRelic 
  Prometheus/G   ----> is what we use the most.
  AppDynamics 
  Nagios

Note: 

If anything is wrong with our infrastructure like maybe there is a ticket or alert that the ansible server has failed, the devops or infrastructure team will be the one to resolve it.

CONTAINERISATION:

Some of the containerization software are

  Rocket
  CoreOS
  Docker  - 85%

  docker has the following

-docker engine 

- docker has a cli= is docker. everything starts with docker when running docker commands

when you run docker build/run/ps/kill/image/  ,  it is being executed by docker engine.

      registry=dockerHub / ECR / Nexus/JFrog  
    docker build/run/create/start/pull/push/login/ps/ps-a/kill/images

    Dockerfile = List of instructions that will be executed 
                 orderly from top to bottom to create images.

                 This file contain key words
                 FROM / CMD/ENTRYPOINT/ RUN/ EXPOSE / COPY/ADD/ENV  

                 / CMD/ENTRYPOINT/ RUN/ are used to execute commands

                 RUN=  command are executed when creating the image. like "docker build"

                 / CMD/ENTRYPOINT/=  are executed while creating the docker container

                 ADD=  to copy files for both local and remote location

                 COPY=  to copy

                  FROM = is used to determine our base image

                  EXPOSE=  ussed to expose the containerPoints

    BEST PRACTICES FOR DOCKERFILE: 

  -use minimun RUN instructions to minimize the number of layers to create lightweight
  -use official images from official site to avoid vulnerabilities
  -use alpine linux where possible  
  -Avoid installation of unnecessary applications
  -use docker-compose file for deployment ( to deploy with docker). With docker compose file, it makes your file easy and re-usable instead of running multiple commands 

     docker build  

CONTAINER ORCHESTRATION: examples of container orchestrators are
   Docker Swarm
   Kubernetes
   OpenShift

Above can be used to manage and orchestrate containerized applications

Cloud PROVIDERS:
   AWS = 
   AZURE
   GCP
   IBM
   VMWARE 
   aws: 
    server = ec2, rds, 
    Serverless = Lambda, farget / 
    storage=EBS/EFS/S3 
    network: vpc / route53 / vpn /
    elb/asg/lc/lt  
    IAM / nACL / SG /    
Infrastructure as a code - IaaC:
   Terraform
   CloudFormation 


VIDEO 146

ANSIBLE

What is Ansible?

Ansible is a simple and powerful automation engine. It is used to help with configuration management, application deployment, and task automation.

Configuration Management: - 

some of the configuration management tools are below
Tools:

  Ansible 
  Chef - Pull
  Puppet
  SaltStack


Ansible - PUSH -- AGENTLESS 

Ansible are AGENTLESS. You dont need an agent when using ansible

We have what is called Ansible controlServer


Ansible manages a fleets of hosts or multiple hosts.

Let say host1. we have 50 appServers in host1

Let say host2. we have 25 webServers in host2

Let say in the third host group "host2", we have 45 dataBaseServers in host3

Ansible uses files and command. The files are called playBooks. The playBooks are written in yaml 

With the help of this playBook files, ansible can manage host1 "25 webServers", host2 "50 appServers" and host3 "45 dataBaseServers"

We are going to install ansible software in a server, to make it an Ansible server

We say Ansible is AGENTLESS because ansible do not require any clients software to communicate with the host.

We you are using a configuration management tool like CHEF or PUPPET, an agent is required to communicate with the host. A clients software is required or should be installed.

Ansible PUSHED configuration system  while CHEF or PUPPET is a PULL configuration system

CHEF or PUPPE what is called COOKBOOK.
IQ

Why did you use Ansible in your environment?

answer: We use Ansible in our environement because it is agentless. It does not require any client's software to be installed before it can be able to communicate with hosts whereas PUPPET OR CHEF requires an agent software to be able to communicate with the host. 

With the use of Ansible, work done  interms of installing other agent software is avoided and this reduce task or make use deplo with ease

WHAT ARE SOME OF THE TASK THAT YOU CAN USE ANSIBLE TO MANAGE

Ansible is a configuration management tool. some task that can be accomplished by ansible are as follows:

Servers: In your serve, you can use ansible to configure the below

 UsersMGT: who use have access
 FilesMGT
 ServicesMGT
 PackagesMGT


Ansible is an open source : that can do the below
  Configuration Management tool to configuration, deployment tool for deployment and 
  provisioning tool to create resources.

Ansoble is  maintained by Redhat

Whst is Ansible?

Ansible is an open source Configuration Management tool, deployment tool, and provisioning tool that is maintained by redhat

Ansible can manages what is called an HOST INVENTORY FILE

when ansible manages hosts, it is going to list all the hosts and each of the host will have an ip address

Host Inventory -- > it contains the host server details like the ip address, host name etc

let SAY ANSIBLE WANTS TO MANAGE THE BELOW SERVER in group which is a host

 Host groups are below

[dbservers]   group
[webservers]   group
[appservers]  group

[db]
172.31.42.91
172.31.42.90
172.31.42.47

[web]
172.31.42.91
172.31.42.90
172.31.42.47

[app]
172.31.42.91
172.31.42.90
172.31.42.47

It mneans we have 3 webserver, 3 appserver, and 3 dB-swerever

When you classify this servers in the way, we call it inventory file (static).

Static Inventory is a file where hosted servers are grouped. You can group all our servers in this file

ANSIBLE INSTALLATION

Let us launch an ubuntu server in aws and install ansible in it and call the server ansible. allow ssh and https in the security group

Let us launch an another server using aws linux 2and call the server webserver. allow ssh and https in the SG

Let us launch a 3rd REDHAT server in aws and call the server DBserver. select the SG that has ssh and https allowed.

Ansible installation in ubuntu
# ssh into your ec2-server you created
sudo hostnamectl set-hostname ansible
sudo adduser ansible  # You have to manage ansible as ansible user
echo "ansible  ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ansible  # To assign sudo right
sudo su - ansible
# To install ansible software run the below two commands
sudo apt-add-repository ppa:ansible/ansible  
sudo apt install ansible  -y 

To in ansible has been installed, run

ansible --version

if we have ansible installed, we can now use it to manage our host.

Ansible installation in different OS is in prof package managments that i forked


Ansible comes with a default config file and the ansible host file upon installation 

1. Ansible config file: is found in /etc/ansible/ansible.cfg
2. Ansible host file: is found in /etc/ansible/host

Default:
config file = /etc/ansible/ansible.cfg
host file = /etc/ansible/hosts  

The host file is where all the info of host server will be inputed. That is 

host file = /etc/ansible/hosts  will contain the below hosts group details like below

[db]
54.175.186.47 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem 
172.31.81.212 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem

[web]
172.31.87.249 ansible_user=ubuntu    ansible_ssh_private_key_file=/tmp/ansible.pem
172.31.82.102 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem
54.84.32.131  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem

[app]
52.91.235.177 ansible_user=ubuntu    ansible_ssh_private_key_file=/tmp/ansible.pem
172.31.87.249 ansible_user=ubuntu    ansible_ssh_private_key_file=/tmp/ansible.pem

The above information is called a host-inventory-file

when you install ansible in linux server as in the above, you need connection plugin to be able to log in into ansible. 

Our connection plugins is ssh for linux server.

Note: if ansible want to connect with a host that is not a linux server. Let say the host is a windows OS, we use localhost.

Examples of connection inventory are

ssh/winrm/localhost 

HOW TO CREATE INVENTORY FILE

When the ansible server wants to talk to host, he need to talk to the host server as an ansible user. and also, the host server will an OS to which the host server was installed. 

That is, the db server was installed in ec2-user linux server and so the host will be ec2-user

web server OS was ec2-user also

app server OS was ubuntu

We want to use a private key of the ansible to connect the ansible to its host server.

The inventory host file command in ansible will be

ServerPublicIpaddress ansible_user=ec2-user ansible-ansible_ssh_private_key_file=PathTossh-key
ServerPrivateIpaddress ansible_user=ec2-user ansible-ansible_ssh_private_key_file=PathTossh-key

You can copy the ssh-key you created somewhere in your desktop and paste it in a new created file in your linux environment and then try and chmod 400 the file to make your sshkey not to be publicly viewed

Therefore the inventory file will be written as below


[db]
54.175.186.47 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem 
172.31.81.212 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem

[web]
172.31.82.102 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem
54.84.32.131  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/ansible.pem

[app]
52.91.235.177 ansible_user=ubuntu    ansible_ssh_private_key_file=/tmp/ansible.pem
172.31.87.249 ansible_user=ubuntu    ansible_ssh_private_key_file=/tmp/ansible.pem

The above means that, mean our private key is been stored in tmp directory of your ansible server via cli and the name of the file is ansible.pem

[hostName]
hostPublicIpAddress ansible_user=amiType  ansible_ssh_private_key_file=PathToSSHKey
hostPrivateIpAddress ansible_user=amiType ansible_ssh_private_key_file=PathToSSHKey

We are just adding host private address also just to see that it can actually manage multiple host. 

Do not bother to add the host private address. only the public address is ok. That is 

[hostName]
hostPublicIpAddress ansible_user=amiType  ansible_ssh_private_key_file=PathToSSHKey


vi /etc/ansible/ansible.cfg

and paste the above 

When prof tried to edit the file by pressing i, it will not able to do so because the file stated readOnly Access.

Let us see who owns or who is managing the file by running

ls -l /etc/ansible/

You will see that it is root that owns the file. For we to be able to modify the file, we must make sure that the ansible user is granted permissions to have write-acceess by making ansible user to be the owner of the host file by running

sudo chown ansible -R  /etc/ansible/

The above command means that you changed the ownsership from root to ansible user that you created

if you run

ls -l /etc/ansible/

You will see that ansible user now have some level of permission

If you now re-run   vi into the host file by running

vi /etc/ansible/hosts

You will see that there is no more warning. you can now be able to modify the file. 

Inside the host file, scroll down to the end of line of code where there is no more line of code and then copy and paste the above written inventory file inside the host file. save-quit

For further illustration ,we mean that for ansible to be able to communication with all the host listed, there is a private key which is found in tmp diretory as inllustrated above 

Since you have already written that your private key called aws key pair is in your tmp directory, 

vi tmp/ansible.pem

copy your ansible ssh-key that you created in aws and paste it inside tmp/ansible.pem.
let us hide the key so that it can not be easily tracked. to remove it from public view by running

chmod 400 /tmp/ansible.pem


Ansible concepts:

1. Ansible commands: 

 ansible hosts -m module -a "df -f"

where host can be any name of server, -m=modules, -a=arguements, df -f = to check storage size

To check if four server are alive, we can run

ansible all -m ping

we can also check individaul host like db server to know if its alive or running by  running

ansible db -m ping

 ansible web -m ping

 You can even ping you local host. Your localhost here is your ansible server or ansible engine. The server that ansible is installed is your localhost. To check if alive. You can run

 ansible localhost -m ping

 If you ping the localhost here, it will not respond. it will fail. until you configure the config file of the ansible server.

 NOTE: Inventory host file is used to list and group servers. 

 it default location is   /etc/ansible/hosts

 We are done with ansible hosts file  = /etc/ansible/hosts file


ANSIBLE CONFIGURATION FILE

Let us modify our ansible configuration file = /ect/ansible/ansible.cfg

vi /ect/ansible/ansible.cfg

when prof via into this file, the file was not muh with few lines wneh it is supposed to be more than that. But it gave the url of prof ansible github where the configuraion file was. Prof said it is simply because he did not clone the repo. Prof copied the url that was given to him by the command and the paste it on web browser, the file came out and prof copied the file and paste it inside the (vi /ect/ansible/ansible.cfg) file. The command inside the file are all commented (# and so prof).

Prof is looking for a line that says ( host-key). What pro did was to scroll down and typed
/host_key  and the host_key we need searched out a bit above what we typed. he went to the main host_key and removed the comment sign

Once you uncomment, you are telling ansible to to search for host_key for authentication but to only check for ssh key of the localhost server or server where ansible software were installed.

If you now ping localhost sever which is the ansible server, you will see that it pinged. that is

ansible localhost -m ping

ansible localhost -m command -a "df -h"  ---> this is check the storage size of the ansible server.

-m command = comand module
-m ping module

To check the memory size of your ansible server, run

ansible localhost -m command -a free -m                       where  m= memory

2. modules: Types of module. These are
    ping module/ 

       ansible all -m ping

    command module / 

       ansible all -m commad -a "ping"
       ansible all -m shell -a "ping"
       ansible all -m commad -a "df -h"

    shell module/ 
       ansible all -m shell -a "ping"

    service/systemd / systemctl /  module:

       ansible db -m service -a "name=ssd state=restarted"
       ansible db -m systemd -a "name=httpd state=started"

    copy module: is use to coy a file
      ansible app -m copy -a "src=/home/ansible/app.war dest=/opt/tomcat/webapps/"
      ansible web -m copy -a "src=/home/ansible/index.html dest=/var/www/html/"

    yum / apt / package module:  

      ansible app -m yum -a "name=httpd state=present"
      ansible app -m yum -a "name=httpd state=absent"

 The command to check number of modules we have. That means the modules that can perform more task, just run

    ansible-doc -l   

Ticket001245
============

1. Install apache using the package module
2. cupy index.html into apache

1. Using package modules like the yum module to install some packges like httpd by running. That is

ansible web -m package -a "name=httpd state=present" -b   but the package is yum or apt, then

ansible web -m yum -a "name=httpd state=present" -b 

-b = means "become sudo privileged"  --> since we do not have sudo right. be have to include -b

but web is an ubuntu server which does not support yum but only apt.  we can use a general command for any package manager for this purpose and that package manager is "package"  therefore we run

ansible web -m yum -a "name=httpd state=present" -b  (Will install apache on a redhat host or server)

ansible web -m apt -a "name=httpd state=present" -b  (Will install apache on a ubuntu host or server)

ansible web -m package -a "name=httpd state=present" -b  (Will install apache on all kind of server)

ansible web -m package -a "name=httpd state=absent" -b  (Will uninstall apache on all kind of server)

Let us use systemd module to start apache

ansible web -m systemd -a "name=httpd state=started"  -b 

For you to know if apache has been started, run

copy the web ipadddress and paste it on the web. if you displayed, it means you have started apache.

- Let us write something on copy some content inside our ansible server into web.

let us create an index.html file. 

NOTE. it is only index.html that support apache. run

vi index.html

type in

Welcome To LandMark Technology

save-quit


Using copy module to copy index.html file into httpd that you have just created and started by running

ansible web -m copy -a "src=/home/ansible/index.html dest=/var/www/html/" -b 

/home/ansible/index.html   = home directory of the index.html you created

/var/www/html/ = the default home directory of apache

scr = means SOURCE in this context
dest = means DESTINATION

If you are running the above commands one after the other without putting them in a file, it is called IMPERATIVE APPROACH

Note: 

it is not good to manage ansible with commands only (with imperative approach). we should use DECLARATIVE approach by using a playBook to manage ansible (declarative approach)

3. playbooks:
============
Playbooks
  It's a configuration script written in yml. It contains plays and tasks.
  those tasks will be executed in the hosts.

 Playbook is written in yml.

PlayBook has a lot of play for a multiple host. That is 

- hosts:   ----> play  
- hosts:   ----> play  
- hosts:   ----> play  

 If you have a multiple host (server0), Each host will be attached to a play.

Let us use a playBook to install vim, install apache, start apache and then copy index.html in ansible server to db server while you are still in ansible server.

create a file called apache.yml

- hosts: db  
  become: true
  tasks: 
  - name: install apache
    yum: name=httpd state=present
  - name: start apache
    service: name=httpd state=started   
  - name: copy index file  
    copy: 
      src: index.html
      dest: /var/www/html/  
- hosts: all 
  become: true
  tasks:
  - name: install vim 
    package: 
      name: vim 
      state: present
- hosts: localhost 
  tasks: 
  - name: check system resources
    shell: df -h   

NOTE:

You will observer that the way will wrote the PLAY for apache installation was different from the PLAY for vim installation. Both can be written in both ways as below.

- hosts: db  
  become: true
  tasks: 
  - name: install apache
    package: name=httpd state=present
  - name: start apache
    service: name=httpd state=started   
  - name: copy index file  
    copy: 
      src: index.html
      dest: /var/www/html/  
- hosts: all 
  become: true
  tasks:
  - name: install vim 
    package: name=vim state=present
- hosts: localhost 
  tasks: 
  - name: check system resources
    shell: df -h   

Note: package is used to install packages in all type or kind of server. Whether in ubunto or redhat or etc

vi into apache.yml

paste the above playbook into the abek yml filena nd sae-quit

to check if the syntax used in writting the script is correct by running

ansible-playbook playbookFile --syntax-check

ansible-playbook apache.yml --syntax-check

if it displays  playbook:apache.yml    it means the syntax is correct. There is no syntax error

Note:

You can also a dry-run for your playbook to see what would be the result of your playbook when it is fully executed. Just to get know what resources would be created and also know if there would be an error with the playbook that you want to execute.  You can run the playbook dry command as

ansible-playbook apache.yml --check

The purpose of a dry run is to identify potential errors or issues in the program's logic or flow before it is executed. By going through the code step-by-step, programmers can catch mistakes and identify areas where the program may not work as intended. 

To finally execute the playbook. you run

Then run

ansible-playbook playbookName

ansible-playbook apache.yml

apache, vim would be installed and started in the host server. index.html content will also be copied in host server httpd. 

You can verify the content of the index.html by pasting the host ip address on the web, it will display.

Let us add more content to the file by

vi index.html   and add the below content

<p1>Welcome to Marsh Hill Homes, Port Perry</>

If you refresh the db server ip on the web, it will reflect.

Most playbook you will see will have a single play


Modules:

We have two type of modules

Core modules and Custom modules

  1. Core modules   -   

  If you want to see the ansible core modules, you can go to ansible website to search for ansible core modules. 

  more than 1000 modules in ansible website

     https://docs.ansible.com/ansible/2.9/modules/list_of_all_modules.html

     If you paste the above url in your web browser, you will see all modules you looking for.

NOTE:  If you want to manage k8s using ansoble, you can be able to do that by running

ansible-doc -i | grep docker

You can use the above command to know all the core modules that ansible can use to manager docker

Let  us copy the  "community.docker.dcoker_container"  from the result of the command

community.docker.dcoker_container  is the playbook that is ansible responsible in creating and starting up a container in docker

Let us run the below command to see how the playbook for the default creation of docker container is carried out by scrolling down to see example on how it was done using ansible playbook.

ansible-docker community.docker.docker_container

it will bring out the modules for k8s service. If you scroll down to examples, you will see how the ansible playbook defining modules was written for k8s service.

ansible-doc -i | grep k8s

You can use the above command to know all the core modules that ansible can use to manager docker

You will see all k8s objects here. let us check for service module, anyone that service is attached to is the service module

let us check the service module by running

ansible-doc kubernetes.core.k8s_service

it will bring out the modules for k8s service. If you scroll down to examples, you will see how the ansible playbook defining modules was written for k8s service.

2. custom modules -  custom modules is the module that you have written by yourself. A playbook defining modules which you have written by yourself.

If your work for a company like tesler. Tesler can decide to write their onw modules or write their own playbook.

What is the difference between ansible modules and ansible playbook?

ans:

Ansible modules are standalone scripts that can be used inside an Ansible playbook. A playbook consists of a play, and a play consists of tasks. These concepts may seem confusing if you're new to Ansible, but as you begin writing and working more with playbooks, they will become familiar

NOTE:

Ansible modules are written in python script

one of the  prerequisit for ansible installation is that you must have python installed

NOTE:
You may be asked in an interview if you have used ansible modules before.

The answer is NO. We have not had the need to create a custom module in our environment because most job we have got, the core modules is what we have been using in landmark to execute our project because it has been sufficient for us.

CLASS QUESTION

Can we use ansible to create resources instead of using terraform?

answer: in our environment, we use terraform to provision infrastructure and not ansible.

Most companies uses terraform for provisioning rather than using ansible because most terraform is simplier than ansible when it comes to provisioning because terraform has a state file that makes it easy to manage all the infrastructure from just a single file.


Which is better Terraform or Ansible?

Ansible and Terraform are two effective infrastructure as code (IAC) tools that have different functions. 

Ansible is better suited for setting and automating activities on individual node or host

while Terraform is suitable for provisioning and maintaining cloud infrastructure.

147  ANSIBLE 2 

Connections Plugins

Ansible is used to automate task and for automation to be possible, it need to connect to the host. Some connection plugins are 

   SSH ----> for connecting to linux host or server. SSH is the connection plugin
   WINRM  ----> for connecting to windows host or server.  WINRM is the connection plugin

Installation of ansible  

ansible home directory (AHD) = /etc/ansible  

Default: The defaul file in ansible upon installation are below

hosts File  = /etc/ansible/hosts   -----> is a file that contains all our hosts.
 Configuration file = /etc/ansible/ansible.cfg     

NOTE:

The default user name or the username that ansible would be using to manage every hosts or other servers would be ansible. You must create a username called ansible after ansible installation has been carried out on a server and that server becomes an ansible server and also called localhost

also make sure that you change the permission from being owed by ec2-user or unbuntu or other ami to be owe by ansible by running

sudo chown ansible -R  /etc/ansible/    -----> to be owned by ansible

also paste your ansible server ssh-key that you created in aws, in a new file and save it on the ansible server in mobaxterm and then run

sudo chmod 400 sshkeyFile.

NOTE:

When ansible wants to establish a connection, he can use

ssh-private-key or password or passworless (without password or ssh but just the ipaddress of the  host). That is

hosts 
[db]
52.91.235.177 ansible_user=ubuntu  ansible_ssh_private_key_file=/tmp/ansible.pem
32.222.02.23 ansible_user=ubuntu  ansible_ssh_private_key_file=/tmp/ansible.pem

172.31.87.249 ansible_user=kops  ansible_password=admin123

[k8s]
kOPSPublicIPAddress ansible_user=kops  ansible_password=admin123

NOTE:

Prof decided to try to talk to his k8s cluster that he created by pasting his k8s server ipaddress (masterNode) on his ansible server CLI to see if he can be able to access k8s cluster in ansible
environment. It was the server he installed kops.

When he ssh kops in ansible by running

ssh -i kops@kopsIpadsress

it says permission denied. His aim is to see if he can access k8s in ansible via k8s password.

He then went to k8s server and switch user to kops user (sudo su - kops) because he created kops user in that k8s server and then created a password in kops user by running

sudo passwd kops

and typed in admin123 as password

he needed to do password authentication on the kops server to authenticate the password that he just created by running

sudo vi /etc/ssh/sshd_config

scroll down to where it says passwordAuthentication and replace NO to YES and save-quit

Restart the sshd service to make the changes you just made permanent by running

sudo systemctl restart sshd

He went back to his ansible server and try again to access the server by running

ssh kops@kopsIpadsress

and ansible was now connected to kops server

when he run 

kubectl get pod

it was not able to access pod

He went back to his kops server and also run

kubectl get pod

he was not able to access pod also

he ran history command and grep for kops by running

history | grep for kops

To search for all the command that he has run on a kops user because he is searchin for a particular command and that command is kops export kubeconfig $NAME --admin.

He first of all deleted the .kube file by running

rm -rf .kube                to delete the .kubeconfig file

and then want to export another kubeconfig file by running

kops export kubeconfig $NAME --admin

he went back to ansible where kops has been connected and ran again

 kubectl get pod

He was now able show pods in his cluster.

Note: when he connected to kops in ansible, the ansible environment changed to kops environment and so he could rum the command

kubectl get pod

TO MANAGE K8S FROM ANSIBLE ENVIRONMENT

To manage k8s from ansible environment,  Prof exited from the kops environment to ansible environment and  vi into our default host file or inventory host file we created earlier by running

vi /etc/ansible/hosts

and paste the below command just after the last line in the file

[k8s]
52.201.217.3 ansible_user=kops  ansible_password=admin123

and then save quit

To know if ansible can communicate with kops server, he ran

ansible k8s -m ping

and it responded. 

Note. k8s is the hostName that we assigned to kops server inside ansible default inventory host file

ansible k8s -m shell -a "kubectl get po"

all the pods in k8s cluster diplayed

This means that we are able to querry or talk to our k8s cluster from ansible

This means that he can biow be able access k8s cluster in ansible environment by passing  a module -m and an argument -a

-a is the one that will container the command of the result you want from the other host.

We have a good number of ansible playBooks in prof github repo. I have forked the below in my gitgub

https://github.com/LandmakTechnology/AnsiblePlaybooks

HOW DO ANSIBLE ACCESS HOSTS PASSWORDLESSLY

How to configure passwordless access

Ansible to access hosts without using the hosts password.


Let us first of all just create a custom host file using new hosts or we can also use the once we created before but avoid the kops server. but i create new hosts

We can ALSO achieve access to hosts by creating a custom inventory host file and not default inventory host file in our ansible environment. 

let us just create 4 new ec2-user by doing the registration, create ssh key name as key31 in SECURITY GROUP allow ssh, allow https and http, all all traffic and then selecting number of instances 4 so that 4 instances will be launch at once. rename each of the instances as ks8, app, web, db. use the file configuration below to create your custom host file
 
[k8s]
52.201.217.3 ansible_user=ec2-user ansible_ssh_private_key_file=/tmp/key.pem 
[web]
44.202.29.54  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[app]
44.202.138.3 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[db]
3.92.199.98  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 

Let us create the custom file custom-hosts in our ansible environment and paste the above into it and save-quit

let us create a key in tmp directory and call it key.pem and then paste the key31 we created for all our server inside the key.pm file in our ansible environment and save-quit 

To make our key not view publicly , we run

chmod 400 /tmp/key.pem 

if you run

 ll

You will see that our key is no longer exposed. you will not see any read access in public but only in ownership which is ansible.

Let us ping the hosts to see if ansible can actually communicate with each hosts by running

ansible hostName -m ping -i customHostFileName

ansible db -m ping -i custom-hosts

You will see that it will communicate.

NOTE:

When you create a custom host file, you must passe a -i host because -i hosts indicate that there is a custom host file

let us ping all the server by running

ansible all -m ping -i custom-hosts

yOU will see that will CAN communicate with all the server.

in our ansible environment, Let us create a create a file called create-user.yml and paste the below
yml script

- hosts: all
  become: true
  tasks:
  - name: Create Ansible User
    user:
      name: ansible
      create_home: true
      shell: /bin/bash
      comment: "Ansible Management Account"
      expires: -1
      password: "{{ 'DevOps@2020' | password_hash('sha512','A512') }}"
  - name: Deploy Local User SSH Key
    authorized_key:
      user: ansible
      state: present
      manage_dir: true
      key: "{{ lookup('file', '/home/ansible/.ssh/id_rsa.pub') }}"
  - name: Setup Sudo Access for Ansible User
    copy:
      dest: /etc/sudoers.d/ansible
      content: 'ansible ALL=(ALL) NOPASSWD: ALL' 
      validate: /usr/sbin/visudo -cf %s
  -  name: enable Password Authentication
     lineinfile:
        dest=/etc/ssh/sshd_config
        regexp='^PasswordAuthentication'
        line="PasswordAuthentication yes"
        state=present
        backup=yes
     notify:
       - restart ssh
  handlers:
  - name: restart ssh
    service:
      name=sshd
      state=restarted

Some explanations of the components that the above file contains

1. hosts all --> means we want all the hosts present in the host file to be a partaker of this 
configuration. meaning we want to create ansible user in all our host
2. we want to deploy ssh keys to all our hosts so that it will be passwordless for ansible user to access all the hosts
3. setup sudo access to our hosts.
1. tasksName:  We want to create a devops ansible user, we are creating the user in our web server
2. user is the module. The name of the custom user is ansible.
3. expires: -1 = means this user account you want to create will not expire
4. password was assigned for the user as devops2020 so he can access his user acct with devops2020
5. we also grant sudo access to the ansible user we are creating because the default ansible user name called ansible already has sudo access
6. we copied the sudoer file into the new custom ansible user we are creating
7. we have another module called name that talk about disable password meaning you cannot have access to host with password
8. we also have a module called "lineinfile" this is used to manage files in a host
9> Th handlier will be activated when there is notifier. it handlier was notified to restart ssh. Once a package is installed, the Handlier would be notifried to start the package. If the handlier is not notified, the handlier will not start uo the package to be running

save-quit the above playbook and run the command 

ansible-playbook create-user.yml -i host

You will see that a user called ansible will be created ans susccessful.

LET US SEE IF OUR SSH KEYS HAS BEEN DEPLOYED.

Let us place a comment on the part that requires keys in each of the hosts. That is

[k8s]
52.201.217.3 #ansible_user=ec2-user ansible_ssh_private_key_file=/tmp/key.pem 
[web]
44.202.29.54  #ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[app]
44.202.138.3 #ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[db]
3.92.199.98  #ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 

let us ping all the host to see if it will respond by running

ansible all -m ping -i hosts

You will see that it all pinged.

This means that ansible user can querry or communicate with each of the hosts without a password because shh keys has been exported to all of our hosts.

With ansible, we do not need to export our keys manually. 

Ansible will automate the process of exporting ssh keys to multiple hosts

USING A PLAYBOOK WITH LOOPS TO INSTALL PACKAGES.

Playbooks with loops

- hosts: localhost
  become: true
  tasks:
   - name: Install list of packages
     yum:
       name: ['nano', 'wget', 'vim', 'zip', 'unzip']
     when: ("ansible_distribution" == "RedHat")

or


- hosts: localhost
  become: true
  tasks:
    - name: Install list of packages
      yum: name='{{item}}' state=present
      with_items:
       - nano
       - wget
       - vim
       - zip
       - unzip  


The two playbook loop file are both the same. You can use any of the playbook.

Let us just use the first loop file by creating a file called loop.yml and paste the first playbook inside it.

If you run

ansible-playbook loop.yml

You will that it failed. This is because prof installed his ansible server or localhost on an ubuntu server and so, ubuntu server does not support yum but only support apt or package. you can re-write the loop playbook as below

- hosts: localhost
  become: true
  tasks:
   - name: Install list of packages
     apt:
       name: ['nano', 'wget', 'vim', 'zip', 'unzip']

and re-run

ansible-playbook loop.yml

You will see that all the packages has been installed in the localhost or ansoble server.

Another example is when you want to install packages using loop to just one host. like installing the packges in db host. The loop playbook will be in form of below:

- hosts: web
  become: true
  tasks:
   - name: Install list of packages
     package:
       name: ['nano', 'wget', 'vim', 'zip', 'unzip']

 This will install the packages on the host called web


Playbooks with Conditions:

This a playbook that has a condition attached to the file. like the word "when"  as see below

- hosts: localhost
  become: true
  tasks:
   - name: Install list of packages
     yum:
       name: ['nano', 'wget', 'vim', 'zip', 'unzip']
       state: present
  when: ("ansible_distribution" == "RedHat")

  ---

- hosts: dev
  become: true
  tasks:
    - name: Install list of packages
      package: name='{{item}}' state=present
      with_items:
       - nano
       - wget
       - vim
       - zip
       - unzip  
     state: present

You can copy and paste both in one file called loop.yml and save the file and the run

ansible-playbook fileName -i hosts --syntax-check

to see if there are any syntax error

ansible-playbook loop.yml -i hosts --syntax-check

It will install that packages in your localHost called ansible and also in your host called dev

You can use both method of playbook to install packages. I think the first one is more simplier. 

The first playbook has a condition  (when:) which illustrates that you can only install the packages on localHost (ansible server) whem the ansible-server or ansible-instance distribution is Redhat. This means that if the ansible distribution or if the ec2-server that ansible was installed on is not redhat, packages installation will not be possible. This is why using "package" module is preferrable for all host distribution when installing packages using PLAYBOOK.	

The second playbook is also used to install list of packages

NOTE: If we have multiple hosts that has different distribution in one play book for the purpose of installing packages, we can manage them by using the package module.

NOTE: If you have become: true on your playbook, it means that the playbook has privilege escalations or has sudo access.

NOTE: Loop playbook can be used to carry a certain number of common task. as shown above.

HOW DO YOU KNOW YOUR HOSTS AND LOCALAHOST DISTRIBUTION?

 You can know your server distribution by checking your server is setup and then scroll to find your server distribution setup.

 1. For localhost----> you run

 ansible serverName -m setup -f hosts

 ansible localhost -m setup -f hosts   or  ansible ansible -m setup -f hosts

 because our default server name is ansible.

where hosts is the name given to our custom hostFile or custom inventory file. you must passe -f to indicate that it should read from the custom file.

2. If the host is dev called dev----> you run

 ansible serverName -m setup -f hosts

ansible localhost -m setup   or  ansible ansible -m setup 

because our default server name is ansible.. Th above will read from your default server. so the server name is very important.

If you run the above command, it will read how the dev and localhost is setup. You can now find the distribution by scrolling down to look for the distribution.

You can create a condition from the server setup if need be.

UNSTALLING PACKAGES.

You can use playbook to uninstall packages. Let us uninstall the packages listed on loop playbook above by reconfiguring the file loop.yml by vi into loop.yml and re-editing "the present to absent" That is

- hosts: localhost
  become: true
  tasks:
   - name: Install list of packages
     yum:
       name: ['nano', 'wget', 'vim', 'zip', 'unzip']
       state: absent
  when: ("ansible_distribution" == "RedHat")

 This will uninstall packages in your localhost with redhat distribution


ANOTHER EXAMPLE;

Let us create a playbook named app.yml and paste the below into it
=======
- hosts: dev
  become: true 
  vars:
  -     
  tasks: 
  - name: 1. install httpd   
    yum: name=httpd state=present  
  - name: 2. start apache
    service: name=httpd state=started   
  - name: 3. copy index file 
    copy: src=index.html  dest=/var/www/html/index.html
    ignore_errors: true 
  - name: 4. enable password authentication
    copy: src= dest=  
    notify:
    - restart sshd 
  - name: 5. change the default port for apache 
    copy: src= dest=  
    notify:
    - restart apache  
  handlers:
  - name: restart apache   
    systemd: name=httpd start=restarted
  - name: restart sshd
    service: 
      name: sshd 
      state: restarted  

THINGS TO NOTE ABOUT A PLAYBOOK

1. If the first task failed, other task will not be executed.

2. module: it is a tool that will execute a task: Example is yum, shell, copy etc. we can use the shell module to execute a port change in a playbook. we can also use a copy module to copy a file that has the correct port that we want. we can also you copy module to copy a file that has password authentication already enabled.


3. Notify: Notify is the one that inform the handlier on instruct the handlier to carry out a task due to a change that occured.

4. Handliers: is a form of task that have executed if something has changed. 

Example, we may need to restart our apache, if there is a configuration that was carried out in our apache server, like changing the default port for apache to another port number. 

we may need to restart our apache, if there is any change that occured in our ssh service like disabling or enabling password authentication.

NOTE: become: true   in a playbook is a condition. it means the host must have privilege escalation or sudo access.

TAKE NOTE OF BELOW

Let us say you are trying to start apache and it failed but you still want other task to run. You can include a condition on the playbook like 

ignore_error: true  

under what should not stop other task from running as shown in the above playbook

It means that even if apache refused to start, it should not break the playbook and should not deprive other tasks from running. 

ignore_error: true is very good for automation.

Example of a simple playbook with handliers is below

apache.yml
---
- hosts: all
  become: true
  tasks:
    - name: Install Apache HTTP Server
      yum: name=httpd update_cache=yes state=latest
      notify:
        - Start HTTP Server
  handlers:
    - name: Start HTTP Server
      service:
          name=httpd
          state=restarted

To explain the above PLAYBOOK.

1. to install appache in all hosts in the ansible hostfile or inventory host file
2. to use yum to update and install the latest version apache server
3. notify will instruct handliers to restart http server

If apache is not installed, the handlier will not run

SOME AINSIBLE COMMANDS

 ansible-playbook playbookFile --syntax-check    to check syntax error

 ansible-playbook playbookFile  --check  = to do a dry run to see how the outcome will look like

 ansible hostName -a "curl 44.195.23.204"  to curl

 ansible hostName -a "df -h"  to check storage size of our server from the default inventory hostfile

 ansible hostName -a "df -h" -i hosts to check storage size of our server from the custom inventory hostfile

 ansible hostName -a "df -h" -i hosts

 The command module is the default module for ansible. like the commands below

 ansible serverName -a "df -h" -i hosts

 ansible serverName -a "df -h" 


VARIABLE

To know how define or passe or include a variable on the playbook. Example of playbook with variable is below

vars.yml 
---
- hosts: db
  vars:
    name: From Playbook
    password: dev@123
  tasks:
  - name:  demo vars
    debug:
      msg: "{{name}}"
  - name: vars demo2
    debug:
      msg: "{{password}}"

 You will observe that "name and password" where entered as variable. the variable where entered under message (msg). Meaning, you can input any "name and passowrd" you wish. It is portable playbook because it can be re-usable. it is not hard-coded.

 "{{input something here}}"  is how you define variables in yml

If you save the above playbook with the name var.yml and then run

ansible-playbook var.yml -i hosts          where -i means should be read from custome inventory file

You will see that a vars name and password for db server will be replaced by  "From playbook "  and "dev@123"

the playbook can be re-usable because of the variable configuration inside the playbook in terms of the name and password.

Note: We should not have our passowrd defined or expose as it is on the playbook above.

If you want to change the password that was configuration as variable in the playbook to be replaced by another. You can run


ansible-playbook playbookName --extra-vars "password=passwordName"

ansible-playbook vars.yml --extra-vars "password=test"

if we run the below

ansible-playbook vars.yml -i hosts              

it will read or display the name and password that was defined on the 


TYPE OF VARIABLES

1. RunTime variables
2. playbook variables = this is a variable that is defined in your playbook
3. host_vars variables 
4. group_vars variables

HOST VARS VARIABLES

Example

Le us say you have a custom inventory host file as below

[kops]
52.201.217.3 ansible_user=kops ansible_password=admin123
[web]
44.202.29.54  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[app]
44.202.138.3 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[db]
3.92.199.98  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 

The was we defined the password above on this custom inventory file is not in a secured way. 

We can re-defined it in a secured way by not exposing on writing out the password as shown below

[kops]
52.201.217.3 ansible_user=kops ansible_password="{{mypasswd}}"
[web]
44.202.29.54  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[app]
44.202.138.3 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[db]
3.92.199.98  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 

my password is now a variable that can re-usable

Let say the name of the custom inventory host file is hosts.

Let us ping the host called kops by running

ansible hostName -m ping      to read from default inventoryFil

ansible hostName -m ping -i InventoryHostFile     to read from default custom inventoryFile

ansible kops -m ping -i hosts

where -m = modules, is the guy that does the task. the action guy.

If you run 

ansible kops -m ping -i hosts

It will say undefined variable password

HOW TO DEFINED THE UNDEFINED VARIABLE IN THE INVENTORY FILE ABOVE

You can define the variable password above by passing the password using the command

ansible kops -m ping -i hosts --extra-vars="password=admin123"

This is because a password admin123 has been created for kops before

GROUP VARS VARIABLES

Let us create an ansible directory for 

group_vars/, db_vars/, app_vars/, all_vars/,   as a directory names. 

Knowing that the default home directory for ansible is 

/etc/ansible/hosts    

let the hosts be replaced by the directory. therefore, to create directory, it will pass the absolute path of the ansible by running


 mkdir /etc/ansible/group_vars/
 mkdir /etc/ansible/db_vars/
 mkdir /etc/ansible/app_vars/
 mkdir /etc/ansible/all_vars/


 paste the above directory in your ansible environment and press enter. if you

 ls -l
 
 you will see that group_vars/, db_vars/, app_vars/, all_vars/ has been created 



 Let us assume that the default hostfile was below

[web]
44.202.29.54  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[app]
44.202.138.3 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[db]
3.92.199.98  ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem 
[k8s]
52.201.217.3 ansible_user=kops ansible_pwd=test

create a file called all.yml inside  group_vars/ by 

cd into group_vars/

and run

vi group_vars/all.yml

inside the file, write

mypasswrd= admin123
pwd= test

save-quit.

Let us ping k8s by running

amsible k8s -m ping 

It will ping because you have already save the value of mypasswrd and pwd as a file in your ansible environment.

With these, our variable is not still secured. because our password is still exposed and vulnerable.

ANSIBLE VAULT.

we can use ansible vault to secure a confidential data like passowrd by encrypting the password.

we can also use ansible vault to decrypt confidential data like password

Ansible vault is used to store confidential or secretes data in ansible.

We usually use ansible vault to emcrypt confidential data that is ain a file.

Hashicorp vault is a close associate of ansible vault also used to store confidential data.

IQ

How do you store confidential data in your environment?

answer: we use ansible vault to store confidential data in our environment.

We usually use ansible vault to emcrypt confidential data that is in a file.

Example

1. Let us encrypt all.yml file to make it a secured file by running

andible-vault encrypt /etc/ansible/group_var/all-yml

It will ask you for vault password, enter the password you which to use, and then re-enter the password for confirmation. it will say successful.

if you cat the file all.yml by running

cat /etc/ansible/group_vars/all.yml

You will see that the data or passwords that was easly read in the file has been encrypted inform of a token that would be difficult to memorise or guess.

Since we have just encrypted the confidentia file, let us try to ping k8s by running

amsible k8s -m ping 

You will see that it will not ping, it will say no vault secret found.

For me to be able to ping k8s host successfully, we run

amsible k8s -m ping --aks-vault-pass

It will ask you to enter the vault password. Enter the vault password you used in encrypting the file

as soon as you have done that, it will be able to ping

example. Let us see the pods in our k8s cluster from ansible environment by running

ansible k8s -m shell -a "kubectl get pod" --ask-vault-pass

It will ask you to enter the vault password. Enter the vault password you used in encrypting the file and you will see that it will list the pod in your k9s cluster.

If you want a situation whereby you do not want the vault to ask for passpwrd for the benefit of automation, You can also just create a vault password file.

Let say the vault password is kim123

You can create a file called v-file and paste kim123 inside the file and save-quit.

if you run the command

ansible k8s -m shell -a "kubectl get pod" --vault-password=v-file

it will list pods in k8s cluster without asking for vault passwrd

ansible k8s -m ping --vault-password=v-file

it will ping host called k8s without asking for vault passwrd

2. DECRYPT AN ENCYPTED DATA LIKE ENCRYPTED PASSWORD

We can decrypt an encrypted file containing password by running

ansible-vault decrypt /etc/ansible/group_vars/all.yml

if you cat the file, you will see that it has show the way it was before. 

3. it can be used to view confidential file

ansible-vault view /etc/ansible/group_vars/all.yml

3. it can be used to edit confidential file

ansible-vault edit /etc/ansible/group_vars/all.yml

3. it can be used to rekey confidential file

ansible-vault rekey /etc/ansible/group_vars/all.yml

3. it can be used to create a confidential file

ansible-vault create  /etc/ansible/group_vars/all.yml

NOTE:

Type of  ansible hostfiles

1. Static hostFile. This comprises of 

a. Default host file or default inventory file

b. Custom  host file or default inventory file

2. Dynamic Inventory  

VIDEO 149 ANSIBLE 3

what si a playbook?

A playbook is a yaml file that is used in ansible for the configuration of infrastructures. The yaml comaprrises of the following below.

Playbook composition: playbook comprises of 

1. plays=hosts
2. task
3. notify
4. handlers
5. modules
Examples of modules =  types of module will have are below:
        yum / file / apt / copy / template / script / lineinfile
        package / shell / setup / command / ping / authorized_keys
  6. loops
  7. conditions= like (become: true, ignore_error: true   etc)
  8. variables= like dynamic variabvle contenst  "{{pwd}}"
  9. tags: to know the version number or tag number of the task
  9. roles   

plays = dev: play book can comprise of different plays. (plays means  hosts like appserver, db-server, webserver, etc). it means you can have different hosts in just one playbook as show below.

 hosts: app-server
 hosts: db-server
 hosts: web-server  etc

 1. hosts: app

 2. become: true. become is a condition. Since the beccome condition is placed after hosts, this means all the task must have sudo access to be able to run.

3. gather_facts: "false" : This means to gather facts about the operating system of the host. You can decided. 
You can enter "false", if you dont want it to gather facts and enter "true" if you want it to gather facts.
The module for gather_facts is the setup module

Note: in a playbook, gather_facts is another condition, that is always true by default, so if you do not defined gather_facts in your playbbok, it will be true by default and that may take a long time for the command to run especially when you have multiple host in your playbook, it will begin to gather facts about the hosts and can slow things down abit.
You may wnat to defined gather_facts: false  so that it can run faster

 4. tasks: Each task we have a list of what we can do
    -name: install apache
     package: name=httpd state=latest
     tags:   to assign a tag name or number to the task for quick recognision purpose, like a version
     -install

5 notify: start apache

6. ignore_error: true. This is also a condition. ensure other commands run on the playbook even when apache filed to install 
    -name: copy index.html
    (module: template: template module is used to copy file and to copy dynamic file. it is better than copy module)
    template: src=index.html dest=/var/www/html/

    tags:   to assign a tag name or number to the task "copy index.html"
    -copy

    -name: (dynamic content: we want the content to be dynamic in the form of variable. we can use the debug modules)
     debug:
     msg: "{{index}}"

     tags:   to assign a tag name or number to the task "dynamic content"
    -index
7. handlers: handlers can also be a list
    -name: start apache
    service: name=httpd state=started
     tags:   to assign a tag name or number to the task "start apache"
    -start

  let us re-arrange the above properly without the expalnations

     hosts: app
     gather_facts: false
     become: true
     tasks:
     - name: install apache
       package: name=httpd state=latest
       notify: start apache
       ignore_error: true 
       tags:
       - install
     - name: copy index.html
       template: src=index.html dest=/var/www/html/
       tags:
       - copy
     - name: dynamic content
       debug:
        msg: "{{index}}"
       tags:
       - index
     handlers:
     - name: start apache
       service: name=httpd state=started
       tags:
       - start

The arrangement of the yml file is very important. If it is not properly arranged, thee will be a syntax problem. This is why it is important to always run a syntax check to see if it is properly arranged.

    copy the above and paste in a new file by running

    vi apache.yml

    paste the above and save-quit

  Let us create an index.html file inside a directory called app by running

  vi app/index.html

  paste the below into the index.html

  <p1> welcome to landmark </p1>

Note: playbook works from tasks to bottom.

We will need to check if there is any syntax error  by running syntax check as below

ansible-playbook apache.yml  --syntax-check

we can also execute a dry run to see how it when be after the command is finally executed. the different task that will be runned but would not be enforced, by running

ansible-playbook apache.yml  --check  =# dry run

We have different task to be executed in a playbook and you want to be sure of which task you actually want to execute. Maybe you change you mind to execute a specifc task in the playbook. You can run a playbook step command like

ansible-playbook playbookName --step   it will run from a default inventoryFile

It will run each task step by step that is in your playbook. If you do not want a task that display, you can type in NO and you want the task that display, you can type in YES.

When prof run the playbook step command, it shows gathering facts, prof press "n" as NO because he didnt want it to gather facts. You can do same for any task you do want it to execute.

IQ

What module is responsible in the gathering of facts in ansible.

Answer: the answer is SETUP module

The setup modules will tell you how your operating system for your host is. The name of your operating system, the size and other info.

IQ

HOW IS A SPECIFIC TASK RUNNED OR EXECUTED IN A PLAYBOOK?

answer: 

There are different options that you can use.

1. ansible-playbook apache.yml  --step   this run task step by step and allow you to type in YES or NO. THis may be tideous if you have multiple tasks that may be up to a 100. it will be time consuming and tideous to be typing yes or no.


2. Assign tags to task
 
 Let us place a tag on each task we have in our playbook as already done above. 

 Let only install apache as task we want to do in the playbook above as an example.

 the tag name for the task "install apache" is 'install'   we can now run the command

 ansible-playbook apache.yml  --tags 'install'

You will see that it will install apache. Note: since there is a notifier that will notify handler that appache has been installed and the it should start apache as defined in the playbook, handler will run. and so apache will be started.

I think assiging a tag to all the tasks will be it more easier to run a specific tasks in a playbook than a step command

You can skip tags that is assigned to task.

Let us skip the tags called install and index to make sure that they do not run, since we already have install tags running already. We want others to run also by skipping index tags and install tags. we decided to also skip install tags because its already running. by running

ansible-playbook apache.yml  --skip-tags 'install,index'

once you run the command, it will tell you that there us undefined variable called dynamic content which as defined on the playbook. There was a variable content on that task and the content was "{{index}}"  we need to define this variable by saying what that variable means.

You can defined "{{index}}"  as -e "index=devops"

You can defined "{{index}}"  as --extra-vars "index=devops"

You can now skip both install and index tags as below

ansible-playbook --skip-tags 'install, index' -e "index=devops"

ansible-playbook --skip-tags 'install, index' --extra-vars "index=landmark"

You will see that both tags will be skipped and other will run.

Let us skip only install by running

ansible-playbook --skip-tags 'install' -e "index=devops"

You will see that it will copy the content inside your index.html file to /vars/www/html/

SOME OTHER ANSIBLE PLAYBOOK COMMANDS ARE BELOW

 ansible-playbook apache.yml  --list-hosts

1. ansible-playbook apache.yml  --step
2. assign tags to tasks 

ansible-playbook apache.yml  --skip-tags 'install,start'
ansible-playbook apache.yml --tags 'install'
ansible-playbook apache.yml --tags 'copy'
ansible all -m setup

ansible-playbook tag.yml  --skip-tags 'install,index' -e "index=Landmark"
ansible-playbook tag.yml  --extra-vars "index=test"

IQ

What is verbose mode in K8S?

Answer: verbose mode in k8s (let say you want to create a namespace called dev) is written as 

kubectl create ns dev -v=7


VERBOSE MODE IN ANSIBLE

ansible k9s -a 'free -m'   there will show you the un-used resources or free in your k8s host

ansible ks8 -a 'df -h'  there will show you the used and un-used resources or free in your k8s host

ansible ks8 -a 'df -h'   is same  with ansible ks8 -m command -a 'df -h' 

command module (-m command ) is the default module and so if you did not defined a module, command module will run. 

To check ansible in verbose mode, you run

ansible ks8 -a 'df -h' -vvv

The above is show more logs and more details about the command you run.

Verbose mode show more details, logs and info about the task that you have runned.

Verbose mode can be used in troubleshooting problesm in ansible or any environment.

you can also run ansible playbook in verbose like

ansible-playbook apache.yml -v 

It will give you details about what you just runned.

if you add more v like

ansible-playbook apache.yml -vvvvv

It will give you more details and more logs. The more you add v, the more details it will display.

You can use a verbose mode to troubleshoot a problem in ansible because it will give you more insight of what is possibly wrong with the commands that you runned and possible way to fix the problem.

NOTE: You can used the ignore_errors=true condition to make you playbook to run regardless of any errors upstream by placing ignore_errors=true the just immediately after or below the line that has the error so that other line downward will run regardless.

IQ

1. What is the difference between tasks and handlers?:

Answer:

Tasks are executed by default from top to bottom.

Some tasks has to notify handlers. 

Handler will be executed only if the tasks changes.

2. What is difference b/w copy and template module? :  

 answer; 

  template module copy both static and dynamic file content 

  copy modules copies only static file content 

Static files= are files that does not change. It remain unchangeable. input and output remain the same

Dynamic files= are files that does change. It can be changeable. output may change.  Exampole of dynamic files are files with variable contents or variable commands


Which module is used to gather facts in ansible playbook?

answer; setup modules

We can refer variables in ansible-playbook  using this ginger template -- {{}} 

For example

{{ class }}

ANSIBLE ROLES

Ansible role --> 
  It's is a set of tasks, handlers, variables, files and other 
  components organised in a predefined structure to configured specific 
  requirements. It is easy to understand, maintained and shared.
  Roles are used in ansible-playbooks.

You can have a playbook of 1000 lines and reduced it to 4 lines. Like below

- hosts: all
  become: yes
  roles:
   - httpd

If we create a role in ansible, it will come out with the netire directory structure. It will organize the file.

Role in ansible aare similar to helm chart in k8s.

CREATE A ROLE

WE use ansible-galaxy to create a role in ansible

Let say you want to install apache, some files content will be copied. ansible-galaxy can organize and put together all ths files need for the installation in a directory.

let say you want to install https, you can use ansible-galaxy role to put all the needed files together in a directory by running

ansible-galaxy init httpd

if you ls

You will see that a directory called httpd will be created as a result of the above command

Prof. wanted to run a tree command of the https directory that was created as a result of the above role command using ansible-galaxy, it said COMMAND NOT FOUND. It emans tre is not installed in the ansible environment.

According to him, tree can be installed in 2 ways

1. Since we are in ansible environment. it is good we should use ansible to install tree by running

ansible hostName -m moduleName -a "name=serviceName state=latest/present/absent"  

That is

ansible localhost -m yum -a "name=httpd state=latest"

we use localhost since we want to install it in our ansible server because we are in ansible environment. You can also install it other hosts by defining the hostName.

if it is an ubumtu server, you use apt or use package as module that can support all linux distributions

ANSIBLE PLAYBOOK roles makes it easier to group multiple tasks together in one playbook.

Roles take possession of tasks and other command and files that will make the task to run. 

Hanlers is not part of the roles. Handler can only be effective when there is any change at the top which would be notified by notify. Notify is part of the roles.

It may say, you need a sudo access to install tree if you are not the root use. 

To have sudo access, you will pass the condition -b on the command. That is

ansible localhost -m yum -a "name=httpd state=latest" -b

2. sudo yum install tree


He cd into httpd directory and then run the tree command, 

tree

you will see that it will display

httpd/
├── defaults
│   └── main.yml
├── files
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── README.md
├── tasks
│   └── main.yml
├── templates
├── tests
│   ├── inventory
│   └── test.yml
└── vars
    └── main.yml

Above is the role that was created when we run

ansible-galaxy init httpd

In the above, you see that it has automatically organize the ansible role for apache installation 

i HAVE JUST FORKED PROF ANSIBLE ROLE AND ANSIBLE PLAYBOOK REPO TO MY GITHUB ACCOUNT AS BELOW

https://github.com/LandmakTechnology/AnsiblePlaybooks
https://github.com/LandmakTechnology/AnsibleRoles

We acn clone the repo into our environment and use it for yourself.

If you look at the above role, you see that it has directory and files, you can cd into each of the directory and then to the file and write your content of command inside the files. For example

cd into tasks, and that task a file called main.yml, vi into the file and the write in the write commands apache installation tasks, like

name: "install appache"
yum: "name=httpd state=latest"
name: "copy https configuration files"
template: src=index.html dest=/etc/httpd/conf/

You will see tasks content inside ansible role repo you have cloned or forked in my github. copy the task content and paste it above.

PLEASE NOTE

TO UTILIZED THE ROLES IN GITHUB 

You have to clone the role repo that i forked in my github to your ansible environment.

You will see many roles inside. cd into roles and ls

You will see so many roles inside inform of a directory. like tomcat, commom, java and so many others.
You will also see a playbook called site.yml

Inside tomcat role, cd into it, you will see the role on how to install tomcat. same with others.

Note: this roles organizes or produces what is needed to run a task.  You will see a file called site.yml, that is the playbook.

all you need to do is to vi into the playbook file called site.yml and write in the name of the role that you want to embark on, under roles. 

You can use use one playbook for multiple hosts or plays and multiple roles. like example

host: web
become: yes
roles:
- apache
host: app
become: yes
roles:
- tomcat

above will install apache in web and tomcat in app.

Make sure that your hostfile or inventory file has web host and app host. if there is non, create one inside. You can verify by running

/etc/ansible/hosts            ------> this is your default hostfile

also make sure that the name of the roles are correct. and each role contains files or commands that are needed for it to run.  All this info can be found in the repo that you cloned.

After you have done what is needed, you can now run the command


remember that the name of the roles is the name that was used to saved the role.

Pof was able to install and start apache and tomcat using ansible roles-playbook.

In the repo that was cloned, there was also role called group_vars, inside that dicrectory, we have a file called appservers, in that file, we have username and password for tomcat.

The above credentials was used to defined a dynamic variable file in tomcat template. if you cd into tomcat role, cd into tomcat template, you will see a file called toocat-user file. There is a line inside that template file that was not commented. 

in the line, the username and password for tomcat was writtene in dynamic variable.

after all configuration was completed, you can now run

ansible-playbook site.yml        it will run from the default hostfile.

You will see that both apache and tomcat was installed and started.

As soon as tomcat was created, it was access and login on the browser with those login credentials

You will also be able to access the https online because there was an index.html file inside the apache role that is inside the repo that we cloned from github.

You can vi into the index.html file and modify the content if you wish.

NOTE:

every task has a name and a module. Thst is

tasks:
-name:
 moduleName

You will see all the roles of any software that is prof. repo that we cloned.

The roles, always contains

task, modules, notify

Ansibvle roles just create an empty directory structure for you to input all the files and info needed. like the Roles for apache installation below

httpd/
├── defaults
│   └── main.yml
├── files
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── README.md
├── tasks
│   └── main.yml
├── templates
├── tests
│   ├── inventory
│   └── test.yml
└── vars
    └── main.yml 

This structure is called ansible galaxy

VIDEO 150, ANASIBLE

ANSIBLE DYNAMIC INVENTORY.

Dynamic Inventory:
=================

In our environment, we uses aws to provision our resources.

Dynamic Inventory uses scripts and Plugins to classify hosts from a cloud provider.
 scripts:
    ec2.py  and 
    ec2.ini 
 Plugins:
  ec2   


If you run the above script (ec2.py  and ec2.ini), it will dynamically fetch resource from aws and classify them. The 2 scripts are python script. This scripts are scripts we have commonly used before.

We can also use plugins as said above. If you have a plugin like ec2, you can also fetch for server resources in aws.

Prerequisite for ansible dynamic inventory

make sure pythiong is running in your ansible environment

install python3 and boto3

sudo yum -y install python3-pip              -----> non-ubuntu linux server
sudo apt -y install python3-pip               -----> ubuntu linux server

Pip is the package manager for python

pip3 install boto3 botocore --user
pip3 install --upgrade requests --user

Ansible will fetch and group servers/hosts/ec2  

servers are called ec2-instances in aws

For ansible to be able to etch and group servers/hosts/ec2, we need an ec2 plugins like

plugin: aws_ec2

If you go to ansible website, you will see the plugin in form of below

plugin: aws_ec2

FOR YOU TO GROUP SERVER, you can do either of the below

1. Assign an IAM role to your Ansible control server with:
  require permissions to group servers in aws via GUI

2. you can use command line in your ansible environment to create a file and paste the below

aws_access_key: AKIAVA7MDXUTQKZMK3IS
aws_secret_key: ePC63m+v5HelXInbf5g8TXm3izKidlfdZ7P/yXST
regions:
  - us-east-1
  - us-east-2
filters:
  instance-state-name : running
keyed_groups:
  - key: tags.Name
    prefix: ""
    separator: ""
hostnames:
  - private-ip-address
compose:
  ansible_host: private_ip_address

 dynamic inventory file for ec2-plugin must be must be or must end with aws_ec2.yml 

That is

aws_ec2.yml or *aws_ec2.yml    like  app-aws_ec2.yml, file-aws_ec2.yml  etc

Paste the above in a file called di-aws_ec2.yml by

vi app/di-aws_ec2.yml                 a directory called app is created

save and quit

Go to your aws account, create an IAM key for a user. you can do that by typing IAM in your aws console search, click on user, if already have a user, click on the userName you want, check in the user have aws-ec2FulAccess, if it has, check if you already created access key and secret access key before, by clicking on security. you can create a new one if you wish by clicking creating key. Copy the access key and secret access key.

Ti list the servers or resources that would be created. You can run the below command

ansible-inventory --graph -i di-aws_ec2.yml   

It will list all the server that I have in my aws ec2 in the AZs (us-east-1 and us-east-2) that I specified in the file above. You will see that it has grouped all the server resources you have in the regio you specified.

LET US PING ALL THE SERVERS IN THE REGIONS SPECIFIED. We run

ansible-all -i di-aws_ec2.yml -m ping

It will ping the servers that has ansible user installed in them.

TO PING ALL THE SERVERS THAT HAS ANSIBLE INSTALLED OR NOT. you can use a simple playbook like below

---
- hosts: all
  gather_facts: no
  remote_user: ec2-user
  tasks:
    - name: Test connection
      ping:
      remote_user: ec2-user
...

Let us copy te above and paste it inside a file and name the file test.yml

For ansible dynamic inventory, we do not need any hostfile.

For static inventory, we need an inventory file or hostfile like written below

[app]
172.31.80.9  
172.31.80.9 ansible_user=ec2-user  ansible_ssh_private_key_file=/tmp/key.pem  
172.31.80.9 ansible_user=ec2-user ansible_password=admin123  

But we are particular about dynamic inventory

NOTE: yOU KNOW THAT When create or provision an ec2-instances or server, it comes with 

ssh-privaye-key

When you want to connect to your server (ubuntu) for the first time, you can connect by running

ssh -i ssh-privaye-keyName ubuntu@serverIpaddress

TO PING ALL THE SERVERS IN THE ANSIBLE DYNAMIC INVENTORY FILE, USING THE PLAYBOOK ABOVE, you can run

ansible-playbook playbookName -i InventoryName -u DefaultUserName --private-key=path-To-ssh-key

ansible-playbook test.yml -i di-aws_ec2.yml -u ubuntu --ubuntu=/tmp/key.pem 

ansible-playbook test.yml  -i di-aws_ec2.yml -u ec2-user --ubuntu=/tmp/key.pem

You will see that it will ping all the servers that are grouped in the dynamic inventory file. 

NOTE: We did not use -m ping  because it was defined (test connection) in our playbook above.

For servers hosted by redhat linux distribution, the default user will be ec2-user

For servers hosted by ubuntu linux distribution, the default user will be ubuntu.

FOR YOU TO PING ONLY A PARTICULAR SERVER IN THE DYNAMIC INVENTORY FILE, 

you would have to specify the name of the server: 

If the name of the server is app and the server distribtion is redhat linux, you run 

ansible-playbook test.yml -i di-aws_ec2.yml -u ec2-user --ubuntu=/tmp/key.pem -l app

If the name of the server is app and the server distribtion is ubuntu, you run 

ansible-playbook test.yml -i di-aws_ec2.yml -u ubuntu --ubuntu=/tmp/key.pem -l app

You will see that it will only ping the app server and will not ping other servers in our aws ec2 using dynamic inventory.

SOMEONE COMPROMISED PROF AWS ACCOUNT.

The created a large instances type that we cost prof money. Prof wanted to terminate the instances but he was not able. He can only be able to stop it. this is beacause the person enabled TERMINATION PROTECTION. Prof had to ticked on each instance by ticking the first instance that was created by the hacker, click action, click on change protection termination, and unticked the enable. he did that to each of the instance, before he was able to terminate all the instances.

According to prof, the type of instances that was provisioned by the hacker is very expensive and can cost 1000 dollars per day for one of the instances. 

Pro said that he think the hacker may have gotten access via his access key and secret access key in IAM, he may have shared in the running note sometime ago. the person may be using command line to provision instance resources using the access key and secret access key for authentication.

What prof did, was to stop programmatica access to his environement, whereby can access his environment via command line. He this by going to his IAM, click on the user, click on the username, click on security credentials, and de-activate by clicking make it inactive and delete all of his access key and secret access key that he has ceeated before.

He also deactivated the keys in the root user by clicking on his account user name at the top right, clicked on security credentials, make inactive any of the access key and secrete keys he has created before and ten delete them

After he gas deleted them, it means that the environmental variable is no longer needed or useful because its been deleted from the backend. 

That is the authentication for the aws-ec2.yml file has been delete, he then removed the access key and secrete access key in aws_ec2.yml file as below

regions:
  - us-east-1
  - us-east-2
filters:
  instance-state-name : running
keyed_groups:
  - key: tags.Name
    prefix: ""
    separator: ""
hostnames:
  - private-ip-address
compose:
  ansible_host: private_ip_address

He then assigned another IAM role to the ansible server by selecting the ansible seerver, click on action, click on security, click on modify/IAM role, he selected an IAM role that has has created before which have ec2FullAccess and admin access (you can create a new IAM role by clicking create IAM role), clicked on update IAM-role.

and run the command

ansible-playbook test.yml -i di-aws_ec2.yml -u ec2-user --private-key=/tmp/key.pem -l app

You will see that the playbook task were fulfilled. Note. The playbook task was to ping or test connection of the app server.

EXAMPLE 2

Let us provision and configure a tomcat application in a particuler ec2-instance that we named tomcat by using a playbook and a dynamic inventory file for aws ec2 plugin

The playbook tomcat installation file was in prof github repo. prof copied the script and name it in our ansible environment as tomcat.yml this playbook to written for the task of provsioning and configuring a tomcat application in ansible environment.

an ec2-instance was already created and named as tomcat. we already have a dynamic inventory file that was written in yml which is purposed to group servers, which we have runned.

NOTE: If we created another server in our aws ec2, you wuld have to use dynamic inventory to group the server that you just created in a grapgical form by running

ansible-inventory --graph -i di-aws_ec2.yml

Then you new instance called tomact will be grouped among others. This methos is used to group all instances creatd. 

We want ton install tocat application in one of ther servers that was grouped. And the name of the server is tomcat.

Prof now run the command to install tomcat and its dependencies (like, java, apache etc) by running

ansible-playbook tomcat.yml -i di-aws_ec2.yml -u ec2-user --private-key=/tmp/key.pem -l tomcat

You run the above, it will install tomcat application and its dependencies only in an ec2 instances named tomcat among the ec2-instance group.

If you now copy the ipaddress of the tomcat and paste it in web browser with tomcat port number (8080) as below

23.12.122.4:8080

END OF ALL THE VIDEOS.

TERRAFORM

This is an IaaC= Infrastructure as a code.

Infrastructure as a code = simply means managing and provisioning of infrastructure through code instead of through manual processes.

It means putting all your infrastrature in a code format.

With IaaC. configuration files are created that contains your infrastructure specifications that makes it easier to edit and distribute configurations.

One of the advantages of IaaC as a code is that it makes you to know and see what was done in provisioning and configuring resources. 

Like when you use an aws console or GUI to provision an ec2-instance, you do a whole lot by selecting the type of instances you used, the volume of the ebs, the linux distributiuoin that you used and you may forgot the appropriate specification of measures that you used in provisioning such instance especially when you want to replica such specifcations in creating the infrastructure.

But with IaaC, it is a written code or script or document that gives details of all the specifications and configuration that were used to provision such infrastructure. It will there for life and you can re-use it if you want to create a similar infrastructure. You can also edit the document or code to suit your need.

IaaC is a simple way fo proviosning and configuring your resources.

Using IaaC, it means that you are deploying your infrastructure as a modular component that can be combined in defferent ways via automation.

If we have IaaC, it means we can break our infrastructure into modules,.

 NOTE

This means that, Automating infrastructure PROVISIONING using IaaC like Terraform implies that developers or engineers do not need to manually provision and manage servers, operating systems, storage, and other resources each time they want wants to develop want or deploy an application.

Coding your infrastructure gives you the template to following in provisioning your resources. 

The Coding your infrastructure can be done via automation by using an automation coding infrastructure tool called TERRAFORM.

With IaaC, you are basically writing all the procedures, command and the configurations that you took in provsioning such infrastructure or resource. It is a way of documenting everything that you did in form or a script or code so that anytime you want to do the exact thing or something similar, you can simply refer to that document or code and modify or use it for provisioning and configuration.

ADVANTAGES OF IaaC (Terraform)

1. Cost Reduction: 

The longer time you take in provisioning resources that you want to use will make you to pay more money. time is money. it cost you more money when you take more time in provisioning the resources.

2. Increase speed of deployment:

Because  everything or the infrastructures were written down as a code, it will increarse the speed of deployment. It means the resource deployment will be swift. When you run it through jenkins pipeline job, the deployment is swift

3. Reduce error

It reduces error because before the code is been deployed in our CI/CD pipeline, the code would have beeb tested, scanned, checked if error is found, it can be corrected immediately. it is better than when you configure the pipeline manually

4. Improve infrastructure consistency: Because the entire infrastructure was written a code format, provisioning and configuration will be consistent because you have a template as a guide.

5. Eliminate configuration drift: there will not be any form of moving a away from the template in place because it is written and documented. the whole infrastructure will be consistent

Terraform is an IaaC tool. Its vendor is hashicorp

Terraform is tool that i used in changing, managing and building infrastructure in a safe repeatable way.

Terraform configuration language is HCL (hashicorp configuration Language). 

HCL is used for both human and machine readable for automating deployments.

Terraform code is written in hashicorp language.

Cloud providers like aws, GCP can allow terraform to be able to manage some resources hosted by them,including IaaS, Paas, SaaS and hardware services.

ADVANTAGE OF USING TERRAFORM

1. its supports all clouds. It can be used to provision and manage resources in aws, GCP, Azure etc

2. State management: Terraform creates a state file when a project or a resource is first launched and initialized. 

A state file is the file that contains all of your configurations and resources as soon as an infrastructure is created using terraform.

With this state file, you will be able to see all the resources that terraform is managing.

3. Operator Confidence: With the use of terraform, operators are confident that the resources and infrastructure that was set up with terraform will not fail and will not cause disruption in their environment. It instill confidence in Users because as soon as terraform apply is initiated, users should agree and accept the changes updates and if users or operator do not accept, terraform will not deploy such commands. It would need usually to validate the changes made by terraform.

PRE-REQUISITE FOR TERRAFORM

1. You need to install terraform CLI  called the terraform BINARY.

We can use a package manager for windows called choco to install terraform (using powershell in windows to install administrator).

As an administrator, you can run

choco install terraform

It will install terraform for you.

OR

  terraform binary can be download from the below hashicrop link

https://developer.hashicorp.com/terraform/install

scroll down and select widows if you want to install it on windows and click download

if it is linux, it is going to be in commands, you will select the linux distribution that you want to use (ubutun/debian, amzon-Linux, centOs/RHEL etc)

RHEL = redHat enterprise linux


#copy the binary file into a folder

mkdir /users/<YOUR-USERS>/document/terraform-install

#copy package to your terraform-install folder

#unzip packageName

unzip packageName

#rename the already unzipped file to terraform by

mv unzippedFileName terraform


create a path for terraform binary by

echo $PATH

#copy terraform binary to /usr/local/bin

mv terraform /usr/local/bin

#verify terriform by running

terraform version or terraform -version

2. Since our cloud platform we are using is AWS, we should install

aws CLI

https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html


3. Whats we be using to write our code called terraform manifest files would be an editor.

the name of the editor specified is VS-CODE = vitual studio code

to install visual studion, we use

https://code.visualstudion.com/download

4. install a terraform plugins in our VS-code. This plugins make it easier for us to write our code in manifest file because it has what is called auto-complete. When you are writing commands in the manifest file, it sensed what you about to write and then it will just auto-complete it for you or give you suggestion.

5. You must have an aws account to be able to use terraform because terrafrom will querry the api of your aws to be able to create resources in your aws. Terraform can only be able to querry your api via your aws with your aws credentials.

FOR security reasons, it will be better for you to create an ROLE and attached it to your server or ec2-instance so that it is through that role terraform can be able to create reasources. You do not want to expose your credentials.

For you to be able to access your aws account via CLi command in windows or linux, it would be good for you to create an IAM user. and this IAM user would have an access key and secret acces key. with these credentials, you can access your aws account depending on the permission of role that was granted to the user.

if you go to your windos and run

aws configure

it will ask you for the access key and secret acces key

put in your default region: the region the resource you want to access was created. like us-west-2

default format: you can put json

#verify if you are able to list resouces created. like, list bucket is s3 if you have by running

aws s3 ls

#verify your aws credential profile by running

To see where your aws credentials are, run

cat $HOME/.aws/credentials

EXAMPLE

HOW TO INSTALL TERRAFORM IN LINUX

LET us install terraform in our redhat server. 

1. let us create ec2-instance on redhat linux distribution, let us open all ports from anywhere in security group.

Let us use shh clients called mobaxterm to access the server.  detach the section or terminal so you want 2 section.

---> lET US FIRST OF ALL INSTALL AWS CLI on the server by running

sudo yum update -y
sudo yum install curl unzip wget -y    #To install curl, unzip and wget
curl 'https://awscli.amazonaws.com/awscli-exe-linus-x86_64.zip' -o awscliv2.zip
unzip awscliv2.zip
sudo 
sudo ./aws/install

2. let us install terraform by running the commands below

sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
sudo yum -y install terraform
#will install from hashicorp repo

NOTE: To clear youR screen, you can press ctrl + L   instead of typing clear and enter key

To confirm if terraform was installed , you can run

terraform --version

You will see the version that you have just installed.

3. If you try to list access resources that was created in your aws server, it will ask you for credentials. that is, like running

aws s3 ls

it will ask you for credentials.

In as much as we do not want to use or expose our IAM credentials like our access and secret access key in our CLI environment, we can create a role and that role will be our credentials.

For best practices, it is better we create a IAM role in aws.

a. let us go to our aws console, search for IAM and click, click on roles, click on create roles, name the role  tf-role, assgn administrator access (awsAdminFulAccess), click on create role.

b. attach the role to your ec2 server by clicking on ec2, select the server, click on action, click on security, click on modify IAM role, select tf-role you just created and click save.

If you now run

aws s3 ls

It will list all the buckets that i have installed in my aws account.

So, in this server, I have my terraform, aws CLI and IAM role configured. 

If I want to create any infrastructure in aws using terraform, terraform will be able to queryy aws api through aws role as an authorization.

HOW TO INSTALLL TERRAFORM AND AWS CLI IN WINDOWS

1. terraform binary can be download from the below hashicrop link

https://developer.hashicorp.com/terraform/install

scroll down and select widows if you want to install it on windows and click download

the executables (terraform.exe) will be downloaded, you can click on the downloaded terraform executable file, you will see that it says unzip to, select when you want to it unzip or extract the file to, by click on the hook to select the file, click on music, right click and create another folder called terraform, then click unzip, the terraform executable file will be extracted or unzipped to terraform folder.

double click on terraform folder where the terraform executable were exrerated, and copy the address of path or right click on the terraform folder and copy the path or addresss. the address of path starts with c:/user

Note that the path is the one that link you to where terraform binaries or exe are located

You need to set the path where that file was extracted to. You know the file was extracted to terraform folder. you can set the path do by

going to your computer, search for  this pc, right click on this pc , click properties, click on advance systems settings, click on environmental variables, click on path, click on edit, click on new, paste the executable address that you copied, click ok, ok, ok

To verify is terraform has been installed

Go to your command prompt or gitbash and run

terraform                         will tell you that terraform has been installed
terraform --version              show you the terraform version
terraform -v                      show you the terraform version

INSTALLING TERRAFOIRM USING CHOCO

This ia  a bit easier 

Go to your computer search, search for command prompt, right click on the comm]and prompt, click on administrator, and the run

choco install terraform

It will install terraform in your windows for you completely.

2. You can also install aws CLI in your windows. 

You may use the below link to install aws CLI in your window:

https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html

If it does not work, Please try and find out or research about this. Though i have already install aws CLI in my windows.

INSTALLATION OF VS CODE AND TERRAFORM PLUGINS

https://code.visualstudion.com/download

after you have installed the vs code in your computer, click on terminal, drag the cursor from buttom to upward so that you have enough space in your terminal to be able to write your commands.

Since the vs code has been installed on your windows, You can confirm if terraform has been installed in your windows by running

terraform or terraform -v or terraform --version

also, you have installed aws CLI on your windows and so you can still access it via vs code terminal. Let us list our bucket by running

aws s3 ls

If it says that your credentials is not valid, it says the keys provided is not in our records, as in the case of what happened to the man that is teaching on this video {terraform video 1 link}.

You need to go back to your aws to create another IAM credentials. 

What he did was to create an IAM user called tf-new and grant the user programmatic access and also admin access and then create an IAM credentials (access key and secrete access key).

He now went back to vs code terminal and run

aws configure

and he put in the access and secrete acces key, and it will show you the default region, just press enter key, it will show you the default format, just press enter key.

when he now run

aws s3 ls

It still says, key provided is not in our record.

He want to see and confirm that credentials that he has if what he just created is stored her.

he ran the command

cd ~/.aws

and then

ls

Files below came out

config 
credentials 
and others came out

He the cat credetials

All the IAM default credentials that he has created on his aws account came out. The first credential is the most recent one which he intend to use.

It like the system was not using the latest credential that he generated, that is why it was saying key provided is not valid.

COMMAND TO RESET AWS CREDENTIAL TO CORRECT CREDENTIAL ERROR

He decided to reset credentials to avoid the above error by running

$ for var in AWS_ACCESS_KEY_ID AWS_SECRETE_ACCESS_KEY AWS_SESSION_TOKEN AWS_SECURITY_TOKEN ; do eval unset $var ; done

copy the above command and paste it on the terminal and press enter key

He now list the bucket again or run a command on the vs code terminal and it was able to list s3 bucket you have in aws.

This means that the user credential that has admin access is now very active to do everything in our aws account.

HOW TO INSTALL HASHICORP TERRAFORM PLUGINS IN YOUR VS CODE

1. in your vs code, click on your extession simbol by your left, type in terraform on the search button above, you will see hashicorp terraform, click on it click install and  it will be installed.

You can stiil install it using, if you stiil go to your web browser and paste 

https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform

click on install, click on vscode, click on install and it will be install in your vs code.

harshicorp terraform is now installed in our vs code. and so anytime we are writing a command in terrafor, it auto-complete what we are about to write or make suggestion for us. It makes codes writing for developers to be simnple.

- FIRST SIMBOL OF THE EXPLORER

The first simbol under explorer is the create simbol. where you can create a folder and also how you can create a file.

TO CREATE A FOLDER OR DIRECTORY IN VS CODE

click on open a folder, it will take you to where your computer folders are,  right click, click on new, click on folder and name the folder (maybe, terraform-series) and the click on select folder and it will take you to vs code.

FILE CREATTION

to create a file inside the folder, click on the first simbol opposit the folder you just created and type in the name of the file (main.tf) and press enter key and you will now be inside the file.

Note: terraform file always end with .tf or .tfvars

Since you are now inside your main.tf, you can now be able to write codes. for example if you type resource, it will auto-complete what you are typing and make suggestion on how you can write the terraform code.

You can also will be able to run commands with the file you are creating with vs code terminal which you can see below by dragging the cursor above so you have enough terminal space to write your command.

If you run ls

you will seee that main.tf you just created will appear.

NOTE:

Note: As soon as you create a folder/directory or open a folder/directory, your vs code terminal will be open in that particulare directory or folder. anything command you run with the terminal will be run inside the directory.

You can also create another directory inside a directory by clicking on the second simbol opposit the directory called "create a folder" and then enter your director name.

You can also create another directory inside the second directory and so on

You can always creat file inside the directory that you have selected from your computer directories.

You can now create file on the child directory or you can move file from parent directory to child directory by click and drag the file where you want it to be.

any directory you want to collapse or minimize, you can click on the last simbol opposit the directory called "collapse directory"

- 2ND SIMBOL OF THE EXPLORER

The simbols is the search simbol. where you can search for word, files and rename orcreplace them if need be

HOW TO SEARCH FOR FILES AND WORDS AND RENAME OR REPLACE THEM WITH NEW NAME.

- If you click on search simbol and type in the filesName you looking for, the file that has the word and part of the word will appear. Let say you want to rename or replace the filename with another new name, you can click on replace, type in the new fileName you want and then click on the replace simbol opposit.

You can also revert back  by using the same procedure.

- If you are inside a file and the file contains lots of commands or content and you want to find a word in that file, press ctrl f and type in the word, it will highlight places where the word are.

If you want to replace the word, click the greater than > sign and type in the new word and then click replace simbol (or click all occurence simbol  to replace all word that starts with that word ) that is opposit it

- 3RD SIMBOL OF THE EXPLORER

The 3rd simbol on the explorer is the simbol for git. It give you git functionality. where you can do anything about git, you can do commit, git init, git clone, create git branch, stash and all that has to do with git.

- 4TH SIMBOL OF THE EXPLORER

The 4th simbol is the extension simbol. It is the one that is used to install softwares in vs code. Vscode do not come or fully loaded with all the softwares or finctionalities needed. You will be the one to install them. You can install kubernetes, docker desktop, docker, ansible, hashicorp terraform just like the one we did before, and so on. You can always use the search button to search the software that you want to install.

HOW YOU CAN WRITE TERRAFORM FILE  A AND A MANIFEST FILE 

If you create a file that ends with  .tf and .tf part of the ending of the file, the vs code will sensed that it is a terraform file and open a terrafrom file page with terraform simbol for you, and so anything that you type in, it will give you a suggestion or autocomplete for you

Example

If you type in

provider,  it will suggest things for you. you can just press enter key to accept or click on waht it suggested for you. If it does not suggest for you, you can press control and space key and suggestion will come up. With the press control and space key, you can scroll down to search for what you want, you can come accross where it says "this is required", it means anything there is required and so you must click on that thing.

You know that you can use terraform to provision and manage resource in aws

tf file for the provison of ec2-instance in aws

provider
resource
cidr block
ami
instance_type
user_data

If you want to create a manifest file with a yml file. (k8s file).

We know that yml file is most of the time a k8s file and so we also know that KAMS is some of the components needed for k8s yml file. That is 

apiVersion
kind
metadata
spec

When you create a file that ends with .yml , the vscode will sense that this is a k8s file if you have installed k8s in the vscode and so, it wiil display a yml file with yml simbol and then anything you type in, it would suggestion or autocomplete it for you. like when you type in

api, it will suggestion apiVersion for you, just click the suggestion if you want it.

when you type in kind, it will suggest what kind you want. Just click on the suggestion it give you if thats what you want and it will give you a template om how you can write a k8s manifest file. You can now edit the steps of what it suggested for you to suit what you are lokking for.

If you install docker desktop on your windows, you als install k8s on your vscode, if you click on ks8 simbol on your vscode (ks8 simbole is found under extension simbol), you can ve able to see the docker deskop cluster that was created in your windows if you created one before.

In the extension, you can also have access to your docker when you install docker in your vs code.

NOTE

If you have a resource in aws like ansible or k8s cluster and you do not want to use gitbash or mobalxterm terminal to access the resource via CLI, you can configure your vs code such a way that you can use your vs code terminal to run a task using the resource. This is what is called REMOTE DEVELOPMENT.

REMOTE DEVELOPMENT OR ENVIRONMENT

HOW TO CONNECT REMOTELY TO REMOTE SERVER LIKE LINUS USING VSCODE

HOW TO CONNECT TO YOUR AWS RESOURCE REMOTELY IN YOUR LOCAL VS CODE

For you to be able to connect to your resources in aws remotely without connecting locally (that is using your computer windows or any os that was installed in your computer), you will need to install remote-SSH on your vsCode extension. by clicking on the extension simbol, type in REMOTE-SSH on your search button, and install it. 

as soon as the remote-ssh is installed, go down to the left button of your vs code, you will see a blue box called (open a remote window), click on the blue box, click on connect to host, click on configure ssh host, click on user/.ssh/config which is the first word, it will open a template or a written template on how you can connect. it has HOST, USERS, IDENTITYfile. in the form of

Host k8s-server
 HostName  4.23.12.11.3
 User ubuntu
 IdentityFile ssh-key-fileName

Host nexus
 HostName  7.23.12.11.3
 User ec2-user
 IdentityFile ssh-key-fileName

Host ansible
 HostName  8.23.12.11.3
 User ubuntu
 IdentityFile ssh-key-fileName

Host jenkins
 HostName  5.23.12.11.3
 User ec2-user
 IdentityFile ssh-key-fileName


As shown above, you can see that you can fill in multiple host or server that you are working with if need be, using vs code remote-ssh. If it just one host, you can do that.

host = (this is the Name that you named the ec2-instance. like jenkins)

HostName =  is the ipaddress of the host (you can use the public address)

User = is the default user of the host. This depend on the linux distribution of the host. If it is a rehat linux, the default user is ec2-user, if it is ubutun, the user is ubuntu

IdentityFile = This is ssh-key. you know you normal save the ssh-key file in your computer.

After yopu have fill in the above, click on file, click on save. You can even ignore saving it because it is usually auto-save. that is once you leave, it will save automatically.

then click on open remote window simbol that is like a blue box below, under remote-shh, click on connect to host, it will display all the host that you have filled (if you have filled multiple host), click on the host that you want to connect to. enter the platform of the remote (this means if it is linux or windows or mac) in this case, click on linux because we want to connect to linux host, click on continue.

If you look below, you will see a blue rectangular box with ssh-jenkins written on it, this means that your have connected remotely to your jenkins host or server via vs code.

click on create simbol on the explorer simbol when you want to navigate to your file by creating or open. You can also be able to clone a repo if you want to.

If you want to start running command in your vscode with the server you just connected to, go to terminal by take your cursor below and grab the line from the down and take it up so that your terminal will come out or you can click on terminal and click on new terminal.

EXAMPLE.

Let us create a directory using command line in the vs code terminal by running

mkdir test

ls

You will see that the directory was created

NOTE:

If you want to run command inside the directory you have created in your new host or server, in your terminal, you can run 

code .


it will ask you if you trust the author, click on YES.

It will open a new vs code window where you can see directorries and files including the one that was created on the host in a GUI way. 

Let us create a terraform file on the test directory by clicking on test directory, click on the createFile simbol which is the first simbol opposit the Default-userName (ec2-user or ubunt of the host), type in main.tf as the fileName and press enter.

You are now inside the file.

let us type the word resource, select null_resource, it will auto complete giving the the steps for null_resource.

You can now go to your terminal to cd into test directory you created and ls, you will see the file (main.tf) you created in a GUI way. You can now run any command or what you want to do with the file. Maybe, fist of all, run

pwd

You will see that ec2-user home directory will appear.

/home/ec2-user/

If you 

ls, 

you will see a directory called jenkins and other files and directories that was created before


NOTE:

You main idea is that as soon as you have connected remotely to your host or server via remote-ssh in vscode, you can create and write a terraform code using the GUI way where the vscode system can assist you in auto-completing and auto-suggesting how you can write the code for you and once you are done, it auto-save by itself come and then you can come down to the terminal and run you command with the file.

Since we are using jenkins-Host. This means that jenkins has already be installed on the host. 

Let us check the jenkins home directory, you first of all switch user to jenkins user because jenkins user ( jenkins user was creation command was among along jenkins installation command in the file )  by running

sudo su - jenkins

if you now run

pwd

It will give you

/var/lib/jenkins

That is the Jenkins Home Directory

Since will now have JHD as /var/lib/jenkins, exit by running


exit

to ec2-user and the cd to JHD by running

cd /var/lib/jenkins

If you ls, you will see all files and directories in jenkins. All the jobs you have done in jenkins.

If you want to navigate to vscode GUI window with your present working direction, so that you can do some coding, you can run

code .

It will open vscode GUI window, then click YES.

You will now be in the home of jenkins inside a new window where you can see files and directories in jenkins. You can drag the terminal from bottom upward so that you can be able to run command on the new window jenkins terminal.

NOTE: You can not navigate to vscode GUI window with a jenkins user here because you can only navigate when you are a default user. jenkins is not the default user here. This is why you need to exit to ec2-user if you are in jenkins user so that you can navigate.

if you do not have to navigate to a new GUI window on the new user, you can click on the blue rectangular box and click CONNECT CURRENT WINDOW TO HOST

You can see that you are now looking at the server remotely. All your jobs remotely. You can also run task remotely.  This is what remote development is all about. Without the local environment

HOW TO TERMINATE REMOTE-SSH CONNECTION

if you want to close or terminate the remote-ssh connection, click on the rectangular box and click on CLOSE REMOTE CONNECTION.

Once you close the connection, it will take you to your vscode computer local environment (your computer window envronment)

NOTE:

vs code is maintained my microsoft

What you have done above is that you want been able to access your cloud computing platform like aws and run commands remotely using the vscode you installed in your local computer. This ws possible because aws was also installed in your computer.

HOW TO RUN LINUX COMMAND LOCALLY USING VSCODE.

You can run linux command locally in your window computer with vs code installed d in your windows using what is called WSL

REMOTE-WSL

what WSL

WSL (window sub-system for linux) = This is used to enable linux command to be runned in windows environment successfully.

WSL = window sub-system for linux

Vs code has what is called REMOTE-WSL

WSL = This is used to enable linux command and linux file system to be runned in windows environment when you install it in your windows environment.

You can search REMOTE-WSF in your vs code explorer and install the software.

HOW IT WORKS.

Go to your local computer and type in microsoft home in your search below in your computer  and click , once it open, type in ubuntu (which is a linux distribution) on the search and click ubuntu 20 or more recent one and install it in your windows. Most of thme are open source and they are free.

This would be cheaper to manage than linux distribtion in aws because instead of ubuntu server to continuous running in ec2 aws platform which will cost you more, you can use WSF which will be managed by windows and you would not bother about cost of it running in your windows. 

As soon as you have installed it in your local window computer, open the ubutun my clicking on the ubuntu icon or logo on your local computer, ubuntu linux sub-system environment in windows will be open.

HOW TO CONNECT TO VS CODE

Since you have also installed remote-WSF in your vscode. Click on the open-remote-window sign which is blue box down left and the click on WSL

Open the ubutun WSL and then run

code .

You are telling ubutun-WSF to open a vscode GUI window in the present directory and it will open.

EXAMPLE

The lecture installed ansible on the ubuntu-WSF which now make it to be ansible local server. He was able to run all his ansible command from his local environment via vscode. he ran

ls

it was successful. where some of his files displayed. he ran

ansible all -m ping

He was able to ping all the host in his default host inventory file

NOTE: With remote-wsf, i do not need to be go to aws ec2-instance back and forth to start stopping amd starting my server or running my server in aws that will cost me alot. wsf would be sufficient for me intermS of practising and testing.

LIVE-SHARE

Live share is a functionality in vs code that enable a developer to be able to share his environment with another developer for possible collaborations, assistance and experience.

You can type in live-share in your vscode explorer and then install the software. Once it is installed, live-share simbol that lokks like a round forward arrow will be at the vscode explorer below. if you click on the simbol, click on share, it will create a collaboration like or shre link, click on copy link or copy again (which will be down right side), and send the link to somebody.

Let say i want the person to collaborate with me for code writing. Once the person click the link, it will open. The person can click CONTINUOUS IN ANANYMOUS  or SIGN in. if the person click CONTINUOUS IN ANANYMOUS, the person can type his name at the top for recognition purpose and press enter. It will start booting trying to join the session.

It will notify the sender or me that a person called Mark want to joing the session, it will ask me to accept-read-only or accept-read-write. If you want him to be able to read and write then click on accept-read-write, the developer can now see my work and collaborate. So any file that i open, they are able to see the file that i open and cal even type on the file and i am able to see it also. we can both modify the file. This is because the developer was give both read and write access. If i create any file like creating a yml file like test-yml and press enter, they will also see what i did. and vice-versa. You can actually do coding with a friend. You can also troubleshoot together.

In the life-share, you can what is called session-chat after the sessiuon connection has been initiated. You can communicate with each other via the session-chat by clicking on life-share simbol and then click on session chat and then chat with your collaborator.

With Vs code, you can move and split a file to become multiple file. 

TO DRAG: You can click on the file, take you cursor to when the fileName is and the click on the fileName and hold and then drag it to where you want it to be. then you have have 2 identical files.

TO SPLIT: You can split to have the same file but in multiple way by right clicking on the fileName and the click ob split up or click on split down. Doing so, you have split the file into to but they are the same file and same content but displaying in multiples places.

OPEN MULTIPLE TERMINAL

You can also open multiple terminals going to terminal window or environment. You will see something that looks like a square sign, after a plus sign, at the right hand side. click on the square sign and another terminal will be opened

Ypou can use ther second terminal to monitor or watch a command you runned in the other terminal.

If you click on the delete simbole after the square simbol, you have deleted the multiple terminal.

If you want to add another separate terminal, a different terminal, you can click the plus sign to add another terminal.   

If you want to change or switch terminal to another terminal like switching to powershell or gitbash, you can click on a sign immediately after plus sign. It looks like hook or greater than sign pointing upward. once you click the sign, you can click powershell if you want to connect to power, you can click gitbash, if you want to connect to gitbash if installed in your computer (gitbash have a more functionality than powershell). You can terminate the terminal by clicking the delate simbol.

You also want to bring out the terminal you can do that by draging it up from down or go up and click terminal and then click on new terminal.

HOW TO RUN WORK LOAD USIMG TERRAFORM IN VSCODE EDITOR

First of all, Let us connect our terraform server in aws remotely to our vscode. You know we installed hashicorn terraform in aws ec2-instance.

To connect the terraform server to our vs code. Using remote-ssh (remote development), Click on the open remote connection blue box below left, click on open ssh configuration file (under remote-ssh), click on user-config because i am using the local system vscode to connect remotely. it config file will open. You can now write your remote-ssh connect configuration file as below

Host k8s-server
 HostName  4.23.12.11.3
 User ubuntu
 IdentityFile ssh-key-fileName

Host nexus
 HostName  7.23.12.11.3
 User ec2-user
 IdentityFile ssh-key-fileName

Host ansible
 HostName 8.23.12.11.3
 User ubuntu
 IdentityFile ssh-key-fileName

Host jenkins
 HostName  5.23.12.11.3
 User ec2-user
 IdentityFile ssh-key-fileName

 Host tf-server
 HostName  5.23.12.11.3
 User ec2-user
 IdentityFile ssh-key-fileName

Go to your ec2-instance, copy the ipaddress, name that you gave to the server, and the path to your ssh-key which you have definitely saved in your local computer when you were lunching the ec2-instance in aws, the default user for the linux distribution that you used. and used them to modify any of the host above. the file will auto-save the modification and the close the config file.

Click on the blue square box and click on connnect to host (under remote-ssh), click on your host (tf-server), click on linux (since it is linux), click on continue. A new window is out, this vscode window is now the vs code window that is connected remotely to your tf-server in aws. You can drag the terminal from below to upward. To know if its up and running, in the terminal, just run

ls

Since i have already attached an IAM role to this tf-server when i installed it, i can just run

aws s3 ls

It will list all the buckets that i have in my account

Let us create a directory in this tf-server by running

mkdir tf-works

cd tf-works

pwd

You are now in the home of tf-works. To open another vs code window inside tf-works, you run

code .

. means to execute command in the pwd

code .  = means To open another vs code window inside pwd (present working directory)

Another vscode window will be open inside tf-works.

We can now begin to create our terraform manifest file for the provisioning and management of our resources.

Terraform uses declarative syntax or commands to describe your infrastructure as an IaaC and then persit or stored it in a configuration file that can be shared, edited, reviewed, re-used, visioned or preserved.

Terraform configuration file can be written in 2 format

HCL format = Hashicorn Configuration Format is the format that ends with .tf or .tf file extension (.tf*   like .tfmanager etc. This means anything can be added after .tf)

.tf file can both human readable

Jason format = is the format that ends with .tf.jason   

 .tf.jason  file is only machine readable

 The recommended one .tf file extension

 HOW TO PROVISION AWS RESOURCES WITH TERRAFORM USING VS CODE

 PROVISIONING OF EC2-INSTANCE RESOURCE

 1. You need to know what resource you want to provision. Since we want to provision aws ec2-instance, we can write it in form of

 resource: aws_instance

 2. You need to know that AMI ID of the instance you want to provision. Knowing the amazon machine image ID. You can actually know your ami by going to your aws console and click on lauch, choose the distribution and then copy the ami-ID of the distribution. every distribution has the ami or os

just copy the ami-ID that is in the form of 005e54dee72cc1d00 (64 bit.x86) which is the first ami-ID. make sure the ID ends with (64 bit.x86) and then Copy only 005e54dee72cc1d00.

 3. You need to know the instant type. This has to do with the type of instance you need. This instannt has to do with ythe size of the instance, the storage capacity (like, t2 medium, t2 micro etc)

 NOTE:

Just google  "terraform ec2-instance resource"  and that you can go to terraform registry. you will see structure or format on how the provisooning of ec2-istance tf manifest file will be.

Any terraform aws resource provisioning that you want to do must start with aws_ResourceType

While in the ec2-instance resource in terraform registry, scroll down to find a resource with aws_instance which has ami ID that looks like ami-007e23dee72cc1d00. We found one after scrolling down

resource "aws_instance" "foo" {
  ami           = "ami-005e54dee72cc1d00" # us-west-2
  instance_type = "t2.micro"

You can now edit it as below

resource "aws_resourceType" "ServerName" {
  ami           = "ami-ami-D" # us-west-2
  instance_type = "t2.micro"

Using GUI vscode, Create a manifest file in the tf-works directory and call it ec2.tf and then paste the above inside tre file. let us called ther serverName demo-instance

Note: ami is region specific. You can actually know your ami by going to your aws console and click on lauch, choose the distribution and then copy the ami-ID of the distribution. every distribution has the ami or os

just copy the ami-ID that is in the form of 005e54dee72cc1d00 (64 bit.x86) which is the first ami-ID. make sure the ID ends with (64 bit.x86) and then Copy only 005e54dee72cc1d00.

Make sure you found out and copy the region the ami-ID is located (like, us-east-1. us-west-2). Edit the file


esource "aws_instance" "demo-instance" {
  ami           = "ami-002e34dee66cc1d00" # us-west-1
  instance_type = "t2.micro"

  You need to inform terraform which cloud provider you want to use.

  You can go to google and type in terraform provider or terraform aws provider , scroll untilo you see aws provider format like below

  provider "aws" {
  version = "~> 5.0"
  region  = "us-east-2"
}

delet the version, we dont nedd that for now. You now have 

 provider "aws" {
  region  = "us-east-1"
}
resource "aws_instance" "demo-instance" {
  ami           = "ami-002e34dee66cc1d00" # us-west-1
  instance_type = "t2.micro" 

This configuration file or manifest file needs authentication but since we have already defined an IAM role for tf-server whereby he was giving an awsFull admine access role, we can go ahead to initialized the config file.

NOTE: 

1. The first thing you do when a new config file is written, you we need to initialize the file. Drag the terminal of your vscode upward so you have space and the run

terraform init

This initialization will download the provider and modules

If there is an error when you run the above command, it will say error in read color. You have to pay attention to the error and know what to do.

It says error: unclosed configuration block. we have an open curly bracket will did not close. we closed the bracket by adding }

 provider "aws" {
  region  = "us-east-1"
}

resource "aws_instance" "demo-instance" {
  ami           = "ami-002e34dee66cc1d00" # us-west-1
  instance_type = "t2.micro" }

  and the run

  terraform init

  It was successfully downloaded. You will see that you now have another hidden directory called .terraform inside the tf-works directory in your vscode code where aws provider and logs files are downloaded. This files contains all the source code that aws uses to run ec2-instance.


2. You need to run terraform validate. This validate the configuation files. to make sure that there are no syntax error. by running

terraform validate


NOTE  

The teacher encountered a situation whereby the connection TO COONECT VS remotely between the remote-SSH server and the vscode was taken so much time to connected to each. 

So the teacher decided to use the terraform windows in my local environment that was connected to my vscode.

In the vscode that was conneted to terraform windows of his computer, he went to the terraform folder or directory that he created there (please check above explanations. You can create another folder or directory if you like), he created a manifest file called ec2.tf and started writing code for aws-instance resource in aws in the form of

PrpRai =

provider
 region
resources
 ami
 instance_type

 That is

provider "aws" {
 region = "us-west-1"
}

resource "aws_instance" "ec2-demo" {
  ami =  "ami-002e34dee66cc1d00" # us-west-1
  instance_type = t2.micro
}

he drag the terminal upward and then run

terraform init 

and was initialized and a .terraform directory was created as usual.

he  also ran

terraform validate

and it say coniguration validated.

3. You run a terraform plan command. The plan tells us what terraform is actually going to created with the configuration file or manifest file above. It is what it in the file that terraform will create. 

NOTE:

The prerequisite for ec2-instance provisioning with terraform are as follows

- Our manifest file can contain a default vpc:

You can see above that we did not specify any vpc which means that it is going to be created on a default vpc

- You must specify the region where your ami is gotten from, in your manifest file

- You must know define yur credentials.

Ways To Defined your Credentials In Provisioning Resource In aws.

The ways you canb define your credentials in aws are as follow

- Static Credentials

 provider "aws" {
  region = "us-west-1"
  access_key = "put access key here"
  secrete_key = "put secret key here"

}

- Environmental variables

$ export AWS_ACCESS_KEY_ID="accesskey"
$ export AWS_SECRETE_KEY="secrete"
$ export AWS_DEFAULT_REGION="put preferred region here"

$ export AWS_ACCESS_KEY_ID="accesskey"
$ export AWS_SECRETE_KEY="secrete"
$ export AWS_DEFAULT_REGION="us-west-1"

- Shared credentails/configuration file

 provide "aws" {
  region = "us-west-1"
  share_credentials_file = "/users/tf-user/.aws/creds/"
  profile = "dev"

}


---> The static method of credentials is not recommended because you are hard-coding. You are pasting your access and secret key in your manifest file. It is not secured. You are exposing your credentials and it is vulnerable for hacking. it is not re-usable. 

---> The environment variable method of credentials is recommended because you are not hard-coding. It is very secured. You are not exposing your credentials and it is not vulnerable for hacking. 

You can set the environmental variable for accesskey and secretkey so that terraform software will scan through your environment variables and pick up the credentials securedly. Manifest file containing this variable can be re-usable. 

---> shared credentials method of credentials is also another way. 

This is also called profile method of credentials. Whereby if you specify profileName in your manifesr file, terraform will read the profile and determine the credentials

Anytime you run aws configure, a default profile is created for you and the default profile contain your default access key and secret key.

You can save your custom access key and secret key in a file and then define the path to file in your panifest file.

Let us see what we have in our .aws directory by running

cd ~/.aws

You will see the files that we have there. One of the file contains where our aws credentials are saved (accesskey and secretkey). 

NOTE: OUr IAM AWS credentials file is always saved in .aws and you can get to it via ~/ thats in the home of your local window computer. you run

~/.aws


if you inside the .aws directory and cat credentials, you will see the access and secret key that you created and it would be under name which you used as the name of the credentials. The name if called profile and in this case, you created the name or the profile.

You will also see a default name with its credentials. the is created by aws by default.

NOTE: profile means the name of your credentials (could be be a default credentials or a custom credentials). Just as you can say profileName. 

The profile that will be insie the credential file will be in for of

osas_accessKeys.csv
osas_accesskeys.csv

[default]
aws_access_key_ID = WTUIO9FG737839
aws_secrete_key = rl4789gah8989377jmdjkdnj6737r37

NOTE: if you do not parse a profile inside your provider, it will automatically parse a default profile which would have the IAM credentials you created in aws

[landmark]
aws_access_key_ID = WTUIO9FG737839
aws_secrete_key = rl4789gah8989377jmdjkdnj6737r37

With the above credentials , you can pass the prefered credentials by only defining the profile on your manifest file as below

provider "aws" {
 region = "us-west-1"
 profile = "default"
}

resource "aws_instance" "ec2-demo" {
  ami =  "ami-002e34dee66cc1d00" # us-west-1
  instance_type = t2.micro
}

  Note: if you did not define any profile on the above manifest, terraform will read credential from a default profile. So if you want it so, do not bother to define any profile.

The above shows that we want to use the profiling method of defining credentials.

In your terminal, Leave the .aws directory to the directory where your manifest file is located and run

terraform plan

It may say error in credentials, you need to reset your credentials.

COMMAND TO RESET AWS CREDENTIAL TO CORRECT CREDENTIAL ERROR

He decided to reset credentials to avoid the above error by running

for var in AWS_ACCESS_KEY_ID AWS_SECRETE_ACCESS_KEY AWS_SESSION_TOKEN AWS_SECURITY_TOKEN ; do eval unset $var ; done

It will reset the credential and then run again

terraform plan


It will inform you all the things that it will create. The resrouce, vpc, subnets, userdata, ebs

4. You run a terraform apply. this used you to apply or run the changes on the config file to a desirable state or result of the configuration. by running

terraform apply

A state file (terraform.ftstate) is created when terraform apply is run for the first time and then aws instances will be created. 

The state file contains the logs and information about the resources that terraform has created. It enable terraform to know which resource that is under his controls so that he can nanage such reseources.

If you now go to your aws ec2 console, you will see an instance that has just been newly created.

If you look at the instance created, there no name or tag. You can name the instance by defining tag or name of the instance inside the manifest file as below (tags is a key value).

provider "aws" {
 region = "us-west-1"
 profile = "default"
}

resource "aws_instance" "ec2-demo" {
  ami =  "ami-002e34dee66cc1d00" # us-west-1
  instance_type = t2.micro
  tags = {
   "name" = "my-first-tf-ec2"
  } 
}

You can now run

terraform apply

It will update or modify the statefile and then name the instance to be my-first-tf-ec2. If you go to your ec2 and refresh, you will see it renamed.

The manifest file is the code. This is what we call infrastructure as a code (IaaC). Creating infrastructure or resources simply using a code.

5. Terraform destroy is used to destroy or terminal resources that you do not want. This is done by running

terraform destroy

NOTE: we just provision ec2 resource using vs code that was installed in our local window computer. It was possible because we have aws installed in our windows, we have terraform installed in our windows computer and we have terraform plugins installed in vscode.


NOTE: Remember that the teacher left the provision of ec2-instance with terraform using remote-ssh in vscode with terraform to provision ec2-instance. He left because the remote-ssh connection between vscode and remote terraform server was slow and so he decide to use local window terraform wuth vscode to provision the ec2-instance as in the above.

Now, the remote connection is up and runnning and so he went back to completeth provisioning.

He went to the vscode that is connected remotely and cd into the pwd by running

cd tf-works

and then

ls

he manifest file called ec2.tf was there. to open vscode window on this pdw called tf-works, he ran

code .

the vscode window opened, he went into the new window, he dragged the terminal upward. 

In the vscode gui, he clicked on the manifest file and the file content appeared and he added tags to the file as below

provider "aws" {
 region = "us-west-1"
 profile = "default"
}

resource "aws_instance" "ec2-demo" {
  ami =  "ami-005e444dee66cc1d00" # us-west-1
  instance_type = t2.micro
  tags = {
   "name" = "ec2-demo"
  } 
}

On the terminal, he ran

terraform init

To initialize the file and then he ran

terraform validate

to make sure its free from syntax error. the ran 

terraform plan

to show you whats the plan will be. what would be instored. and then run

terraform apply

This will run the changes on the file or plan to created what is expected to be created and a statefile will be created on the pwd. if you do not want the resources created, you can destroy or terminate the instance by running.

NOTE: You know that for terraform aplly to work, credentials are necessary. This remote connection is working with the IAM role that i attached to the terraform server when i created the server. 

terraform destroy

that would remove the instance from terraform statefile.

Above are examples of using Teraform to run your workflow or projects or tasks

ANOTHER EXAMPLE OF USING TERRAFORM TO EXECUTE TASKS OR PROJECTS OR WORKFLOW.

CREATION OF VPC USING TERRAFORM

1. create a new folder/directiory called "project 1"
2. create a vpc called "first-vpc"
3. cidr range:  192.168.8.8/24

Solution

Let us create vpc using terraform in our local window.

Go to your vscode in your local computer windows and create a directory inside your vscode terraform folder by clicking the second simbol and name it project-1, create a file by clicing the first simbol and called it vpc.tf and then go to google and type-search for 
"teraform vpc resource example" and you have get something like below

resource "aws_vpc" "main" {
  cidr_block       = "10.0.0.0/16"
  instance_tenancy = "default"

  tags = {
    Name = "main"
  }

delete the  instance_tenancy = "default" because that not what we want. you have 

resource "aws_vpc" "main" {
  cidr_block       = "10.0.0.0/16"
  tags = {
    Name = "main"
  }
  }

modify it and put in your details

resource "aws_vpc" "first-vpc" {
  cidr_block       = "192.168.8.8/24"
  tags = {
    Name = "first-vpc"
  }
  }

If you initialize and valaidate, it will work but if tyou run plan and apply, it will ask you define your cloud provider that wants to host this resource. We did not define our cloud provider here. let us add provider details. since we are using aws, we use aws provider and add it as below:

provder "aws" {
  region = "us-west-1"
}

resource "aws_vpc" "first-vpc" {
  cidr_block       = "192.168.8.8/24"
  tags = {
    Name = "first-vpc"
  }
  }

Then run 

terraform init
terraform validate
terraform plan
terraform apply     or    terraform apply -auto-approve

terraform apply -auto-approve   will apply or update changes without asking for authorization of confirmation of yes or no.   just like you have  -y  in linux

It will create the vpc. You can go to your vpc console and aws and you will see the vpc name that was created.

You can also terminate the server if you do not need it anynore by running

terraform destroy -auto-approve

OTHER COMMAND IN TERRAFORM

terraform refresh

terraform import  + used to import external files that are not managed by terraform to be manage by terraform henceforth.

NOTE; If you want to know more comands in terraform and what each commands does, in your vscode terminal, run

terraform

It will list out all the command and what they do.

ASSIGNMENT

Try to create s3 bucket using terraform in your custom vpc.

NOTE; The resaon why we install software plugins like hashocorn terrfaorm plugin, k8s plugins and so on in our vsc code is simply because when we are coding or wrting their files, it auto-complete or auto-suggest to make coding easier. It happens when yopu name the file with the right format. Like terraform used HCL (harshicorn configuration language) format which is .tf while k8s using a file format called yml (yaml)

Always destroy the resource to avoid cost. It is better you use windows terraform to provision any resource in aws to minimize cost

TERRAFORM VIDEO 4A

VARIABLES

Variable is a value that can be changed, depending on information passed to the program.

TYPES OF VARIABLES in terraform

1. String variable:

its represented by a double-quote like this "first-vpv"

variables "vpnName" {
	
	type = string
	default = first-vpc
}


2. number variable:

its represented by a unquoted sequence of digits number with or without decimal like 13, 12.3 etc. in the form of 

variable "sshport" {
	
	type = number
	default = 12
}

3. boolean variable:

its represented by a unquoted simbols that is true or false . in the form of 

variable "enabled" {

	default = false

}

4. list variable:

its represented by a pair of square brackets containing a comma-separated sequence of values, like ["a", 15, true ] . in the form of 

variable "mylist" {
	
	type = list/string

	default = ["values1", "values2" ] 
}

How to reference list variables

instant_type = var.mylist[1]

5. map variable: it is represented by a pair of curly brackets {} and it is a key value pair  key = value



variable "mymap" {
	
	type = map
	default = {

	key1 = "value1"
    key2 = "value2" 

	}
}

how to reference the map variable.

instant_type = var.mymap{"key"}

6. Input variables

When a default is not defined, then the  variable becomes an input variable and you would be prompted to enter an input value at runtime. as below


variables "inputName" {
	
	type = string
	description = "set the name of the vpc"
}


#Output

output "vpcid" {
  value = aws_vpc.myvpc.id
}
#Tuple

Lists/tuples are represented by a pair of square brackets containing a comma-separated sequence of values, like ["a", 15, true].
variable "mytuple" {
  type    = tuple([string, number, string])
  default = ["cat", 1, "dog"]
}
#Objects

variable "myobject" {
  type = object({ name = string, port = list(number) })
  default = {
    name = "Landmark"
    port = [22, 25, 80]
  }
}
#Variables with Lists and Maps

#AWS EC2 Instance Type - List

 variable "instance_type_list" {
  description = "EC2 Instance Type"
  type = list(string)
  default = ["t3.micro", "t3.small"]
}

#instance_type = var.instance_type_list[0]
#AWS EC2 Instance Type - Map

 variable "instance_type_map" {
  description = "EC2 Instance Type"
  type = map(string)
  default = {
    "dev" = "t3.micro"
    "qa"  = "t3.small"
    "prod" = "t3.large"
  }
}

#instance_type = var.instance_type_map["qa"]


The teacher terraform landmarkteck github repo was forked.

https://github.com/LandmakTechnology/terraform-series

TERRAFORM INPUT VARIABLES.

THE USE OF VARIABLES FOR PROVIONING

How to use variable block. looking at the below configuration for provisioning ec2-resource in aws via terraform, you can create a variable for each components like

region, ami, instance_type and tags that is

provider "aws" {
 region = "us-west-1"
 profile = "default"
}

resource "aws_instance" "ec2-demo" {
  ami =  "ami-005e444dee66cc1d00" # us-west-1
  instance_type = t2.micro
  tags = {
   "Name" = "ec2-demo"
  } 
}

Let us create variables that will represent instanceType, ami and region for the tf resource file. 

variable "variableName" {
  description = "short description of the variable"
  type    = variableType
  default = "any default that relate what the variable is"

  Example
}
let us create a file called var.tf and paste the below inside

# variable for region is below

variable "my-region" {
  description = "aws region"
  type    = string
  default = "us-west-1"
}

This can be represent as region = var.instance_type

# variable for ami is below

variable "my-ami" {
  description = "aws ami"
  type    = string
  default = "paste ami Id numbers here"
}

This can be represent as ami = var.my-ami

# variable for instance_type is below

variable "instance_type" {
  description = "aws ec2-instance"
  type    = string
  default = "t2.micro"
}

This can be represent as instance_type = var.instance_type

Let us create another file called ec2-server.tf that has our resource and paste the below inside. You can now represent the instancetype, ami and region in variable.

m


resource "aws_instance" "ec2-demo" {
  ami =  "var.put the name of your ami variable here" # us-west-1
  instance_type = var.put the name of your instance_type variable here
  tags = {
   "name" = "ec2-demo"
  } 
}

That is

resource "aws_instance" "ec2-demo" {
  ami =  "var.my-ami" # us-west-1
  instance_type = var.instance_type
  user_data = file("${path.module}/app-install.sh")
  tags = {
   "name" = "ec2-new"
  } 
}



Let us create another file called aws-region.tf as our provider and region and paste the below inside.

You can add provider requirement that you want inside the provider file as below

terraform {
  required_version= ">= 1.0"
  required_providers {
    aws = {
      version = ">= 3.0"
      source = "hashicorp/aws"
    }
  }
}

# i search google to get this provider requirement by typing terrafrom provider requirement

provider "aws" {
 region = "put the name of the region variable here"
 profile = "default"
}

That is 

terraform {
  required_version= ">= 1.0"
  required_providers {
    aws = {
      version = ">= 3.0"
      source = "hashicorp/aws"
    }
  }
}

# i search google to get this provider requirement by typing terrafrom provider requirement

provider "aws" {
 region = "my-region"
 profile = "default"
}


Let create another file and name it app-install.sh. Let us paste any installtion script inside the file (like maybe mavin installation script).


Sice all the 3 different tf file we created is created under a directory called tf.variables.

grab the terminal upward and cd into tf.variables and then run

code .

it will open another vscode window in tf.variables directory. grab the terminal upward and then ls and you will see all these files that was created. let us provision ec2-instance with a mavin app installed on the ec2-instance by run

terraform init

terraform validate

terraform plan
It may say error in  credentials. you cal always reset it using the reset command above.

terraform apply -auto-approve

You have just created a terraform file template where you can always come back to to modify anytime you want to provision or update or upgrade any resource in aws.

Let say you want upgrade your resource instance-type from t2.micro to t2.medium, you can just go to the variable file and change it to t2.medium and then run

terraform apply -auto-approve

it will modify the statefile and change it to t2.medium and then your instance_type becomes t2.medium

CREATION OF SECURITY GROUP REOURCE IN AWS VIA TERRAFORM USING VARIABLE FOR PROVISIONING

google-search for   aws terraform security group resource

let us create a file under the tf-variable directory we using before and name the file security.tf and then paste the below

# security group file

resource "aws_security_group" "allow_tls" {
  name        = "allow_tls"
  description = "Allow TLS inbound traffic"
 # vpc_id      = aws_vpc.main.id    we want to use the default vpc for now

  ingress {
    description  = "TLS from VPC"
    from_port    = 443
    to_port      = 443
    protocol     = "tcp"
    cidr_blocks  = ["0.0.0.0/0"] #changed from [aws_vpc.main.cidr_block] to ["0.0.0.0/0"] 
   #  ipv6_cidr_blocks = [aws_vpc.main.ipv6_cidr_block]
  }

  egress {
    from_port    = 0
    to_port      = 0
    protocol     = "-1"     #      "-1 " means open all ports in the outbound rule
    cidr_blocks  = ["0.0.0.0/0"]    #  ["0.0.0.0/0"]  means allow all traffic
   # ipv6_cidr_blocks = ["::/0"]    we dont need cidr block ipv6
  }

  tags = {
    Name = "allow_tls"
  }
}

 #ingress means inbound rules in security group
 #egress means outbound rules in security group

 The reourceType = aws_security_group

 The reourceName = allow_tls

How TO ATTACHED THIS SECURITY GROUP TO OUR EC2-INSTANCE RESOURCE.

 Let us attached this security group on our ec2-instance that we have provisioned before called ec2-demo.


NOTE: 


when you want to create instance inside a custom vpc use

vpc_security_group_ids       as a list

when you want to create instance inside a default vpc use

security_group    as a list.

a list goes with square bracket  []


Since we already created a custom vpc, let us use the custom vpc called first-vpc

 Let us use vpc security group IDs for the security group and let be a list[] . that is 

 add the below

 vpc_security _group_ids = [reourceType.resourceName.id]

  vpc_security _group_ids = [aws_security_group.allow_tls.id]

  add the above line to the resource file that we have created above called ec2-server.tf that is 

resource "aws_instance" "ec2-demo" {
  ami =  "var.my-ami" # us-west-1
  instance_type = var.instance_type
  user_data = file("${path.module}/app-install.sh")
  vpc_security _group_ids = [aws_security_group.allow_tls.id]
  tags = {
   "name" = "ec2-new"
  } 
}

and the run

terraform apply -auto-approve

It will create the SG and attached it to your ec2-instance called ec2-demo

DATA SOURCE

Data source are used to get information from a particular resource.

AMI datasource

This contains details or data about AMI resource that you want. As below

data "aws_ami" "example" {
  most_recent      = true  # searches recent AMI in aws based on your filter configuration 
  owners           = ["amazon"] # it is amazon that owms this ami dats source

  filter {
    name   = "name"
    values = ["amzn2-ami-kernel-5.10-hvm-*-gp2"]
  }

  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

    filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}


Since we have ami ID that we copied from amazon-linus as ami-011ab7c70f5b5170a

We went to ami console in aws and click ami and the pasted ami-0ee4f2271a4df2d7d on the search button (select public) and the data about the above ami came out. We now use the data to configured the above ami-datasource config file.

We copied the AMI name. 

Since we want the most recent ami to always reflect in our ami-data-source, we decided to split the ami name above. to be as

amzn2-ami-kernel-5.10-hvm-2.0.20231218.0-x86_64-gp2

broken to become

ami name = amzn2-ami-kernel-5.10-hvm-*-gp2

architecture = x86_64

virualization = hvm

The above as been configured putting these details.

NOTE: The essence of this ami data source is that it will help us to always use the lastest ami from aws instead of us going to aws to search for recent ami.

FOR UBUNYU AMI DATA-SOURCE

ami name that i copied from aws was below

amzn2-ami-kernel-5.10-hvm-2.0.20231218.0-x86_64-gp2

I just pasted the above ami name on the ami-datasource block  and everyother things is same as that of the amazon-linux-2 as shown below


data "aws_ami" "example" {
  most_recent      = true  # searches recent AMI in aws based on your filter configuration 
  owners           = ["amazon"] # it is amazon that owms this ami dats source

  filter {
    name   = "name"
    values = ["amzn2-ami-kernel-5.10-hvm-2.0.20231218.0-x86_64-gp2"]
  }

  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

    filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}


HOW TO ASSOCIATE OR CREATE SSH-KEY TO YOUR EC2-INSTANCE

If your ec2-instance does not have an ssh-key, you will not be able to connect via ssh to your ec2-server in any platform.

If you already have a key before, you can go to your ec2 console, click on key pair and you will see that name of the key you have before. Go to your variable file and then create a variable in for moe

variable "ec2_key_pair" {
description = "ssh key pair"
	
	type = "string"
	default = "nameOfTheKeyPair"

} 
 this will be referenced as   var.ec2_key_pair

Then go to your resrource file and then referenced the ssh key par as

key_name = var.ec2_key_pair


CREATION OF SECURITY GROUP VARIABLES

# Create Security Group - SSH Traffic. This will allow ssh to be opened (port 22)
resource "aws_security_group" "terraform-ssh-SG" {
  name        = "terraform-ssh-SG"
  description = "ALLOW SSH via ssh port"
  ingress {
    description = "Allow Port 22"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # means ports open everywhere
  }


  egress {
    description = "Allow all ip and ports outbound"    
    from_port   = 0
    to_port     = 0
    protocol    = "-1"  # This means open all traffic
    cidr_blocks = ["0.0.0.0/0"] # means ports open everywhere
  }

  tags = {
    Name = "nameOfSecurityGroup"
  }
}

#ingress means inbound rules
#egress means outbound rules

# Create Security Group - Web Traffic. This will allow web search(appache). port 80 and 443

resource "aws_security_group" "terraform-web-SG" {
  name        = "terraform-web-SG"
  description = "allow Web search"
  ingress {
    description = "Allow Port 80"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  ingress {
    description = "Allow Port 443"
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }  
  egress {
    description = "Allow all ip and ports outbound"    
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "vpc-web"
  }
}


it will be referenced as

# vpc_security_group_ids = [ aws_security_group.terraform-ssh-SG.id, aws_security_group.terraform-web-SG.id ]

TERRAFORM OUTPUT VALUES

This deplays the outcome of your result in your environment after deployment if output values manifest file where configured as part of the infrastructure.

EXAMPLE

# Terraform Output Values

#public IP = this is display your public IP

/*

output "outputName" {
  description = "display the name of what you want to display"
  value = value = resourceType.ResourceName.publicIpAttribute 

  # above value applies when did not configure count variable as part of the variable file
  # value = aws_instance.web-server["count=indiceNumber"].public_ip   
  # above value applies when you configured count variable as part of the variable file
  Note: count indice number starts from 0  that is ["count=0"]

["count=0"]   means that you want public ip to display in the first ec2-server.

["count=0"]  means that you want public ip to display in the second ec2-server.

}

*/

output "instance_publicip" {
  description = "display EC2 Instance Public IP"
  value = aws_instance.myec2vm.public_ip  
  # above means --->  value = resourceType.ResourceName.publicIpAttribute
}

# Public DNS  = this is display your public DNS

output "instance_publicdns" {
  description = "display EC2 Instance Public DNS"
  value = aws_instance.myec2vm.public_dns
  # above value applies when you did not configure count variable as part of the infrastructure
  # value = aws_instance.web-server["count=indiceNumber"].public_ip   
  # above value applies when you have configured count variable as part of the variable file

 Note: count indice number starts from 0  that is ["count=0"]

["count=0"]   means that you want public ip to display in the first ec2-server.

["count=0"]  means that you want public ip to display in the second ec2-server.

output "web-server_publicIP" {
 description = "display web-server public ip"

# value = aws_instance.web-server.public_ip  # if you did not configure count variable as part of the infrastructure
  # above means --->  value = resourceType.ResourceName.publicDNSAttribute
}

NOTE SOME TERRAFORM COMMAND

# Terraform Destroy
terraform plan -destroy  # You can view destroy plan using this command
terraform destroy

# Clean-Up Files
rm -rf .terraform*
rm -rf terraform.tfstate*

NOTE

Anytime you create a new directory and create new tf files, that directory need to be initialized so that terraform can download the provider and als the .terraform folder

# Terraform Initialize
terraform init
Observation:
1) Initialized Local Backend
2) Downloaded the provider plugins (initialized plugins)
3) Review the folder structure ".terraform folder"

USING DYNAMIC VARIABLE TO CONFIGURE SECURITY GROUP

The idea of configuring security group in a dynamic way is to make sure that security group is wriiten in a dynamic variable form so that it can be reusable and you can always modify the ports anytime, in your variable file without tampering with the dynamic security block.

For Variable block for security group can now be in form of 

lets the variable name below be ingressrules

Since we want to parse different ports, let us use list(number) variable. 

a list (number) variable is used to parse a variable that has to be with a list. maybe you want to parse different ports in a securityGroup. That is

# list(number) variable

variable "ingressrules" {
    type = list(number)
    default = [80,443,8080,22,9000]
}

# above can be referenced as   var.ingressrules

# lets the variable name here be egressrules

variable "egressrules" {
    type = list(number)
    default = [80,443,25,3306,53,8080]
}

# above can be referenced as var.ingressrules


Dynamic block will be as follows

resource "aws_security_group" "web-server" {
    name = "Allow HTTPS"

    dynamic "ingress" {
        iterator = port
        for_each = var.ingressrules  #where is the variableName above
        content {
        from_port = port.value
        to_port = port.value
        protocol = "TCP"
        cidr_blocks = ["0.0.0.0/0"]
        }
    }

    dynamic "egress" {
        iterator = port
        for_each = var.egressrules  #where is the variableName above
        content {
        from_port = port.value
        to_port = port.value
        protocol = "TCP"
        cidr_blocks = ["0.0.0.0/0"]
        }
    }
}

LOOP VARIABLES ( LIST VARIABLES AND MAP VARIABLES)

You can also use a list variable or map variable to configure instance_type. what we used to configured instant type before was called string variable. That is 

# AWS EC2 Instance Type - List
variable "instant_type_string" {
	decrsiption = "instant_type using string variable"
	type = string
	default = "t3.micro"
}

# AWS EC2 Instance Type - List
variable "instant_type_list" {
	decrsiption = "list of instant_type"
	type = list(string)
	default = ["t2.micro", "t2.medium", "t3.micro", "t3.small", "t3.large"]

	# instant_type = var.instant_type_list[0]
}

# AWS EC2 Instance Type - Map
variable "instance_type_map" {
  description = "EC2 Instance Type using map variable block"
  type = map(string)
  default = {
    "dev" = "t3.micro"
    "qa" = "t3.small"
    "prod" = "t3.large"
  }
}

# instant_type = var.instant_type_list["dev"]

MODULES

While do we use terraform modules?

1. Terraform modules enables us to re-use of code.

2. Supports versioning to maintain compatibility.

3. Stores codes remotely. That means you can store modules in github, s3, etc

4. enables easier testing

5. It enables encapsulation with all the separate resources under one configuration block

6. Modules can be nested inside modules. This means that you canm have a module inside another module. That is called a child module

TYPES OS MODULE

LOCAL AND REMOTE MODULES

---> Local Modules are stored alongside with the terraform configuration under separate directory under the same repository. with source    path ./  or   ../

---> remote Modules are stored externally in a separate repository and its supports versioning. That means you can store modules in github, s3, etc

Terraform modules supports backend like below

local path
github
s3 bucket
bitbucket
GCS bucket
HTTP URLS

If you store your modules in any of the above, you can be able to call them and use them.

REQUIREMENTS FOR USING A MODULE

1. It must be in github and must be a public repo so that external users can be able to used it.

2. It must be named Terraform-ProviderName-ResourceName. That is the naming convention on how you must name your module.

resourceName = type of infrastructure or resource the module manages.

provider = is the name of the provider where the infrastructure is created from.

For example

terraform-aws-ec2-instance

terraform-google-vault

3. Must maintain x.y.z tags for releases to identify module version. And the prefix to the tags may be represented as v  (version) and tags number will follow. Example --> v1.2.0

Above is how you can write a module and publish it

Let us create a directory called ec2 and then will can now create our resources inside the directory.

I created and copied the resources i have in my tf-practice directory into my new directory called ec2.

I went back to me tf-practice directory and  will create a file called module-main.tf

My ec2 directory now becomes my module. 

inside my tf-practice, you have the module called ec2 and also my module-main.tf

It is the main.tf that will now use to call and use the configuration in my ec2 module to create resources and the .terraform and terraform.tfstate will be stored inside tf-practice. 

The ec2 module is a local module because it was created on my computer. when you initialize the directory in which the module is present or created, the module will first of all my initillized and the .terraform will be stored or downloaded on that directory.

EXAMPLE 1

I created a directory inside my desktop directory called module and then created another directory called ec2-module-sxample-1 inside it.

Inside ec2-module-sxample-1, i created another direcortory called ec2

I now copied content contain all the terraform manifest file that i have configured in my tf-practice directory (you can get to my tf-practice via desktop directory or folda) copied into my ec2 directory

I now went back to my ec2-module-sxample-1directory by cd ..  and then create a file called main.tf (module file) and create my module block in the pattern below

module "moduleName" {
  source = "modulePath"
  }


modulePath is the path where the module file will build or create from. In this case, the modulePath will   ./ec2-module-example-1

let us name the module ec2-module

Therefore, it will be 

module "ec2-module" {
  source = "./ec2"
  }

Automaticall, ec2-module-example-1 now becomes a module

Since we have an output block inside in our ec2 module, then will must create one here in our main.tf at ec2-module-sxample-1

output "outputName" {
  value = "module.moduleName.outputResourceName"
  }

Note: The essence of using terraform module is to store .terraform and terraform.tfstate used in creating resources in either local environment on in remote environment.

I now run terraform commands

terraform init
terraform validate
terraform plan
terraform apply    or   terraform apply -auto-approve


EXAMPLE 2

Creating series of modules.

Let us first of all create directory called ec-module-example-2

Let us create a db, sg, web directories inside ec2-module-example-2. They are now child directorues to ec2-module-example-2

1.  db module has the below manifest terraform file

 ami-datasource.tf

 # Get latest AMI ID for Amazon Linux2 OS

data "aws_ami" "amzlinux2" {
  most_recent      = true
  owners           = ["amazon"]
  filter {
    name   = "name"
    values = ["amzn2-ami-kernel-5.10-hvm-*-gp2"]
  }
  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}


# db.tf  file

resource "aws_instance" "db" {
    ami = data.aws_ami.amzlinux2.id
    instance_type = "t2.micro"

    tags = {
        Name = "DB Server"
    }
}

output "PrivateIP" {
    value = aws_instance.db.private_ip
}

2. eip module has the below manifest terraform file  

eip = elastic ip

eip.tf

variable "instance_id" {
    type = string
}

resource "aws_eip" "web_ip" {
    instance = var.instance_id
}

output "PublicIP" {
    value = aws_eip.web_ip.public_ip
}

3. sg module has the below manifest terraform file  

sg.tf

variable "ingress" {
    type = list(number)
    default = [80,443,8080]
}

variable "egress" {
    type = list(number)
    default = [80,443]
}

output "sg_name" {
    value = aws_security_group.web_traffic.name
}

resource "aws_security_group" "web_traffic" {
    name = "Allow Web Traffic"

    dynamic "ingress" {
        iterator = port
        for_each = var.ingress
        content {
            from_port = port.value
            to_port = port. value
            protocol = "TCP"
            cidr_blocks = ["0.0.0.0/0"]
        }
    }

        dynamic "egress" {
        iterator = port
        for_each = var.egress
        content {
            from_port = port.value
            to_port = port. value
            protocol = "TCP"
            cidr_blocks = ["0.0.0.0/0"]
        }
    }
}

4. web module has the below manifesr file

# app.sh

#! /bin/bash
# Instance Identity Metadata Reference - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html
sudo yum update -y
sudo yum install -y httpd
sudo systemctl enable httpd
sudo service httpd start
sudo echo '<h1>Welcome to Landmark Technologies</h1>' | sudo tee /var/www/html/index.html
sudo mkdir /var/www/html/app1
sudo echo '<!DOCTYPE html> <html> <body style="background-color:rgb(250, 210, 210);"> <h1>Welcome to Landmark Technologies</h1> <p>Terraform Demo</p> <p>Application Version: V1</p> </body></html>' | sudo tee /var/www/html/app1/index.html
sudo curl http://169.254.169.254/latest/dynamic/instance-identity/document -o /var/www/html/app1/metadata.html
# Ensure that you update the inbound rules on the security group of your instance. Add http port 80

web.tf

resource "aws_instance" "web" {
    ami = data.aws_ami.amzlinux2.id
    instance_type = "t2.micro"
    security_groups = [module.sg.sg_name]
    user_data = file("./web/server-script.sh")
    tags = {
        Name = "Web Server"
    }
}

output "pub_ip" {
    value = module.eip.PublicIP
}

module "eip" {
    source = "../eip"
    instance_id = aws_instance.web.id
}

module "sg" {
    source = "../sg"
}

ami-datasource.tf


# Get latest AMI ID for Amazon Linux2 OS
data "aws_ami" "amzlinux2" {
  most_recent      = true
  owners           = ["amazon"]
  filter {
    name   = "name"
    values = ["amzn2-ami-kernel-5.10-hvm-*-gp2"]
  }
  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}

Create main.tf  file in the directory ec2-module-example-2 that housed db. Once you do that, db, sg, eip, web now becomes modules. create and paste below in main.tf

# Terraform Block
# Terraform Block
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}
# Provider Block
provider "aws" {
  region = "us-east-2"
}

module "db" {
    source = "./db"

}

module "web" {
    source = "./web"
}

output "PrivateIP" {
    value = module.db.PrivateIP
}

output "PublicIP" {
    value = module.web.pub_ip
}

Example 3

Basic VPC-module

Let us create a 3 Tier VPC module. 

3-tier vpc means vpc with 3 subnets like private subnet, public subnet and database subnet

Let us create a VPC module called vpc-module inside our module directory

let vpc-module contain the below terraform manifest file

variable.tf

# Input Variables

# AWS Region
variable "aws_region" {
  description = "Region in which AWS Resources to be created"
  type = string
  default = "us-east-2"
}

provider-tfversion.tf

# Terraform Block
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# Provider Block
provider "aws" {
  region  = var.aws_region
  profile = "default"
}


vpc.tf

let us go to terraform registry and get our vpc module block by typing vpc module terraform in your google search and then your will see version with version number, click on the version number and select latest version and then scroll down, you will see the basic usage. as below


module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  #you need to parse a version her
  #version = versionNumber you got from terraform registry

  name = "my-vpc"
  cidr = "10.0.0.0/16"

  azs             = ["eu-west-1a", "eu-west-1b", "eu-west-1c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]

  enable_nat_gateway = true
  enable_vpn_gateway = true 

  tags = {
    Terraform = "true"
    Environment = "dev"
  }
}

NOTE: 

You need to know that this vpc module that you copy from terraform registry already has a source code that was written by terraform developers. It has been writteh a in such a way that whatever modification you made correctly, it will still be runned from the terraform source code that has already be written in the public github repository.

You can now edit the above template to what you need. we want to use US AZs. We want to use us-east-2 region.

If you want to input or add more configuration to the vpc-module block, you will see INOUT at the left hand side, click on the input, you will see what you can input, that is were we copied our input that we added to the above terraform vpc module above. input like

 single_nat_gateway = true
 create_database_subnet_group = true
 create_database_subnet_route_table = true
 database_subnets = ["10.0.151.0/24", "10.0.152.0/24"] 
 enable_dns_hostnames  = true
 enable_dns_support = true

That is

# Create VPC Terraform Module

module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "5.4.0"

  name = "vpc-dev"
  cidr = "10.0.0.0/16"

  azs             = ["us-east-2a", "us-east-2c"]
   private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24"]

  enable_nat_gateway = true
  single_nat_gateway = true

 create_database_subnet_group = true
 create_database_subnet_route_table = true
 database_subnets = ["10.0.151.0/24", "10.0.152.0/24"]

 enable_dns_hostnames  = true
 enable_dns_support = true

 public_subnet_tags = {
    Type = "public-subnets"
  }

  private_subnet_tags = {
    Type = "private-subnets"
  }

  database_subnet_tags = {
      Type = "database-subnets"
    }

  tags = {
    Owner = "Landmark"
    Environment = "dev"
  }
  vpc_tags = {
    Name = "vpc-dev"
  }
}

If you notice, we do not have main.tf module file that we indicate our mocdule block that we contain the source of the module we want to use. This is simply because we want to edit the vpc module we copied from the terraform registry.

Just go ahead and run terraform commands inside the directory or module called vpc-module-example-3 you have created

NOTE: The above VPC module that we have just written above was hard-coded. It is not re-usable and so it is not recommended. We need to use variable to write the vpc module. We must write it in a standardized form.

EXAMPL 4

MORE STANDARDIZED VPC MODULE 

This will be written in variables.

1. provider/version block

provider-version.tf

# Terraform Block
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# Provider Block
provider "aws" {
  region  = var.aws_region
  profile = "default"
}

2. variable.tf

# Input Variables. We added more variables to the one we did before
# AWS Region
variable "aws_region" {
  description = "Region in which AWS Resources to be created"
  type = string
  default = "us-east-2"
}
# Environment Variable
variable "environment" {
  description = "Environment Variable used as a prefix"
  type = string
  default = "dev"
}
# Business Division : this talk about the body or department that owes or using this vpc
variable "business_division" {
  description = "Business Division in the large organization this Infrastructure belongs"
  type = string
  default = "Finance" #this means, owner of this vpc is finance (finance division or dept)
}

3. local values block : 

This is one of the high level block.

This is the block that defines some classified data that you do not want to mention inside your code. data like owners (owners of the resource), environment (the environment that would be using the resource like dev, prod, etc), etc This local value will be written in variable form

local.tf

# Define Local Values in Terraform

locals {
  owners = var.business_division
  environment = var.environment
  name = "${var.business_division}-${var.environment}"
  #name = "${local.owners}-${local.environment}"
  common_tags = {
    owners = local.owners      # this will reflect the owner of the vpc
    environment = local.environment
  }
}

4. vpc Input variables:

This is the variables that you inputed or added to make sure variable more effective and standardized. 

vpc-variables.tf

# VPC Input Variables

# VPC Name
variable "vpc_name" {
  description = "VPC Name"
  type = string
  default = "myvpc"
}

# VPC CIDR Block
variable "vpc_cidr_block" {
  description = "VPC CIDR Block"
  type = string
  default = "10.0.0.0/16"
}

# VPC Availability Zones
variable "vpc_availability_zones" {
  description = "VPC Availability Zones"
  type = list(string)
  default = ["us-east-2a", "us-east-2c"]
}

# VPC Public Subnets
variable "vpc_public_subnets" {
  description = "VPC Public Subnets"
  type = list(string)
  default = ["10.0.101.0/24", "10.0.102.0/24"]
}

# VPC Private Subnets
variable "vpc_private_subnets" {
  description = "VPC Private Subnets"
  type = list(string)
  default = ["10.0.1.0/24", "10.0.2.0/24"]
}

# VPC Database Subnets
variable "vpc_database_subnets" {
  description = "VPC Database Subnets"
  type = list(string)
  default = ["10.0.151.0/24", "10.0.152.0/24"]
}

# VPC Create Database Subnet Group (True / False)
variable "vpc_create_database_subnet_group" {
  description = "VPC Create Database Subnet Group"
  type = bool
  default = true
}

# VPC Create Database Subnet Route Table (True or False)
variable "vpc_create_database_subnet_route_table" {
  description = "VPC Create Database Subnet Route Table"
  type = bool
  default = true
}


# VPC Enable NAT Gateway (True or False)
variable "vpc_enable_nat_gateway" {
  description = "Enable NAT Gateways for Private Subnets Outbound Communication"
  type = bool
  default = true
}

# VPC Single NAT Gateway (True or False)
variable "vpc_single_nat_gateway" {
  description = "Enable only single NAT Gateway in one Availability Zone to save costs during our demos"
  type = bool
  default = true
}

5. vpc module block

 vpc-module.tf

# Create VPC Terraform Module
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "3.11.5"
  #version = "~> 2.78"

  # VPC Basic Details
  name = "${local.name}-${var.vpc_name}"
  cidr = var.vpc_cidr_block
  azs             = var.vpc_availability_zones
  public_subnets  = var.vpc_public_subnets
  private_subnets = var.vpc_private_subnets

  # Database Subnets
  database_subnets = var.vpc_database_subnets
  create_database_subnet_group = var.vpc_create_database_subnet_group
  create_database_subnet_route_table = var.vpc_create_database_subnet_route_table
  # create_database_internet_gateway_route = true
  # create_database_nat_gateway_route = true

  # NAT Gateways - Outbound Communication
  enable_nat_gateway = var.vpc_enable_nat_gateway
  single_nat_gateway = var.vpc_single_nat_gateway

  # VPC DNS Parameters
  enable_dns_hostnames = true
  enable_dns_support   = true


  tags = local.common_tags
  vpc_tags = local.common_tags

  # Additional Tags to Subnets
  public_subnet_tags = {
    Type = "Public Subnets"
  }
  private_subnet_tags = {
    Type = "Private Subnets"
  }
  database_subnet_tags = {
    Type = "Private Database Subnets"
  }
}

6. vpc output block

vpc-output.tf

# VPC Output Values

# VPC ID
output "vpc_id" {
  description = "The ID of the VPC"
  value       = module.vpc.vpc_id
}

# VPC CIDR blocks
output "vpc_cidr_block" {
  description = "The CIDR block of the VPC"
  value       = module.vpc.vpc_cidr_block
}

# VPC Private Subnets
output "private_subnets" {
  description = "List of IDs of private subnets"
  value       = module.vpc.private_subnets
}

# VPC Public Subnets
output "public_subnets" {
  description = "List of IDs of public subnets"
  value       = module.vpc.public_subnets
}

# VPC NAT gateway Public IP
output "nat_public_ips" {
  description = "List of public Elastic IPs created for AWS NAT Gateway"
  value       = module.vpc.nat_public_ips
}

# VPC AZs
output "azs" {
  description = "A list of availability zones spefified as argument to this module"
  value       = module.vpc.azs
}

7. terraform variables

There are 10 different ways your can parse terraform variables, you can use .tf, .tfvars

terraform.tfvars

This is used in parsing generic variables.

# Generic Variables. Thease are variables that you have declared in your variable files. If you modified them for the purpose of provisioning any resource, the resource will be provisioned as modified. 

If I change the region to another region like us-east-1. It will change the region to us-east-1.

#Generic variables
aws_region = "us-east-2"
environment = "stag"
business_division = "HR1"

8.

vpc.auto.tfvars

# VPC Variables
vpc_name = "myvpc"
vpc_cidr_block = "10.0.0.0/16"
vpc_availability_zones = ["us-east-1a", "us-east-1c"]
vpc_public_subnets = ["10.0.101.0/24", "10.0.102.0/24"]
vpc_private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
vpc_database_subnets= ["10.0.151.0/24", "10.0.152.0/24"]
vpc_create_database_subnet_group = true
vpc_create_database_subnet_route_table = true
vpc_enable_nat_gateway = true
vpc_single_nat_gateway = true

let us create a directory/module called standard-vpc-module-example-4 inside module directory and create the above inside standard-vpc-module-example-4 

initialize, validate, plan and apply.

TERRAFORM BACKEND.

Since Terraform state file can contain sensitive values, you must keep your state file secure to avoid exposing this data. 

We have 2 types of terraform backend, we have the local and remote backend. It is normal that the remote backend will be safer to store your statefile. That is storing your statefile in remote backed like github, s3 bucket, bitbucket etc.

THINGS TO KNOW ABOUT TERRAFORM BACKEND

1. If a configuration includes or did not parse backend block, Terraform will to use the local backend by default, which performs operations on the local system and stores state as a plain file in the current working directory.

2. When changing backends, Terraform will give you the option to migrate your state to the new backend. This lets you adopt backends without losing any existing state or statefile.

3. You can change your backend configuration at any time. You can change both the configuration itself as well as the type of backend (for example from "consul" to "s3").

4. Terraform will automatically detect any changes in your configuration and request a reinitialization. As part of the reinitialization process, Terraform will ask if you'd like to migrate your existing state to the new configuration. This allows you to easily switch from one backend to another.

HOW TO CREATE S3 BUCKET WITH WITH OBJECT OWNERSHIP PARSED 

With the aid of terraform registry, I created s3 bucket using terraform by creating the following blocks in one manifest file:
1. terraform-provider version block
2. provider block
3. s3 bucket block 
4. bucket ownership block---> that will grant the bucket full control of the bucket, so that i can be able to parse ACL whether to make the ACL private or public by create ACL block
5. ACL block
5. bucket versioning block.

NOTE: I created all the blocks inside one file as below


 # terrafform version/Provider Block
 #provder-version.tf
 terraform {
  required_version = "~> 1.0" # which means any version equal & above 0.14 like 0.15, 0.16 etc and < 1.xx
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

#provider block
provider "aws" {
   region  = "us-east-2"
  # profile = "landmark" I commented here because i want to use my default credentials.
  /*
  profile is a name that has your IAM role credentials that you created in aws that can connect yoour local computer to your aws account after you can installed aws in your local system and also run aws configure to allow you put in your accessKey and secretKey
  */
 }

 # creation of s3 bucket block
resource "aws_s3_bucket" "tf-buc" {
  bucket = "osas-tfstate-file-new00113322"
   tags = {
    Name        = "tf-buc"
    Environment = "Dev"
  }
}

# Object Ownership Block
resource "aws_s3_bucket_ownership_controls" "oo-tf-buc" {
  bucket = aws_s3_bucket.tf-buc.id
  rule {
    object_ownership = "BucketOwnerPreferred" 
  }
} 
   /*
    object_ownership = "BucketOwnerPreferred"  # this will enable ACL for your bucket to be functional So that you can either grant public or private access in your ACL block. It give you full control over 
    your bucket
*/

#creation of  acl block for s3 bucket
resource "aws_s3_bucket_acl" "acl-tf-buc" {
  bucket = aws_s3_bucket.tf-buc.id
  acl    = "private"      # creating a private s3 bucket
}

# creation of versioning block for s3 bucket
resource "aws_s3_bucket_versioning" "versioning-tf-buc" {
  bucket = aws_s3_bucket.tf-buc.id
versioning_configuration {
    status = "Enabled"
  }
}

I created a directory called bucket and i created a file called s3.tf inside the directory and pasted the above inside. I ran

terraform init, terraform validate, terraform plan, terraform apply -auto-approve

When i checked my s3 bucket, the bucket was created and it was in private. This means that whatever object that i upload there cannot be views publicly.

NOTE: 

bucket name usually parsed as   bucket  is globally unique meaning it has to be a bucket name that has not been used anywhere in the world before. so you can to be creative when choosing a bucketname.

If the bucket name has been used, the bucket will not be created when you run the command

I then modified my ACL to public. and when i check my s3 bucket, it was now in public

USING REMOTE BACKEND

STORING STATEFILE IN S3 BUCKET WITH S3 BUCKET BACKEND THAT HAS A LOCK (DynamoDB)

You can create an s3 bucket that has a lock. You can use a dynamoDB lock on the s3 bucket.

Stores the state as a given key in a given bucket on Amazon S3. This backend also supports state locking and consistency checking via Dynamo DB, which can be enabled by setting the dynamodb_table field to an existing DynamoDB table name. A single DynamoDB table can be used to lock multiple remote state files. Terraform generates key names that include the values of the bucket and key variables.

It is highly recommended that you enable Bucket Versioning on the S3 bucket to allow for state recovery in the case of accidental deletions and human error.

NOTE:

1. The reason why we use DynamoDB locking to configure our remote backend like s3 is simply because we do not want to data corruption since we want to use a distribution environment for storage like s3.

Using s3 as your remote backend backup of your statefile, it means that the statefile will be stored in a distribution environment.

Distribution environment is an environment that support multi-users or multiple people to access the environment anytime and if at the same time. s3 is a distribution environment.

If different engineers are accessing the statefile, there is the tendency the state file may be corrupted, It can also lead to data consistency

This is why we use dynamoDB to lock the statefile so that it can only allow one person to the working with the statefile at a particular time to avoid data corruption and data inconsistency.

DynamoDB Table Permissions

If you are using state locking, Terraform will need the following AWS IAM permissions on the DynamoDB table (arn:aws:dynamodb:::table/mytable):

 dynamodb:GetItem
 dynamodb:PutItem
 dynamodb:DeleteItem
##Data Source configurations

To make use of the S3 remote state in another configuration, use the terraform_remote_state data source.

data "terraform_remote_state" "network" { backend = "s3" config = { bucket = "terraform-state-prod" key = "network/terraform.tfstate" region = "us-east-1" } }

EXAMPLE

TERRAFORM STATE FILE STORED IN REMOTE BACKEND LIKE S3 BUCKET
 
Let us create resources in which the statefile will be saved in the remote backend like s3 bucket.

1. 

#ami-datasource block
#ami-datasource.tf
# Get latest AMI ID for Amazon Linux2 OS
data "aws_ami" "amzlinux2" {
  most_recent      = true
  owners           = ["amazon"]
  filter {
    name   = "name"
    values = ["amzn2-ami-kernel-5.10-hvm-*-gp2"]
  }
  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}

2. 

#dynamodb table resource block.  You can search thgis our on terraform registry.

resource "aws_dynamodb_table" "tf_lock" {
  name = "terraform-lock"
  hash_key = "LockID"
  read_capacity = 3
  write_capacity = 3
  attribute {
     name = "LockID"
     type = "S"
   }
  tags = {
    Name = "Terraform-Lock-Table"
   }
   lifecycle {
   prevent_destroy = true #This will prevent the statefile for being destroyed or terminated
  }
 }

3 

backend-block.tf
 make sure you created lock and possible with other resources before applying or parsing this backend block.

 /*
  backend "s3" {
    bucket = "osas-tfstate-new00118404"   # s3 bucket that i created
    key = "remote-backend/terraform.tfstate"
    
    #key is the path inside the s3 bucket where statefile will be created. 
    #remote-backend is the directory that will be created inside s3 bucket and 
    # then add /terraform.tfstate because terraform.tfstate is the name of the file 
    #that the statefile will be stored.
   
    dynamodb_table = "terraform-lock"
    region = "us-east-2"

 } 
 */

4.

 # terrafform version/Provider Block
 #provder-version.tf
 terraform {
  required_version = "~> 1.0" # which means any version equal & above 0.14 like 0.15, 0.16 etc and < 1.xx
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
   region  = "us-east-2"
  # profile = "landmark" I commented here because i want to use my default credentials.
  /*
  profile is a name that has your IAM role credentials that you created in aws that can connect yoour local computer to your aws account after you can installed aws in your local system and also run aws configure to allow you put in your accessKey and secretKey
  */
 }


 5.

 #ec2-instance block
 #ec2.tf

 resource "aws_instance" "ec2" {
    ami = data.aws_ami.amzlinux2.id
    instance_type = var.my_instance_type

    tags = {
      "Name" = "tf-instance"
    }
}

6.


#variable block
#variable.tf
variable "my_instance_type"{
  type = string
  default = "t2.micro"
}

Let us create a directory called tf-directory and the create manifest file for each block listed above.

aws s3api put-object --bucket DOC-EXAMPLE-BUCKET --key key-name --body path-to-file --acl bucket-owner-full-control

NOTE:

Before we run terraform commands,  you need to create dynamodb lock first. You can create the dynamodb lock and other resources first before you can now configure the remote backend that would be used to store the statefile.

I commented on our backend block file called backend-block.tf and then ran

terraform init

because we have tried to initialized before, it shows error and asked me to migrate the statefile  by running

terraform -migrate-state

You can also ignore the to migrate and just delete the statefile and all backups that you have in your pwd. and then re-run

terraform init

If you have not attempted to initialized before, it you run terraform init, it will go through. 

I now ran 

terraform apply auto-approve

my ec2-instance, dynamodb lock were created in my local backend inside my pwd


I now remove the comment in my backend-block.tf and then ran

terraform init

It says that if i want to save if you want to save your state in your s3 bucket, enter "yes"

I entered yes and it was initialized

terraform apply auto-approve

if you go to your s3 bucket, click on remote-backend, clicj on terraform.tfstat, clicj on open at right up, you will see that the statefile in my local has been stored in my s3 bocket we created, because we parsed remote-backend/terraform.tfstate as our key.

If you run 

terraform destroy

it will say you cannot destroy. this is because we parsed  prevent_destroy = true  in our dynamodb table lock

for it to destroy, re-parse  prevent_destroy = false  

 in our dynamodb table lock and save  and then run

terraform destroy --lock=false     # to destroy the lock and the statefile ins s3/

If you go back to your s3 bucket, the file will no longer open

VPC-MODULE-REMOTE-STATE

This deals with the provisioning of vpc with required infrastructures using a module and then store the state file remotely. maybe storing it in a remote backend storage like s3 bucket, github, gitlab etc.

EXAMPLE

I went back to the standard-vpc-module-example-4 directory inside module where I created standard vpc and copy the standard-vpc-module-example-4 directory to another directory that i called remote-state-vpc-module that i created and then rename the standard-vpc-module-example-4 directory inside remote-state-vpc-module as remote-state-datasource.

I first of all created a dynamodb lock file called dynamodb-lock.tf file inside the remote-state-datasource directory

I ran terraform command by

terraform init
terraform validate
terraform apply -auto-approve

in order to create vpc infrastructure and dynamodb lock which will be created. The statefile will be created inside the local remote-state-datasource directory.

Note: It is very important to created the lock first and possibly the resources before you parse your remote-backend storage software that you want and initialized.

Then to store the statefile remotely in s3 bucket, we now created a remote backend file inside the same directory and initialized it.

Once you initialized it, it will ask you if you want to copy the state file to your remote backend like s3, just type yes and it will copy the statefile to your backend.

NOTE: Always created your lock file before parsing your remote-backend block. The lock will be the one to lock your remote statefile for it not to be tampered with or altered in a distributed environment.

To destroy the statefile, you need parse false on the lock file by running

terraform destroy --lock=false

and also go to your dynamodb lock tf file to modify the  prevent_destroy = true  to  prevent_destroy = false

I also ran

terraform destroy -auto-approve

To destroy the vpc and other infrastructure that was created.

The lock is only protecting the statefile inside the directory that was createe in the remote backend called s3 bucket that was parsed in the backend block.

NOTE:

Most of the time, the company you work with may already have an existing statefile of a network (vpc network) stored in s3 bucket or in other storage device like what we have just done above and your duty maybe just to provision resources inside maybe one of the subnet of that vpc network.

Knowing that the vpc statefile has been store remotely or in a remote backend like s3 bucket, you can be able to provision resources by parsing of definining the backend where the statefile has been configured so that your resource can eb provisioned from the vpc network via the statefile.

This is what is called remote state datasource. 
 

REMOTE STATE DATASOURCE

You are now using your ami datasource to provsion a resource like ec2-instance from a custom vpc statefile that has been stored remotely.

As soon as your provsion the resource, your own statefile birthed from the custom statefile would be stored in your local pwd.

NOTE:

If you want to cd back twice, you run

cd ../../

If you want to cd back 3 times, you run

cd ../../../

If you want to cd back 3 times, you run

cd ../../../../

HOW TO PROVSION AN EC2-INSTANCE FROM REMOTE STATE CUSTOM VPC.

This is called remote state datasource

Let us create an ec2-instance from the custom statefile that has been stored inside the s3 bucket.

Let us create a directory and name the directory remote-state-datasource by using the below manifest terraform file  

#ami-datasource block
#ami-datasource.tf
# Get latest AMI ID for Amazon Linux2 OS
data "aws_ami" "amzlinux2" {
  most_recent      = true
  owners           = ["amazon"]
  filter {
    name   = "name"
    values = ["amzn2-ami-kernel-5.10-hvm-*-gp2"]
  }
  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}


 # remote-ect.tf
 
 # terrafform version/Provider Block
 #provder-version.tf
 terraform {
  required_version = "~> 1.0" # which means any version equal & above 0.14 like 0.15, 0.16 etc and < 1.xx
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
   region  = "us-east-2"
  # profile = "landmark" I commented here because i want to use my default credentials.

 # profile is a name that has your IAM role credentials that you created in aws that can connect yoour local computer to your aws account after you can installed aws in your local system and also run aws configure to allow you put in your accessKey and secretKey
 
 }
 
# to parse your remote vpc network statefile stored in your remote backend like s3 bucket, you can you the below

# remote state datasource block
data "terraform_remote_state" "network" {
  backend = "s3"
  config = {
    bucket = "osas-tfstate-new00118404"
    key = "remote/terraform.tfstate" 
    region = "us-east-2"
  }
}

# You can get the above config template from your terraform registry online. 
# by searching with the sentence "remote state datasource"

/*
If you want to create ec2 from the local backend vpc network statefile stored in your pwd, you can use 
the below configuration

data "terraform_remote_state" "network" {
  backend = "local"
  config = {
      path    = "../remote-data-source/terraform.tfstate"
  }
}
*/

resource "aws_instance" "my-ec2"{
  ami = data.aws_ami.amzlinux2.id
  instance_type = "t2.micro"
  subnet_id = data.terraform_remote_state.network.outputs.public_subnets[0]

  tags = {
    "Name" = "my_ec2"
  }
}

then run

terraform init
terraform apply -auto-approve

You will see that that an ec2 instance has been provisioned from the statefile that was stored in s3 bucket because we defined the s3 remote backend where the ec2-instance will provision from.

Another statefile of yours will be stored in your pwd locally in your computer.
You can also store local statefile inside your remote backend by configuring or defining your remote backend.

If you check your ec2 console, you will see that ec2 instance were created in a private subnet and not public subnet even if you parse a public subnet to the file. This is because our s3 ACL was private meaning object cannot be views privately and the vpc subnets that will be aloowed will be a private vpc subnets.

If you run  

terraform destroy -auto-approve

It will destroy the ec2-instance that you created inside the custom vpc that you created.

If you want to destroy your vpc, make sure you first of all destroy the resource that is bening powered by the vpc as done above and then go your your vpc remote directory and run

terraform destroy -auto-approve

This will destroy the vpc but will not be able to destroy the statefile because of the dynamodb lock

To destroy the statefile run

terraform destroy --lock=false

It will not delete or destroy the content inside the statefile in your s3 bucket

THINGS TO NOTE

Anytime you create a new file, always initialize before running aother terraform command

if you change region in your file or other modification inside your file, it is impportant you save the file after modification and then delete the .terraform, terraform.tfstate, terraform.lock.hcl and terraform.tfstate.backup and the re-run

terraform init

So that it will start afresh and download the appropriate terraform provider and version from the new region you chose. always do this whenever you modified your files so avoid error when creating resources.
